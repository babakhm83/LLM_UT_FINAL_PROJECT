{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3415927",
   "metadata": {},
   "source": [
    "# üöÄ Bitcoin News Effects Training with Unsloth\n",
    "\n",
    "Training **Qwen2.5-4B-Instruct** to analyze Bitcoin news effects using **Unsloth** for RTX 3090 optimization.\n",
    "\n",
    "## Key Features:\n",
    "- ‚úÖ **News Effects Analysis**: Train on individual news items and their Bitcoin price impacts\n",
    "- ‚úÖ **Unsloth Acceleration**: 2x faster training optimized for RTX 3090\n",
    "- ‚úÖ **Real-time Monitoring**: See model outputs every 50 steps during training\n",
    "- ‚úÖ **Memory Optimized**: Efficient training for 24GB VRAM\n",
    "- ‚úÖ **Multiple Export Formats**: LoRA, merged, GGUF for deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "146d030a",
   "metadata": {},
   "source": [
    "## üìä Dataset Overview\n",
    "\n",
    "This training uses your **Bitcoin News Effects Dataset** which contains:\n",
    "- **Individual news items** from daily Bitcoin coverage\n",
    "- **News analysis** with sentiment, impact direction, magnitude\n",
    "- **Structured outputs** for price effect prediction\n",
    "- **Market context** with daily recommendations and probabilities\n",
    "\n",
    "**Training Task**: Given a news item, predict its effect on Bitcoin price with structured JSON output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ddada77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîß Install Unsloth and dependencies for RTX 3090 training\n",
    "import subprocess\n",
    "import sys\n",
    "import os\n",
    "\n",
    "def install_unsloth():\n",
    "    \"\"\"Install Unsloth with CUDA support for RTX 3090\"\"\"\n",
    "    print(\"‚ö° Installing Unsloth for RTX 3090...\")\n",
    "    \n",
    "    commands = [\n",
    "        \"pip install unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\",\n",
    "        \"pip install --no-deps trl peft accelerate bitsandbytes\",\n",
    "        \"pip install datasets transformers torch torchvision torchaudio\",\n",
    "        \"pip install wandb tensorboard\"\n",
    "    ]\n",
    "    \n",
    "    for cmd in commands:\n",
    "        print(f\"Running: {cmd}\")\n",
    "        result = subprocess.run(cmd.split(), capture_output=True, text=True)\n",
    "        if result.returncode != 0:\n",
    "            print(f\"‚ö†Ô∏è Warning: {result.stderr}\")\n",
    "        else:\n",
    "            print(\"‚úÖ Installed successfully\")\n",
    "\n",
    "# Install Unsloth\n",
    "install_unsloth()\n",
    "print(\"üéâ Unsloth installation complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a95ab9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìö Import required libraries\n",
    "import torch\n",
    "import json\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from datasets import Dataset, load_dataset\n",
    "from transformers import TrainingArguments, DataCollatorForLanguageModeling\n",
    "from trl import SFTTrainer\n",
    "from unsloth import FastLanguageModel\n",
    "from unsloth.chat_templates import get_chat_template\n",
    "import gc\n",
    "\n",
    "print(f\"üî• PyTorch version: {torch.__version__}\")\n",
    "print(f\"üéØ CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"üöÄ GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"üíæ GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fdef6ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîß Bitcoin News Effects Training Configuration\n",
    "print(\"‚öôÔ∏è Setting up Bitcoin News Effects training configuration...\")\n",
    "\n",
    "CONFIG = {\n",
    "    # Model settings\n",
    "    \"model_name\": \"unsloth/Qwen2.5-4B-Instruct\",\n",
    "    \"max_seq_length\": 2048,\n",
    "    \"dtype\": None,  # Auto-detect\n",
    "    \"load_in_4bit\": True,\n",
    "    \n",
    "    # Dataset settings\n",
    "    \"dataset_name\": \"tahamajs/bitcoin-news-effects-dataset\",  # Update with your dataset\n",
    "    \"dataset_split\": \"train\",\n",
    "    \n",
    "    # LoRA settings\n",
    "    \"lora_rank\": 16,\n",
    "    \"lora_alpha\": 16,\n",
    "    \"lora_dropout\": 0,\n",
    "    \"target_modules\": [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    \n",
    "    # Training settings\n",
    "    \"output_dir\": \"./bitcoin_news_effects_model\",\n",
    "    \"run_name\": f\"bitcoin-news-effects-{datetime.now().strftime('%Y%m%d-%H%M%S')}\",\n",
    "    \"num_train_epochs\": 2,\n",
    "    \"per_device_train_batch_size\": 2,\n",
    "    \"gradient_accumulation_steps\": 4,\n",
    "    \"learning_rate\": 2e-4,\n",
    "    \"weight_decay\": 0.01,\n",
    "    \"lr_scheduler_type\": \"linear\",\n",
    "    \"warmup_steps\": 5,\n",
    "    \n",
    "    # Optimization settings\n",
    "    \"fp16\": not torch.cuda.is_bf16_supported() if torch.cuda.is_available() else False,\n",
    "    \"bf16\": torch.cuda.is_bf16_supported() if torch.cuda.is_available() else False,\n",
    "    \"gradient_checkpointing\": True,\n",
    "    \n",
    "    # Logging\n",
    "    \"logging_steps\": 10,\n",
    "    \"save_steps\": 100,\n",
    "    \"show_sample_every_n_steps\": 50,\n",
    "    \n",
    "    # Hub settings\n",
    "    \"push_to_hub\": False,  # Set to True if you want to push to HuggingFace\n",
    "    \"hub_model_id\": \"tahamajs/bitcoin-news-effects-qwen2.5-4b\",  # Update with your username\n",
    "}\n",
    "\n",
    "# Import is_bfloat16_supported from unsloth\n",
    "from unsloth import is_bfloat16_supported\n",
    "\n",
    "# Update precision settings\n",
    "CONFIG[\"fp16\"] = not is_bfloat16_supported()\n",
    "CONFIG[\"bf16\"] = is_bfloat16_supported()\n",
    "\n",
    "print(\"‚úÖ Configuration loaded:\")\n",
    "for key, value in CONFIG.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "# Set max_seq_length for easy access\n",
    "max_seq_length = CONFIG[\"max_seq_length\"]\n",
    "\n",
    "print(f\"\\nüéØ Training optimized for RTX 3090:\")\n",
    "print(f\"  Precision: {'BFloat16' if CONFIG['bf16'] else 'Float16'}\")\n",
    "print(f\"  Batch size: {CONFIG['per_device_train_batch_size']}\")\n",
    "print(f\"  Sequence length: {CONFIG['max_seq_length']}\")\n",
    "print(\"‚úÖ Configuration ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a4f6e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell will be moved to the proper location after model setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de83394",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ü§ñ Load Qwen2.5-4B-Instruct model with Unsloth optimization\n",
    "print(\"üöÄ Loading Qwen2.5-4B-Instruct with Unsloth acceleration...\")\n",
    "\n",
    "# Load model and tokenizer\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=CONFIG[\"model_name\"],\n",
    "    max_seq_length=CONFIG[\"max_seq_length\"],\n",
    "    dtype=CONFIG[\"dtype\"],\n",
    "    load_in_4bit=CONFIG[\"load_in_4bit\"],\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Model loaded: {CONFIG['model_name']}\")\n",
    "print(f\"üìè Max sequence length: {CONFIG['max_seq_length']}\")\n",
    "print(f\"üíæ Model dtype: {model.dtype}\")\n",
    "print(f\"üèÉ Unsloth acceleration: Active\")\n",
    "\n",
    "# Setup LoRA for efficient fine-tuning\n",
    "print(\"\\n‚ö° Adding LoRA adapters...\")\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=CONFIG[\"lora_rank\"],\n",
    "    target_modules=CONFIG[\"target_modules\"],\n",
    "    lora_alpha=CONFIG[\"lora_alpha\"],\n",
    "    lora_dropout=CONFIG[\"lora_dropout\"],\n",
    "    bias=\"none\",\n",
    "    use_gradient_checkpointing=\"unsloth\",\n",
    "    random_state=3407,\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ LoRA configured:\")\n",
    "print(f\"   Rank: {CONFIG['lora_rank']}\")\n",
    "print(f\"   Alpha: {CONFIG['lora_alpha']}\")\n",
    "print(f\"   Dropout: {CONFIG['lora_dropout']}\")\n",
    "print(f\"   Target modules: {len(CONFIG['target_modules'])} layers\")\n",
    "\n",
    "# Setup chat template for proper formatting  \n",
    "print(\"\\nüìù Setting up chat template...\")\n",
    "try:\n",
    "    tokenizer = get_chat_template(\n",
    "        tokenizer,\n",
    "        chat_template=\"qwen\",\n",
    "    )\n",
    "    print(\"‚úÖ Qwen chat template configured\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Chat template setup failed: {e}\")\n",
    "    print(\"üìù Using default tokenizer configuration\")\n",
    "\n",
    "# Ensure tokenizer has proper settings\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    print(\"‚úÖ Pad token set to EOS token\")\n",
    "\n",
    "print(\"üéâ Unsloth model setup complete!\")\n",
    "\n",
    "# Display model info\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"\\nüìä Model Statistics:\")\n",
    "print(f\"   Total parameters: {total_params:,}\")\n",
    "print(f\"   Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"   Trainable percentage: {100 * trainable_params / total_params:.2f}%\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"   GPU memory allocated: {torch.cuda.memory_allocated()/1024**3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "018e4da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset loading moved to the formatting cell to ensure proper order\n",
    "print(\" Dataset will be loaded in the formatting section...\")\n",
    "print(\"‚úÖ Proceeding to dataset formatting...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d9380e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîß Format dataset for Unsloth training with comprehensive error handling\n",
    "print(\"üîß Setting up dataset formatting...\")\n",
    "\n",
    "def formatting_prompts_func(examples):\n",
    "    \"\"\"\n",
    "    Format examples for Unsloth training with comprehensive validation\n",
    "    Prevents tensor creation errors by ensuring clean string output\n",
    "    \"\"\"\n",
    "    \n",
    "    # Handle both single examples and batched examples\n",
    "    if isinstance(examples.get(\"instruction\"), str):\n",
    "        # Single example case\n",
    "        instructions = [examples[\"instruction\"]]\n",
    "        inputs = [examples[\"input\"]]\n",
    "        outputs = [examples[\"output\"]]\n",
    "    else:\n",
    "        # Batch case\n",
    "        instructions = examples[\"instruction\"]\n",
    "        inputs = examples[\"input\"]\n",
    "        outputs = examples[\"output\"]\n",
    "    \n",
    "    # Validate we have equal lengths\n",
    "    if not (len(instructions) == len(inputs) == len(outputs)):\n",
    "        raise ValueError(f\"Length mismatch: {len(instructions)} != {len(inputs)} != {len(outputs)}\")\n",
    "    \n",
    "    formatted_texts = []\n",
    "    \n",
    "    for i in range(len(instructions)):\n",
    "        try:\n",
    "            # Extract and clean each component\n",
    "            instruction = instructions[i]\n",
    "            input_text = inputs[i]\n",
    "            output_text = outputs[i]\n",
    "            \n",
    "            # Convert everything to strings and clean\n",
    "            instruction = str(instruction).strip() if instruction is not None else \"\"\n",
    "            input_text = str(input_text).strip() if input_text is not None else \"\"\n",
    "            output_text = str(output_text).strip() if output_text is not None else \"\"\n",
    "            \n",
    "            # Skip empty samples\n",
    "            if not instruction or not input_text or not output_text:\n",
    "                print(f\"‚ö†Ô∏è Skipping empty sample {i}\")\n",
    "                continue\n",
    "            \n",
    "            # Use simple format instead of chat template to avoid nesting\n",
    "            formatted_text = f\"\"\"### Instruction:\n",
    "{instruction}\n",
    "\n",
    "### Input:\n",
    "{input_text}\n",
    "\n",
    "### Response:\n",
    "{output_text}\"\"\"\n",
    "            \n",
    "            # Ensure it's a clean string\n",
    "            if isinstance(formatted_text, str) and len(formatted_text.strip()) > 0:\n",
    "                formatted_texts.append(formatted_text)\n",
    "            else:\n",
    "                print(f\"‚ö†Ô∏è Invalid formatting result for sample {i}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error formatting sample {i}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    if not formatted_texts:\n",
    "        raise ValueError(\"‚ùå No valid formatted texts produced\")\n",
    "    \n",
    "    print(f\"‚úÖ Successfully formatted {len(formatted_texts)} samples\")\n",
    "    return {\"text\": formatted_texts}\n",
    "\n",
    "print(\"üîß Loading and processing Bitcoin News Effects Dataset...\")\n",
    "\n",
    "# Load dataset with comprehensive error handling\n",
    "dataset = None\n",
    "try:\n",
    "    # Try loading from HuggingFace first\n",
    "    print(f\"üìÅ Loading dataset: {CONFIG['dataset_name']}\")\n",
    "    dataset = load_dataset(CONFIG[\"dataset_name\"], split=CONFIG[\"dataset_split\"])\n",
    "    print(f\"‚úÖ Loaded from HuggingFace: {len(dataset)} samples\")\n",
    "    \n",
    "except Exception as hf_error:\n",
    "    print(f\"‚ö†Ô∏è HuggingFace load failed: {hf_error}\")\n",
    "    print(\"üîÑ Trying local dataset...\")\n",
    "    \n",
    "    # Fallback to local files\n",
    "    try:\n",
    "        import glob\n",
    "        local_files = glob.glob(\"../bitcoin_news_effects_dataset_*.json\")\n",
    "        if not local_files:\n",
    "            local_files = glob.glob(\"/Users/tahamajs/Documents/uni/LLM/Files/Final Project/bitcoin_news_effects_dataset_*.json\")\n",
    "        \n",
    "        if local_files:\n",
    "            latest_file = max(local_files)\n",
    "            print(f\"üìÅ Using local file: {latest_file}\")\n",
    "            \n",
    "            df = pd.read_json(latest_file)\n",
    "            dataset = Dataset.from_pandas(df)\n",
    "            print(f\"‚úÖ Loaded local dataset: {len(dataset)} samples\")\n",
    "        else:\n",
    "            raise FileNotFoundError(\"No local dataset files found\")\n",
    "            \n",
    "    except Exception as local_error:\n",
    "        print(f\"‚ùå Local load failed: {local_error}\")\n",
    "        print(\"üí° Creating sample dataset for testing...\")\n",
    "        \n",
    "        # Create minimal test dataset\n",
    "        sample_data = [\n",
    "            {\n",
    "                \"instruction\": \"Analyze Bitcoin news and predict price impact with JSON output.\",\n",
    "                \"input\": \"Bitcoin ETFs see massive $500M inflows as institutional adoption grows.\",\n",
    "                \"output\": '{\"sentiment\": \"bullish\", \"price_direction\": \"up\", \"impact_strength\": \"high\", \"confidence\": 0.85}'\n",
    "            }\n",
    "        ]\n",
    "        dataset = Dataset.from_list(sample_data)\n",
    "        print(f\"‚úÖ Created test dataset: {len(dataset)} samples\")\n",
    "\n",
    "if dataset is None:\n",
    "    raise ValueError(\"‚ùå Failed to load any dataset\")\n",
    "\n",
    "print(f\"\\nüìä Dataset Analysis:\")\n",
    "print(f\"  Total samples: {len(dataset)}\")\n",
    "print(f\"  Features: {list(dataset.features.keys())}\")\n",
    "\n",
    "# Comprehensive data cleaning and validation\n",
    "print(\"üßπ Cleaning and validating dataset...\")\n",
    "\n",
    "clean_data = []\n",
    "for i, sample in enumerate(dataset):\n",
    "    try:\n",
    "        # Extract and validate fields\n",
    "        instruction = sample.get(\"instruction\", \"\")\n",
    "        input_text = sample.get(\"input\", \"\")\n",
    "        output_text = sample.get(\"output\", \"\")\n",
    "        \n",
    "        # Clean and validate\n",
    "        instruction = str(instruction).strip() if instruction else \"\"\n",
    "        input_text = str(input_text).strip() if input_text else \"\"\n",
    "        output_text = str(output_text).strip() if output_text else \"\"\n",
    "        \n",
    "        # Only keep samples with all required content\n",
    "        if instruction and input_text and output_text:\n",
    "            clean_data.append({\n",
    "                \"instruction\": instruction,\n",
    "                \"input\": input_text,\n",
    "                \"output\": output_text\n",
    "            })\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è Skipping invalid sample {i}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error cleaning sample {i}: {e}\")\n",
    "        continue\n",
    "\n",
    "print(f\"‚úÖ Cleaned dataset: {len(clean_data)}/{len(dataset)} valid samples\")\n",
    "\n",
    "if len(clean_data) == 0:\n",
    "    raise ValueError(\"‚ùå No valid samples after cleaning\")\n",
    "\n",
    "# Create clean dataset\n",
    "dataset = Dataset.from_list(clean_data)\n",
    "print(f\"‚úÖ Dataset reconstruction complete: {len(dataset)} samples\")\n",
    "\n",
    "# Format dataset with careful batch processing\n",
    "print(\"üéØ Formatting dataset for training...\")\n",
    "\n",
    "try:\n",
    "    # Try batch formatting first\n",
    "    formatted_dataset = dataset.map(\n",
    "        formatting_prompts_func,\n",
    "        batched=True,\n",
    "        batch_size=1,  # Process one at a time to avoid issues\n",
    "        remove_columns=dataset.column_names,\n",
    "        desc=\"Formatting samples\"\n",
    "    )\n",
    "    print(f\"‚úÖ Batch formatting successful: {len(formatted_dataset)} samples\")\n",
    "    \n",
    "except Exception as batch_error:\n",
    "    print(f\"‚ö†Ô∏è Batch formatting failed: {batch_error}\")\n",
    "    print(\"üîÑ Using individual sample processing...\")\n",
    "    \n",
    "    # Fallback to individual processing\n",
    "    formatted_texts = []\n",
    "    for i, sample in enumerate(dataset):\n",
    "        try:\n",
    "            result = formatting_prompts_func(sample)\n",
    "            if \"text\" in result and result[\"text\"]:\n",
    "                # Ensure we get the actual string, not a list\n",
    "                text = result[\"text\"]\n",
    "                if isinstance(text, list):\n",
    "                    text = text[0] if text else \"\"\n",
    "                if isinstance(text, str) and text.strip():\n",
    "                    formatted_texts.append(text)\n",
    "        except Exception as sample_error:\n",
    "            print(f\"‚ö†Ô∏è Failed to format sample {i}: {sample_error}\")\n",
    "    \n",
    "    if formatted_texts:\n",
    "        formatted_dataset = Dataset.from_dict({\"text\": formatted_texts})\n",
    "        print(f\"‚úÖ Individual formatting successful: {len(formatted_dataset)} samples\")\n",
    "    else:\n",
    "        raise ValueError(\"‚ùå No samples could be formatted\")\n",
    "\n",
    "# Final validation to prevent tensor errors\n",
    "print(\"üîç Final validation...\")\n",
    "validation_errors = []\n",
    "\n",
    "for i in range(min(5, len(formatted_dataset))):\n",
    "    sample = formatted_dataset[i]\n",
    "    \n",
    "    # Check structure\n",
    "    if \"text\" not in sample:\n",
    "        validation_errors.append(f\"Sample {i}: missing 'text' key\")\n",
    "        continue\n",
    "    \n",
    "    text = sample[\"text\"]\n",
    "    \n",
    "    # Check type\n",
    "    if not isinstance(text, str):\n",
    "        validation_errors.append(f\"Sample {i}: text is {type(text)}, expected str\")\n",
    "        continue\n",
    "    \n",
    "    # Check content\n",
    "    if not text.strip():\n",
    "        validation_errors.append(f\"Sample {i}: empty text\")\n",
    "        continue\n",
    "    \n",
    "    # Test tokenization\n",
    "    try:\n",
    "        test_tokens = tokenizer(text, truncation=True, max_length=CONFIG[\"max_seq_length\"])\n",
    "        if len(test_tokens[\"input_ids\"]) == 0:\n",
    "            validation_errors.append(f\"Sample {i}: tokenization produced empty result\")\n",
    "    except Exception as token_error:\n",
    "        validation_errors.append(f\"Sample {i}: tokenization failed - {token_error}\")\n",
    "\n",
    "if validation_errors:\n",
    "    print(f\"‚ùå Validation errors found:\")\n",
    "    for error in validation_errors:\n",
    "        print(f\"  {error}\")\n",
    "    raise ValueError(\"Dataset validation failed\")\n",
    "\n",
    "print(f\"‚úÖ Dataset validation passed!\")\n",
    "print(f\"üìä Final dataset: {len(formatted_dataset)} samples ready for training\")\n",
    "\n",
    "# Show sample\n",
    "if len(formatted_dataset) > 0:\n",
    "    sample_text = formatted_dataset[0][\"text\"]\n",
    "    print(f\"\\nüìù Sample formatted text (first 300 chars):\")\n",
    "    print(sample_text[:300] + \"...\" if len(sample_text) > 300 else sample_text)\n",
    "\n",
    "print(\"‚úÖ Dataset formatting complete and validated!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14901a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìä Custom callback to monitor training outputs every 50 steps\n",
    "from transformers import TrainerCallback\n",
    "import random\n",
    "\n",
    "class NewsEffectsOutputCallback(TrainerCallback):\n",
    "    \"\"\"Monitor Bitcoin news effects predictions during training\"\"\"\n",
    "    \n",
    "    def __init__(self, model, tokenizer, dataset, show_every_n_steps=50):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.dataset = dataset\n",
    "        self.show_every_n_steps = show_every_n_steps\n",
    "        \n",
    "        # Prepare sample indices for monitoring\n",
    "        if len(dataset) > 0:\n",
    "            self.sample_indices = list(range(min(10, len(dataset))))\n",
    "            print(f\"üéØ Monitoring callback ready with {len(self.sample_indices)} samples\")\n",
    "        else:\n",
    "            self.sample_indices = []\n",
    "            print(\"‚ö†Ô∏è No samples available for monitoring\")\n",
    "    \n",
    "    def on_step_end(self, args, state, control, **kwargs):\n",
    "        if (state.global_step % self.show_every_n_steps == 0 and \n",
    "            state.global_step > 0 and \n",
    "            len(self.sample_indices) > 0):\n",
    "            \n",
    "            print(f\"\\n\" + \"=\"*80)\n",
    "            print(f\"üìä TRAINING STEP {state.global_step} - News Effects Sample Output\")\n",
    "            print(f\"‚è∞ Time: {datetime.now().strftime('%H:%M:%S')}\")\n",
    "            print(\"=\"*80)\n",
    "            \n",
    "            try:\n",
    "                # Get random sample index\n",
    "                sample_idx = random.choice(self.sample_indices)\n",
    "                \n",
    "                # Get the original sample data to reconstruct input\n",
    "                if hasattr(self.dataset, 'map'):  # If it's the original dataset\n",
    "                    # This is the formatted dataset, need to extract parts\n",
    "                    sample_text = self.dataset[sample_idx][\"text\"]\n",
    "                    \n",
    "                    # Extract instruction and input from formatted text\n",
    "                    parts = sample_text.split(\"### Input:\")\n",
    "                    if len(parts) < 2:\n",
    "                        print(\"‚ö†Ô∏è Could not parse sample for monitoring\")\n",
    "                        return\n",
    "                    \n",
    "                    instruction_part = parts[0].replace(\"### Instruction:\", \"\").strip()\n",
    "                    input_response_part = parts[1]\n",
    "                    \n",
    "                    response_split = input_response_part.split(\"### Response:\")\n",
    "                    if len(response_split) < 2:\n",
    "                        print(\"‚ö†Ô∏è Could not parse sample response\")\n",
    "                        return\n",
    "                    \n",
    "                    input_text = response_split[0].strip()\n",
    "                    expected_output = response_split[1].strip()\n",
    "                    \n",
    "                    # Create prompt for generation\n",
    "                    prompt = f\"### Instruction:\\n{instruction_part}\\n\\n### Input:\\n{input_text}\\n\\n### Response:\\n\"\n",
    "                    \n",
    "                else:\n",
    "                    print(\"‚ö†Ô∏è Unexpected dataset format for monitoring\")\n",
    "                    return\n",
    "                \n",
    "                # Tokenize for generation\n",
    "                inputs = self.tokenizer(\n",
    "                    prompt,\n",
    "                    return_tensors=\"pt\",\n",
    "                    truncation=True,\n",
    "                    max_length=1500  # Leave room for generation\n",
    "                ).to(self.model.device)\n",
    "                \n",
    "                # Generate response\n",
    "                FastLanguageModel.for_inference(self.model)\n",
    "                start_time = datetime.now()\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    outputs = self.model.generate(\n",
    "                        **inputs,\n",
    "                        max_new_tokens=200,\n",
    "                        do_sample=True,\n",
    "                        temperature=0.1,\n",
    "                        top_p=0.9,\n",
    "                        pad_token_id=self.tokenizer.eos_token_id,\n",
    "                        eos_token_id=self.tokenizer.eos_token_id\n",
    "                    )\n",
    "                \n",
    "                generation_time = datetime.now() - start_time\n",
    "                self.model.train()  # Back to training mode\n",
    "                \n",
    "                # Decode response\n",
    "                generated_text = self.tokenizer.decode(\n",
    "                    outputs[0][inputs.input_ids.shape[1]:], \n",
    "                    skip_special_tokens=True\n",
    "                ).strip()\n",
    "                \n",
    "                # Display results\n",
    "                print(f\"üì∞ Input (truncated):\")\n",
    "                print(f\"   {input_text[:150]}{'...' if len(input_text) > 150 else ''}\")\n",
    "                \n",
    "                print(f\"\\nüéØ Expected:\")\n",
    "                print(f\"   {expected_output[:100]}{'...' if len(expected_output) > 100 else ''}\")\n",
    "                \n",
    "                print(f\"\\nü§ñ Generated ({generation_time.total_seconds():.2f}s):\")\n",
    "                print(f\"   {generated_text[:200]}{'...' if len(generated_text) > 200 else ''}\")\n",
    "                \n",
    "                # Try to parse JSON from both expected and generated\n",
    "                try:\n",
    "                    import re\n",
    "                    \n",
    "                    # Parse expected\n",
    "                    expected_json = None\n",
    "                    expected_match = re.search(r'\\{.*\\}', expected_output, re.DOTALL)\n",
    "                    if expected_match:\n",
    "                        expected_json = json.loads(expected_match.group())\n",
    "                    \n",
    "                    # Parse generated\n",
    "                    generated_json = None\n",
    "                    generated_match = re.search(r'\\{.*\\}', generated_text, re.DOTALL)\n",
    "                    if generated_match:\n",
    "                        generated_json = json.loads(generated_match.group())\n",
    "                    \n",
    "                    if expected_json and generated_json:\n",
    "                        print(f\"\\nüìä Comparison:\")\n",
    "                        for key in ['sentiment', 'price_direction', 'impact_strength', 'confidence']:\n",
    "                            exp_val = expected_json.get(key, 'N/A')\n",
    "                            gen_val = generated_json.get(key, 'N/A')\n",
    "                            match = \"‚úÖ\" if exp_val == gen_val else \"‚ùå\"\n",
    "                            print(f\"   {key}: {exp_val} ‚Üí {gen_val} {match}\")\n",
    "                    \n",
    "                except Exception as parse_error:\n",
    "                    print(f\"‚ö†Ô∏è JSON parsing failed: {parse_error}\")\n",
    "                \n",
    "                print(\"=\"*80)\n",
    "                \n",
    "                # Clear memory\n",
    "                del inputs, outputs\n",
    "                torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
    "                \n",
    "            except Exception as callback_error:\n",
    "                print(f\"‚ùå Callback error: {callback_error}\")\n",
    "                print(\"=\"*80)\n",
    "\n",
    "print(\"üìä News effects monitoring callback defined\")\n",
    "print(\"   Will be initialized after dataset is ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b2d076",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-tokenize dataset and configure LM data collator to avoid 'text' nesting errors\n",
    "print(\"Pre-tokenizing formatted dataset...\")\n",
    "\n",
    "def tokenize_function(batch):\n",
    "    texts = batch[\"text\"] if isinstance(batch.get(\"text\"), list) else [batch[\"text\"]]\n",
    "    # Sanitize any problematic unicode\n",
    "    def _safe(s):\n",
    "        s = str(s)\n",
    "        try:\n",
    "            return s.encode(\"utf-8\", \"replace\").decode(\"utf-8\")\n",
    "        except Exception:\n",
    "            return str(s)\n",
    "    texts = [_safe(t) for t in texts]\n",
    "    return tokenizer(\n",
    "        texts,\n",
    "        max_length=CONFIG[\"max_seq_length\"],\n",
    "        truncation=True,\n",
    "        padding=False,\n",
    "        return_attention_mask=True,\n",
    "    )\n",
    "\n",
    "# Tokenize in batches\n",
    "tokenized_dataset = formatted_dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    desc=\"Tokenizing dataset\",\n",
    ")\n",
    "\n",
    "# Ensure raw text is not used by the collator\n",
    "if \"text\" in tokenized_dataset.column_names:\n",
    "    tokenized_dataset = tokenized_dataset.remove_columns([\"text\"])\n",
    "\n",
    "print(\"Tokenization complete. Columns:\", tokenized_dataset.column_names)\n",
    "\n",
    "# Use full tokenized dataset for training\n",
    "train_tokenized = tokenized_dataset\n",
    "\n",
    "# Configure collator for Causal LM\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "lm_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False,\n",
    ")\n",
    "print(\"Causal LM data collator configured\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83bbae7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure Unsloth SFT Trainer for RTX 3090 - Ultra Safe Mode\n",
    "print(\"Setting up Unsloth SFT Trainer in ULTRA SAFE MODE...\")\n",
    "\n",
    "# Initialize the monitoring callback now that we have model, tokenizer, and dataset\n",
    "print(\"Initializing monitoring callback...\")\n",
    "output_callback = NewsEffectsOutputCallback(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    dataset=formatted_dataset,  # keep original formatted text for readable monitoring\n",
    "    show_every_n_steps=CONFIG['show_sample_every_n_steps']\n",
    ")\n",
    "\n",
    "# Ultra-conservative training arguments for maximum stability\n",
    "print(\"Setting up ULTRA SAFE training arguments...\")\n",
    "training_args = TrainingArguments(\n",
    "    # Basic settings\n",
    "    output_dir=CONFIG[\"output_dir\"],\n",
    "    run_name=CONFIG[\"run_name\"],\n",
    "    num_train_epochs=CONFIG[\"num_train_epochs\"],\n",
    "    \n",
    "    # ULTRA CONSERVATIVE batch settings\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=8,\n",
    "    \n",
    "    # Learning rate settings\n",
    "    learning_rate=CONFIG[\"learning_rate\"],\n",
    "    weight_decay=CONFIG[\"weight_decay\"],\n",
    "    lr_scheduler_type=CONFIG[\"lr_scheduler_type\"],\n",
    "    warmup_steps=CONFIG[\"warmup_steps\"],\n",
    "    \n",
    "    # ULTRA SAFE memory optimization\n",
    "    fp16=CONFIG[\"fp16\"],\n",
    "    bf16=CONFIG[\"bf16\"],\n",
    "    gradient_checkpointing=CONFIG[\"gradient_checkpointing\"],\n",
    "    dataloader_num_workers=0,\n",
    "    \n",
    "    # ULTRA SAFE data handling\n",
    "    remove_unused_columns=False,\n",
    "    dataloader_pin_memory=False,\n",
    "    group_by_length=False,\n",
    "    \n",
    "    # Logging and saving\n",
    "    logging_steps=CONFIG[\"logging_steps\"],\n",
    "    save_steps=CONFIG[\"save_steps\"],\n",
    "    save_total_limit=1,\n",
    "    \n",
    "    # Disable complex features that could cause issues\n",
    "    eval_strategy=\"no\",\n",
    "    save_strategy=\"steps\",\n",
    "    prediction_loss_only=True,\n",
    "    \n",
    "    # Hub settings\n",
    "    push_to_hub=False,\n",
    "    \n",
    "    # ULTRA SAFE misc settings\n",
    "    seed=42,\n",
    "    data_seed=42,\n",
    "    report_to=\"none\",\n",
    "    ignore_data_skip=True,\n",
    "    disable_tqdm=False,\n",
    ")\n",
    "\n",
    "print(\"Ultra-safe training arguments configured\")\n",
    "\n",
    "# FINAL pre-training validation with actual trainer format\n",
    "print(\"FINAL pre-training validation...\")\n",
    "\n",
    "# Test exactly what the trainer will do\n",
    "print(\"Testing EXACT trainer data processing...\")\n",
    "try:\n",
    "    # Get samples in the exact format the trainer uses\n",
    "    sample_texts = [formatted_dataset[i][\"text\"] for i in range(min(2, len(formatted_dataset)))]\n",
    "    \n",
    "    print(f\"Testing {len(sample_texts)} samples:\")\n",
    "    for i, text in enumerate(sample_texts):\n",
    "        s = str(text)\n",
    "        try:\n",
    "            s = s.encode(\"utf-8\", \"replace\").decode(\"utf-8\")\n",
    "        except Exception:\n",
    "            pass\n",
    "        print(f\"   Sample {i}: type={type(text)}, len={len(s)}\")\n",
    "        \n",
    "        # Test individual tokenization\n",
    "        individual_tokens = tokenizer(\n",
    "            s,\n",
    "            truncation=True,\n",
    "            max_length=CONFIG[\"max_seq_length\"],\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        print(f\"   Sample {i} individual: OK {individual_tokens['input_ids'].shape}\")\n",
    "        del individual_tokens\n",
    "    \n",
    "    # Test batch tokenization (the critical operation)\n",
    "    print(\"Testing critical batch tokenization...\")\n",
    "    batch_tokens = tokenizer(\n",
    "        sample_texts,\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        max_length=CONFIG[\"max_seq_length\"],\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    \n",
    "    print(\"CRITICAL TEST PASSED!\")\n",
    "    print(f\"   Batch shape: {batch_tokens['input_ids'].shape}\")\n",
    "    print(f\"   Batch dtype: {batch_tokens['input_ids'].dtype}\")\n",
    "    print(f\"   Min token: {batch_tokens['input_ids'].min()}\")\n",
    "    print(f\"   Max token: {batch_tokens['input_ids'].max()}\")\n",
    "    \n",
    "    # Validate tensor properties\n",
    "    assert batch_tokens['input_ids'].dim() == 2\n",
    "    assert batch_tokens['attention_mask'].dim() == 2\n",
    "    assert batch_tokens['input_ids'].shape == batch_tokens['attention_mask'].shape\n",
    "    \n",
    "    print(\"All tensor validations passed!\")\n",
    "    \n",
    "    # Clean up\n",
    "    del batch_tokens, sample_texts\n",
    "    torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
    "    \n",
    "except Exception as critical_error:\n",
    "    print(f\"CRITICAL ERROR: Final validation failed: {critical_error}\")\n",
    "    print(\"This means the trainer WILL fail\")\n",
    "    raise critical_error\n",
    "\n",
    "# Create ULTRA SAFE SFT Trainer that uses tokenized dataset and LM collator\n",
    "print(\"Initializing ULTRA SAFE SFT Trainer...\")\n",
    "try:\n",
    "    trainer = SFTTrainer(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        train_dataset=train_tokenized,\n",
    "        dataset_text_field=None,\n",
    "        max_seq_length=CONFIG[\"max_seq_length\"],\n",
    "        dataset_num_proc=1,\n",
    "        packing=False,\n",
    "        data_collator=lm_collator,\n",
    "        args=training_args,\n",
    "        callbacks=[output_callback],\n",
    "    )\n",
    "    \n",
    "    print(\"ULTRA SAFE SFT Trainer initialized successfully!\")\n",
    "    \n",
    "    # Validate trainer state\n",
    "    print(\"Trainer validation:\")\n",
    "    print(f\"   Dataset size: {len(trainer.train_dataset)}\")\n",
    "    print(f\"   Model device: {trainer.model.device}\")\n",
    "    print(f\"   Tokenizer pad token: {trainer.tokenizer.pad_token}\")\n",
    "    \n",
    "except Exception as trainer_error:\n",
    "    print(f\"ULTRA SAFE trainer initialization FAILED: {trainer_error}\")\n",
    "    raise trainer_error\n",
    "\n",
    "print(\"\")\n",
    "print(\"ULTRA SAFE TRAINER SUMMARY:\")\n",
    "print(f\"   Dataset: {len(train_tokenized)} samples\")\n",
    "print(f\"   Epochs: {CONFIG['num_train_epochs']}\")\n",
    "print(\"   Batch size: 1 (ultra-conservative)\")\n",
    "print(f\"   Effective batch: {1 * 8} (with accumulation)\")\n",
    "print(f\"   Precision: {'BF16' if CONFIG['bf16'] else 'FP16'}\")\n",
    "print(\"   Safety mode: MAXIMUM\")\n",
    "print(\"   Tensor errors: Mitigated by pre-tokenization\")\n",
    "print(\"Ready for ULTRA SAFE training!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad010ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start Unsloth Training with Real-time News Effects Monitoring\n",
    "print(\"STARTING BITCOIN NEWS EFFECTS TRAINING WITH UNSLOTH!\")\n",
    "print(\"=\"*80)\n",
    "print(\"Task: Training model to analyze Bitcoin news effects\")\n",
    "print(\"Model: Qwen2.5-4B-Instruct with LoRA\")\n",
    "print(\"Acceleration: Unsloth (2x faster)\")\n",
    "print(\"GPU: RTX 3090 optimized\")\n",
    "print(f\"Dataset (tokenized): {len(train_tokenized)} samples\")\n",
    "print(f\"Monitoring: News effects outputs every {CONFIG['show_sample_every_n_steps']} steps\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Record start time\n",
    "training_start_time = datetime.now()\n",
    "print(f\"Training started at: {training_start_time.strftime('%H:%M:%S')}\")\n",
    "\n",
    "# Start training with enhanced error handling\n",
    "try:\n",
    "    print(\"\\nStarting training...\")\n",
    "    trainer_stats = trainer.train()\n",
    "    \n",
    "    # Training completed successfully\n",
    "    training_end_time = datetime.now()\n",
    "    training_duration = training_end_time - training_start_time\n",
    "    \n",
    "    print(f\"\\n\" + \"=\"*80)\n",
    "    print(\"BITCOIN NEWS EFFECTS TRAINING COMPLETED!\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Total training time: {training_duration}\")\n",
    "    print(f\"Final train loss: {trainer_stats.training_loss:.4f}\")\n",
    "    print(f\"Training speed: {trainer_stats.metrics.get('train_samples_per_second', 'N/A')} samples/sec\")\n",
    "    print(\"Unsloth acceleration: ~2x faster than standard training\")\n",
    "    print(\"Task completed: Bitcoin news effects analysis model\")\n",
    "    \n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\nTraining interrupted by user\")\n",
    "    print(f\"Training duration: {datetime.now() - training_start_time}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\nTraining error: {e}\")\n",
    "    print(f\"Training duration: {datetime.now() - training_start_time}\")\n",
    "    \n",
    "    # Detailed error analysis\n",
    "    import traceback\n",
    "    print(\"\\nError details:\")\n",
    "    print(f\"   - Error type: {type(e).__name__}\")\n",
    "    print(f\"   - Error message: {str(e)}\")\n",
    "    \n",
    "    # Check for tensor errors\n",
    "    error_str = str(e).lower()\n",
    "    if (\"too many dimensions\" in error_str) or (\"str\" in error_str and \"tensor\" in error_str):\n",
    "        print(\"\\nDetected tensor formatting error!\")\n",
    "        print(\"   This suggests the dataset still contains nested structures or wrong data types\")\n",
    "        print(\"   - Verify all features are tokenized (input_ids, attention_mask)\")\n",
    "        print(\"   - Ensure no 'text' column remains in train_tokenized\")\n",
    "        print(\"   - Re-run the tokenization cell\")\n",
    "    \n",
    "    print(\"\\nFull traceback:\")\n",
    "    traceback.print_exc()\n",
    "    raise\n",
    "\n",
    "print(\"\\nTraining statistics saved to trainer object\")\n",
    "print(\"Ready for model saving and testing!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fca5e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üíæ Save Bitcoin News Effects Model in Multiple Formats\n",
    "print(\"üíæ Saving Bitcoin News Effects Model...\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Save LoRA adapters\n",
    "print(\"1Ô∏è‚É£ Saving LoRA adapters...\")\n",
    "lora_dir = CONFIG[\"output_dir\"] + \"/lora_adapters\"\n",
    "model.save_pretrained(lora_dir)\n",
    "tokenizer.save_pretrained(lora_dir)\n",
    "print(f\"‚úÖ LoRA adapters saved to: {lora_dir}\")\n",
    "\n",
    "# Save merged model (16-bit)\n",
    "print(\"\\n2Ô∏è‚É£ Saving merged model (16-bit)...\")\n",
    "merged_dir = CONFIG[\"output_dir\"] + \"/merged_model\"\n",
    "model.save_pretrained_merged(merged_dir, tokenizer, save_method=\"merged_16bit\")\n",
    "print(f\"‚úÖ Merged 16-bit model saved to: {merged_dir}\")\n",
    "\n",
    "# Save GGUF format for deployment\n",
    "print(\"\\n3Ô∏è‚É£ Saving GGUF format for deployment...\")\n",
    "gguf_dir = CONFIG[\"output_dir\"] + \"/gguf_model\"\n",
    "try:\n",
    "    model.save_pretrained_gguf(gguf_dir, tokenizer, quantization_method=\"q4_k_m\")\n",
    "    print(f\"‚úÖ GGUF model saved to: {gguf_dir}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è GGUF save failed: {e}\")\n",
    "    print(\"üí° GGUF format requires additional dependencies\")\n",
    "\n",
    "# Push to HuggingFace Hub\n",
    "if CONFIG[\"push_to_hub\"] and CONFIG.get(\"hub_model_id\"):\n",
    "    print(f\"\\n4Ô∏è‚É£ Pushing to HuggingFace Hub...\")\n",
    "    try:\n",
    "        # Push LoRA model\n",
    "        model.push_to_hub_merged(\n",
    "            CONFIG[\"hub_model_id\"],\n",
    "            tokenizer,\n",
    "            save_method=\"merged_16bit\",\n",
    "            commit_message=f\"Bitcoin news effects analysis model - {datetime.now().strftime('%Y-%m-%d %H:%M')}\"\n",
    "        )\n",
    "        print(f\"‚úÖ Model pushed to: https://huggingface.co/{CONFIG['hub_model_id']}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Hub push failed: {e}\")\n",
    "\n",
    "print(f\"\\nüéâ All model formats saved successfully!\")\n",
    "print(f\"üìÅ Base directory: {CONFIG['output_dir']}\")\n",
    "print(f\"üéØ Model purpose: Bitcoin news effects analysis\")\n",
    "print(f\"‚ö° Training method: Unsloth + LoRA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d95d1e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üß™ Test Bitcoin News Effects Model with Real Sample\n",
    "def test_news_effects_model():\n",
    "    print(\"üß™ Testing Bitcoin News Effects Model\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Enable fast inference mode\n",
    "    FastLanguageModel.for_inference(model)\n",
    "    \n",
    "    # Sample Bitcoin news for testing\n",
    "    test_instruction = \"\"\"Analyze Bitcoin news and predict price impact. Return JSON with this exact structure:\n",
    "\n",
    "{\n",
    "  \"sentiment\": \"bullish|neutral|bearish\",\n",
    "  \"price_direction\": \"up|sideways|down\",\n",
    "  \"impact_strength\": \"high|medium|low\", \n",
    "  \"timeframe\": \"immediate|short_term|medium_term\",\n",
    "  \"confidence\": 0.75,\n",
    "  \"key_reason\": \"Brief explanation of main factor\"\n",
    "}\"\"\"\n",
    "    \n",
    "    test_input = \"\"\"News Title: Bitcoin ETFs See Record $1.2B Inflows as Price Breaks $65,000\n",
    "\n",
    "News Summary: Bitcoin spot ETFs experienced unprecedented institutional demand with $1.2 billion in net inflows over the past week, driving Bitcoin price above $65,000 for the first time since November 2021. BlackRock's IBIT led with $800M inflows while Fidelity's FBTC added $400M. The surge coincides with growing corporate adoption and favorable regulatory signals from the SEC.\n",
    "\n",
    "Impact Tags: institutional_adoption, etf_flows, price_breakout, regulatory_clarity\n",
    "\n",
    "Market Context:\n",
    "Bull 75% | Base 20% | Bear 5%\n",
    "\n",
    "Daily Recommendations:\n",
    "Short-term: Strong Buy\n",
    "Long-term: Buy\"\"\"\n",
    "    \n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": test_instruction},\n",
    "        {\"role\": \"user\", \"content\": test_input}\n",
    "    ]\n",
    "    \n",
    "    # Use Unsloth optimized generation\n",
    "    inputs = tokenizer.apply_chat_template(\n",
    "        messages, \n",
    "        tokenize=True, \n",
    "        add_generation_prompt=True,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(model.device)\n",
    "    \n",
    "    print(\"üì∞ Analyzing Bitcoin news...\")\n",
    "    \n",
    "    # Generate news effects analysis\n",
    "    start_time = datetime.now()\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            input_ids=inputs,\n",
    "            max_new_tokens=250,\n",
    "            do_sample=True,\n",
    "            temperature=0.1,\n",
    "            top_p=0.9,\n",
    "            use_cache=True,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    generation_time = datetime.now() - start_time\n",
    "    response = tokenizer.decode(outputs[0][inputs.shape[-1]:], skip_special_tokens=True)\n",
    "    \n",
    "    print(f\"\\n‚ö° Analysis completed in: {generation_time.total_seconds():.2f} seconds\")\n",
    "    print(\"\\nüì§ Bitcoin News Effects Analysis:\")\n",
    "    print(response.strip())\n",
    "    \n",
    "    # Try to parse JSON\n",
    "    try:\n",
    "        import re\n",
    "        json_match = re.search(r'\\{.*\\}', response.strip(), re.DOTALL)\n",
    "        if json_match:\n",
    "            json_str = json_match.group()\n",
    "            parsed = json.loads(json_str)\n",
    "            print(\"\\n‚úÖ News Effects Analysis (Parsed):\")\n",
    "            print(f\"  üìä Sentiment: {parsed.get('sentiment', 'N/A')}\")\n",
    "            print(f\"  üìà Price Direction: {parsed.get('price_direction', 'N/A')}\")\n",
    "            print(f\"  üí™ Impact Strength: {parsed.get('impact_strength', 'N/A')}\")\n",
    "            print(f\"  ‚è∞ Timeframe: {parsed.get('timeframe', 'N/A')}\")\n",
    "            print(f\"  üéØ Confidence: {parsed.get('confidence', 0):.2f}\")\n",
    "            print(f\"  üí° Key Reason: {parsed.get('key_reason', 'N/A')}\")\n",
    "        else:\n",
    "            print(\"\\n‚ö†Ô∏è No JSON found in response\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå JSON parsing failed: {e}\")\n",
    "    \n",
    "    # Switch back to training mode if needed\n",
    "    model.train()\n",
    "\n",
    "print(\"üöÄ Testing Bitcoin News Effects Model...\")\n",
    "test_news_effects_model()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üéâ BITCOIN NEWS EFFECTS TRAINING COMPLETE!\")\n",
    "print(\"=\"*80)\n",
    "print(f\"‚úÖ Model saved to: {CONFIG['output_dir']}\")\n",
    "print(f\"‚úÖ Task: Bitcoin news effects analysis\")\n",
    "print(f\"‚úÖ Training optimized with Unsloth (2x faster)\")\n",
    "print(f\"‚úÖ RTX 3090 memory optimized\")\n",
    "print(f\"‚úÖ Real-time monitoring every {CONFIG['show_sample_every_n_steps']} steps\")\n",
    "print(f\"‚úÖ Multiple export formats available\")\n",
    "if CONFIG.get('hub_model_id'):\n",
    "    print(f\"‚úÖ Model available at: https://huggingface.co/{CONFIG['hub_model_id']}\")\n",
    "print(\"=\"*80)\n",
    "print(\"üéØ Model ready for Bitcoin news effects analysis!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
