{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b7eb871",
   "metadata": {},
   "source": [
    "# Unsloth GRPO Training for Bitcoin Enhanced Prediction\n",
    "\n",
    "This notebook implements Group Relative Policy Optimization (GRPO) using Unsloth for comprehensive Bitcoin prediction.\n",
    "\n",
    "**Dataset**: `bitcoin-enhanced-prediction-dataset-with-local-comprehensive-news`\n",
    "\n",
    "**Training Method**: Unsloth GRPO\n",
    "- Built-in preference learning optimization\n",
    "- Efficient memory usage with Unsloth\n",
    "- Streamlined training pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c992054f",
   "metadata": {},
   "source": [
    "## Install Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd8a9cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
    "# !pip install -U xformers trl peft accelerate bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "532b37b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure protobuf uses pure-Python implementation early to avoid descriptor errors\n",
    "# import os as _osThe code snippet you provided is setting environment variables using the `os.environ.setdefault()` method.\n",
    "\n",
    "# _os.environ.setdefault(\"PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION\", \"python\")\n",
    "# _os.environ.setdefault(\"TOKENIZERS_PARALLELISM\", \"false\")\n",
    "\n",
    "# !pip install -U \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
    "# !pip install -U xformers trl peft accelerate bitsandbytes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "393dee77",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7173964b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from unsloth import FastLanguageModel, is_bfloat16_supported\n",
    "# from unsloth.chat_templates import get_chat_template\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import TrainingArguments, AutoTokenizer, AutoModel\n",
    "from peft import PeftModel\n",
    "import torch, random, os\n",
    "import json\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "# Try to import GRPO classes, with fallback to available alternatives\n",
    "try:\n",
    "    from trl import GRPOTrainer, GRPOConfig\n",
    "    GRPO_AVAILABLE = True\n",
    "    print(\"‚úÖ GRPO classes imported successfully\")\n",
    "except ImportError:\n",
    "    try:\n",
    "        # Try alternative imports (GRPO might be under different names)\n",
    "        from trl import PPOTrainer, PPOConfig\n",
    "        from trl import SFTTrainer, SFTConfig\n",
    "        GRPO_AVAILABLE = False\n",
    "        print(\"‚ö†Ô∏è GRPOTrainer not found, will use SFTTrainer as fallback\")\n",
    "    except ImportError:\n",
    "        # Last resort - use basic trainer\n",
    "        from transformers import Trainer\n",
    "        GRPO_AVAILABLE = False\n",
    "        print(\"‚ö†Ô∏è Advanced TRL classes not found, using basic Trainer\")\n",
    "\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae6f1a9",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a336c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model configuration\n",
    "BASE_MODEL_NAME = \"./Qwen3-8B\"  # Preferred local base model (if available)\n",
    "FALLBACK_MODEL_NAME = \"unsloth/Qwen2.5-7B-Instruct-bnb-4bit\"  # Fallback HF model if local path missing\n",
    "ADAPTER_PATH = \"./my-awesome-model_final_bitcoin-enhanced-prediction-dataset-with-local-comprehensive-news-v2\"  # Pre-trained adapter (folder)\n",
    "CHECKPOINT = \"checkpoint-400\"  # Specific checkpoint within adapter folder\n",
    "MAX_SEQ_LENGTH = 2048\n",
    "DTYPE = torch.bfloat16  # Auto-detection\n",
    "LOAD_IN_4BIT = True\n",
    "\n",
    "# LoRA configuration\n",
    "LORA_R = 32\n",
    "LORA_ALPHA = 32\n",
    "LORA_DROPOUT = 0.0\n",
    "TARGET_MODULES = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"]\n",
    "\n",
    "# GRPO configuration\n",
    "OUTPUT_DIR = \"./qwen_bitcoin_enhanced_grpo_unsloth_pretrained_from_sft\"\n",
    "LEARNING_RATE = 3e-7  # Lower for pre-trained model\n",
    "NUM_TRAIN_EPOCHS = 1\n",
    "PER_DEVICE_TRAIN_BATCH_SIZE = 1\n",
    "GRADIENT_ACCUMULATION_STEPS = 8\n",
    "MAX_LENGTH = 1024\n",
    "MAX_PROMPT_LENGTH = 512\n",
    "BETA = 0.1\n",
    "\n",
    "# Quick sanity run controls (useful for CI or first launch)\n",
    "SANITY_RUN = False            # Set True to run a very short sanity training\n",
    "SANITY_MAX_STEPS = 30         # Number of steps for sanity run\n",
    "SANITY_DATASET_SIZE = 256     # Subset size for sanity run\n",
    "\n",
    "# Dataset\n",
    "DATASET_NAME = \"tahamajs/bitcoin-enhanced-prediction-dataset-with-local-comprehensive-news\"\n",
    "\n",
    "# Reward model for comprehensive analysis\n",
    "REWARD_MODEL_NAME = \"microsoft/DialoGPT-medium\"  # Good for conversational quality assessment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32b42590",
   "metadata": {},
   "source": [
    "## Load Model and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f29f5123",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from pathlib import Path\n",
    "from peft import PeftModel\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from unsloth import FastLanguageModel, get_chat_template\n",
    "\n",
    "# # --- User-defined variables ---\n",
    "# BASE_MODEL_NAME = \"unsloth/mistral-7b-instruct-v0.2-bnb-4bit\"\n",
    "# FALLBACK_MODEL_NAME = \"mistralai/Mistral-7b-Instruct-v0.2\"\n",
    "# ADAPTER_PATH = \"path/to/your/first/adapter\"  # <<-- IMPORTANT: Set this path\n",
    "# CHECKPOINT = \"checkpoint-final\"\n",
    "# MAX_SEQ_LENGTH = 2048\n",
    "# DTYPE = torch.bfloat16  # Set the desired data type here\n",
    "# LOAD_IN_4BIT = True\n",
    "# LORA_R = 16\n",
    "# TARGET_MODULES = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"]\n",
    "# LORA_ALPHA = 16\n",
    "# LORA_DROPOUT = 0\n",
    "# SEED = 3407\n",
    "# REWARD_MODEL_NAME = \"starling-lm/reward-model\"\n",
    "# # --- End of user-defined variables ---\n",
    "\n",
    "\n",
    "preferred_path = Path(BASE_MODEL_NAME)\n",
    "chosen_model_name = BASE_MODEL_NAME if preferred_path.exists() else FALLBACK_MODEL_NAME\n",
    "if chosen_model_name != BASE_MODEL_NAME:\n",
    "    print(f\"‚ÑπÔ∏è Local model path not found at {BASE_MODEL_NAME}. Falling back to {FALLBACK_MODEL_NAME}\")\n",
    "\n",
    "print(f\"üîÑ Loading base model: {chosen_model_name}\")\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=chosen_model_name,\n",
    "    max_seq_length=MAX_SEQ_LENGTH,\n",
    "    dtype=DTYPE,  # Use the configured DTYPE variable\n",
    "    load_in_4bit=LOAD_IN_4BIT,\n",
    ")\n",
    "\n",
    "# Ensure pad token exists and align embeddings to tokenizer size\n",
    "if getattr(tokenizer, \"pad_token\", None) is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Load pre-trained adapter with vocab-size auto-alignment on mismatch\n",
    "first_adapter_loaded = False\n",
    "adapter_path = f\"{ADAPTER_PATH}/{CHECKPOINT}\"\n",
    "if not Path(adapter_path).exists():\n",
    "    print(f\"‚ÑπÔ∏è First adapter checkpoint not found at {adapter_path}. Skipping adapter load.\")\n",
    "else:\n",
    "    print(f\"üîÑ Loading pre-trained adapter: {adapter_path}\")\n",
    "    try:\n",
    "        # Load the first adapter without merging\n",
    "        model = PeftModel.from_pretrained(model, adapter_path)\n",
    "        first_adapter_loaded = True\n",
    "        print(f\"‚úÖ Successfully loaded adapter from {adapter_path}\")\n",
    "    except Exception as e:\n",
    "        err = str(e)\n",
    "        print(f\"‚ö†Ô∏è Could not load adapter on first try: {e}\")\n",
    "\n",
    "        # Try to detect and fix vocab size mismatch\n",
    "        import re\n",
    "        try:\n",
    "            expected_match = re.search(r\"copying a param with shape torch\\.Size\\(\\[(\\d+),\", err)\n",
    "            current_match = re.search(r\"current model is torch\\.Size\\(\\[(\\d+),\", err)\n",
    "            if expected_match and current_match:\n",
    "                expected = int(expected_match.group(1))\n",
    "                current = int(current_match.group(1))\n",
    "                missing = expected - current\n",
    "                if missing > 0:\n",
    "                    print(f\"üîß Detected vocab mismatch. Adding {missing} special token(s) to align.\")\n",
    "                    extra_tokens = [f\"<|extra_{i}|>\" for i in range(missing)]\n",
    "                    tokenizer.add_special_tokens({\"additional_special_tokens\": extra_tokens})\n",
    "                    model.resize_token_embeddings(len(tokenizer))\n",
    "                    model = PeftModel.from_pretrained(model, adapter_path)\n",
    "                    first_adapter_loaded = True\n",
    "                    print(f\"‚úÖ Adapter loaded after aligning vocab size to {len(tokenizer)}\")\n",
    "                else:\n",
    "                     print(\"‚ÑπÔ∏è Vocab sizes match but other mismatch detected. Proceeding without adapter.\")\n",
    "            else:\n",
    "                print(\"‚ÑπÔ∏è No clear vocab mismatch pattern found. Proceeding without adapter.\")\n",
    "        except Exception as e2:\n",
    "            print(f\"‚ö†Ô∏è Auto-alignment failed: {e2}. Proceeding without adapter.\")\n",
    "\n",
    "# ‚ö†Ô∏è IMPORTANT: Merging into a 4-bit model is not recommended.\n",
    "# Instead of merging, we keep the first adapter loaded and active.\n",
    "# The new LoRA adapter for training will be applied on top.\n",
    "if first_adapter_loaded:\n",
    "    print(\"‚úÖ First adapter is loaded and active. Skipping merge step for 4-bit model.\")\n",
    "\n",
    "# Apply chat template\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template=\"chatml\",\n",
    ")\n",
    "\n",
    "# Prepare model for new LoRA adapter training\n",
    "print(\"üîß Preparing model for new LoRA adapter training...\")\n",
    "try:\n",
    "    # This will add a *new* adapter for training, while keeping the first one active.\n",
    "    model = FastLanguageModel.get_peft_model(\n",
    "        model,\n",
    "        r=LORA_R,\n",
    "        target_modules=TARGET_MODULES,\n",
    "        lora_alpha=LORA_ALPHA,\n",
    "        lora_dropout=LORA_DROPOUT,\n",
    "        bias=\"none\",\n",
    "        use_gradient_checkpointing=\"unsloth\",\n",
    "        random_state=SEED,\n",
    "        use_rslora=False,\n",
    "        loftq_config=None,\n",
    "    )\n",
    "    print(\"‚úÖ New LoRA adapter initialized successfully for training\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Error initializing new LoRA adapter: {e}\")\n",
    "    print(\"‚ÑπÔ∏è Continuing with current model state...\")\n",
    "\n",
    "# Load reward model with consistent data type\n",
    "print(f\"\\nüîÑ Loading reward model: {REWARD_MODEL_NAME}\")\n",
    "try:\n",
    "    reward_tokenizer = AutoTokenizer.from_pretrained(REWARD_MODEL_NAME)\n",
    "    reward_model = AutoModel.from_pretrained(\n",
    "        REWARD_MODEL_NAME,\n",
    "        torch_dtype=DTYPE, # Load in the same precision to save memory\n",
    "        device_map=\"auto\",\n",
    "    )\n",
    "    reward_model.eval()\n",
    "    print(f\"‚úÖ Reward model loaded successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Could not load reward model, using rule-based rewards: {e}\")\n",
    "    reward_model = None\n",
    "    reward_tokenizer = None\n",
    "\n",
    "print(f\"\\nüìä Model Configuration:\")\n",
    "print(f\"  Base model: {chosen_model_name}\")\n",
    "print(f\"  First adapter (loaded, not merged): {adapter_path if first_adapter_loaded else 'Not loaded'}\")\n",
    "print(f\"  New LoRA adapter initialized for training with rank: {LORA_R}\")\n",
    "print(f\"  Max sequence length: {MAX_SEQ_LENGTH}\")\n",
    "print(f\"  Load in 4bit: {LOAD_IN_4BIT}\")\n",
    "print(f\"  Data type: {DTYPE}\")\n",
    "print(f\"  Reward model: {REWARD_MODEL_NAME if reward_model else 'Rule-based only'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2263607f",
   "metadata": {},
   "source": [
    "## Load and Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c2c24b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "dataset = load_dataset(DATASET_NAME, split=\"train\")\n",
    "print(f\"Dataset loaded: {DATASET_NAME}\")\n",
    "print(f\"Total samples: {len(dataset):,}\")\n",
    "\n",
    "# Show sample\n",
    "print(\"\\n=== Sample Data ===\")\n",
    "sample = dataset[0]\n",
    "for key, value in sample.items():\n",
    "    print(f\"{key}: {str(value)[:150]}{'...' if len(str(value)) > 150 else ''}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c2ffd99",
   "metadata": {},
   "source": [
    "## Format Dataset for GRPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea341fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Assume 'dataset' is your loaded dataset\n",
    "# dataset = load_dataset(...) \n",
    "\n",
    "# Assume you have your model's max length\n",
    "# For example, for Llama 3 it's 8192\n",
    "MAX_LENGTH = 2048 \n",
    "\n",
    "# ==============================================================================\n",
    "# STEP 1: Format the text prompts (Your original code - this is perfect!)\n",
    "# ==============================================================================\n",
    "def formatting_prompts_func(examples):\n",
    "    \"\"\"\n",
    "    Format examples for GRPO training.\n",
    "    GRPOTrainer expects a single 'prompt' per sample and generates completions internally.\n",
    "    The reference answer is saved in a 'target' column for offline evaluation.\n",
    "    \"\"\"\n",
    "    instructions = examples.get(\"instruction\", [\"\"] * len(examples.get(\"input\", [])))\n",
    "    inputs = examples.get(\"input\", [])\n",
    "    outputs = examples.get(\"output\", [])\n",
    "\n",
    "    prompts = []\n",
    "    targets = []\n",
    "    for instruction, user_input, output in zip(instructions, inputs, outputs):\n",
    "        system_msg = instruction or \"You are a helpful Bitcoin market analyst.\"\n",
    "        user_msg = user_input or \"\"\n",
    "        # Build prompt ending right before the assistant's turn\n",
    "        prompt = (\n",
    "            f\"<|im_start|>system\\n{system_msg}<|im_end|>\\n\"\n",
    "            f\"<|im_start|>user\\n{user_msg}<|im_end|>\\n\"\n",
    "            f\"<|im_start|>assistant\\n\"\n",
    "        )\n",
    "        prompts.append(prompt)\n",
    "        # Store the ground truth completion separately\n",
    "        targets.append((output or \"\") + \"<|im_end|>\")\n",
    "        \n",
    "    return {\"prompt\": prompts, \"target\": targets}\n",
    "\n",
    "print(\"üìù Formatting dataset for Unsloth GRPO (prompt-only mode)...\")\n",
    "formatted_dataset = dataset.map(\n",
    "    formatting_prompts_func,\n",
    "    batched=True,\n",
    "    remove_columns=dataset.column_names,\n",
    "    desc=\"Formatting prompts\"\n",
    ")\n",
    "\n",
    "# Keep a separate copy of targets for evaluation, then remove from the training set\n",
    "reference_targets = formatted_dataset[\"target\"]\n",
    "formatted_dataset = formatted_dataset.remove_columns([\"target\"])\n",
    "\n",
    "print(f\"Formatted dataset samples: {len(formatted_dataset):,}\")\n",
    "print(\"Columns after formatting:\", formatted_dataset.column_names)\n",
    "\n",
    "print(\"\\n=== Formatted Sample Prompt (Text) ===\")\n",
    "print(formatted_dataset[0]['prompt'][:500])\n",
    "print(\"‚úÖ Dataset text formatting complete.\")\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# STEP 2: Tokenize the formatted prompts (The required fix)\n",
    "# ==============================================================================\n",
    "# Make sure to use the correct model name for your tokenizer\n",
    "# Since you use Unsloth and ChatML format, a model like Mistral-Instruct is a good guess\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"unsloth/mistral-7b-instruct-v0.2-bnb-4bit\")\n",
    "\n",
    "# Define the tokenization function\n",
    "def tokenize_function(examples):\n",
    "    # This will process the text in the 'prompt' column\n",
    "    tokenized_output = tokenizer(\n",
    "        examples[\"prompt\"],\n",
    "        truncation=True,\n",
    "        max_length=2048,\n",
    "    )\n",
    "    # Create the 'labels' column by cloning 'input_ids'\n",
    "    tokenized_output[\"labels\"] = tokenized_output[\"input_ids\"][:]\n",
    "    return tokenized_output\n",
    "\n",
    "print(\"\\n‚ö° Tokenizing the dataset...\")\n",
    "tokenized_dataset = formatted_dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=[\"prompt\"] # It's good practice to remove the old column\n",
    ")\n",
    "\n",
    "print(f\"Tokenized dataset samples: {len(tokenized_dataset):,}\")\n",
    "print(\"‚úÖ Final columns passed to trainer:\", tokenized_dataset.column_names)\n",
    "\n",
    "# Now, 'tokenized_dataset' is ready to be passed to your trainer or DataLoader.\n",
    "# It contains 'input_ids' and 'attention_mask', which is exactly what the\n",
    "# data collator expects, and this will solve the ValueError."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f158c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a simple identity collator so Trainer doesn't try to pad raw text\n",
    "class IdentityCollator:\n",
    "    \"\"\"Pass-through collator for raw prompt samples.\n",
    "    Returns list[dict] unchanged so GRPOTrainer can tokenize internally.\"\"\"\n",
    "    def __call__(self, features):\n",
    "        return features\n",
    "\n",
    "raw_text_collator = IdentityCollator()\n",
    "\n",
    "# Quick sanity check: show keys of first sample\n",
    "first = formatted_dataset[0]\n",
    "print(\"üîç Sample keys:\", list(first.keys()))\n",
    "print(\"Prompt length:\", len(first['prompt']))\n",
    "\n",
    "# Optional: truncate very long samples (safeguard)\n",
    "MAX_PROMPT_CHARS = 4000\n",
    "if any(len(r['prompt']) > MAX_PROMPT_CHARS for r in formatted_dataset.select(range(min(50, len(formatted_dataset))))):\n",
    "    def truncate_func(examples):\n",
    "        prompts = []\n",
    "        for p in examples['prompt']:\n",
    "            if len(p) > MAX_PROMPT_CHARS:\n",
    "                p = p[:MAX_PROMPT_CHARS] + \"...\"\n",
    "            prompts.append(p)\n",
    "        return {\"prompt\": prompts}\n",
    "    print(\"‚úÇÔ∏è Truncating overlong prompts for safety...\")\n",
    "    formatted_dataset = formatted_dataset.map(truncate_func, batched=True)\n",
    "    print(\"‚úÖ Truncation pass complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "452de834",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper Functions for Structured Output Parsing\n",
    "def parse_trading_output(text):\n",
    "    \"\"\"\n",
    "    Parse trading output JSON from text response.\n",
    "    Expected format: {\"action\":\"SELL\",\"confidence\":99,\"stop_loss\":10668.23,\"take_profit\":9377.95,\"forecast_10d\":[...]}\n",
    "    \"\"\"\n",
    "    if not text:\n",
    "        return None\n",
    "    \n",
    "    import json\n",
    "    import re\n",
    "    \n",
    "    try:\n",
    "        # Try direct JSON parsing first\n",
    "        return json.loads(text.strip())\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    try:\n",
    "        # Look for JSON-like structure in the text\n",
    "        json_pattern = r'\\{[^{}]*\"action\"[^{}]*\\}'\n",
    "        matches = re.findall(json_pattern, text, re.IGNORECASE | re.DOTALL)\n",
    "        \n",
    "        if matches:\n",
    "            # Try to parse the most complete match\n",
    "            for match in matches:\n",
    "                try:\n",
    "                    # Clean up the match and try parsing\n",
    "                    cleaned = match.strip()\n",
    "                    return json.loads(cleaned)\n",
    "                except:\n",
    "                    continue\n",
    "        \n",
    "        # Alternative: Extract components manually\n",
    "        result = {}\n",
    "        \n",
    "        # Extract action\n",
    "        action_match = re.search(r'\"action\"\\s*:\\s*\"([^\"]+)\"', text, re.IGNORECASE)\n",
    "        if action_match:\n",
    "            result['action'] = action_match.group(1).upper()\n",
    "        \n",
    "        # Extract confidence\n",
    "        conf_match = re.search(r'\"confidence\"\\s*:\\s*(\\d+(?:\\.\\d+)?)', text, re.IGNORECASE)\n",
    "        if conf_match:\n",
    "            result['confidence'] = float(conf_match.group(1))\n",
    "        \n",
    "        # Extract stop_loss\n",
    "        sl_match = re.search(r'\"stop_loss\"\\s*:\\s*(\\d+(?:\\.\\d+)?)', text, re.IGNORECASE)\n",
    "        if sl_match:\n",
    "            result['stop_loss'] = float(sl_match.group(1))\n",
    "        \n",
    "        # Extract take_profit\n",
    "        tp_match = re.search(r'\"take_profit\"\\s*:\\s*(\\d+(?:\\.\\d+)?)', text, re.IGNORECASE)\n",
    "        if tp_match:\n",
    "            result['take_profit'] = float(tp_match.group(1))\n",
    "        \n",
    "        # Extract forecast_10d array\n",
    "        forecast_match = re.search(r'\"forecast_10d\"\\s*:\\s*\\[([^\\]]+)\\]', text, re.IGNORECASE)\n",
    "        if forecast_match:\n",
    "            try:\n",
    "                forecast_str = forecast_match.group(1)\n",
    "                forecast_values = [float(x.strip()) for x in forecast_str.split(',')]\n",
    "                result['forecast_10d'] = forecast_values\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        return result if result else None\n",
    "        \n",
    "    except Exception as e:\n",
    "        return None\n",
    "\n",
    "def calculate_forecast_similarity(resp_forecast, gt_forecast):\n",
    "    \"\"\"\n",
    "    Calculate similarity between two forecast arrays.\n",
    "    Uses multiple metrics: correlation, directional accuracy, and magnitude similarity.\n",
    "    \"\"\"\n",
    "    if not resp_forecast or not gt_forecast:\n",
    "        return 0.0\n",
    "    \n",
    "    import numpy as np\n",
    "    \n",
    "    try:\n",
    "        # Ensure both are numeric arrays\n",
    "        resp_arr = np.array([float(x) for x in resp_forecast if isinstance(x, (int, float))])\n",
    "        gt_arr = np.array([float(x) for x in gt_forecast if isinstance(x, (int, float))])\n",
    "        \n",
    "        if len(resp_arr) == 0 or len(gt_arr) == 0:\n",
    "            return 0.0\n",
    "        \n",
    "        # Align lengths (take minimum)\n",
    "        min_len = min(len(resp_arr), len(gt_arr))\n",
    "        resp_arr = resp_arr[:min_len]\n",
    "        gt_arr = gt_arr[:min_len]\n",
    "        \n",
    "        if min_len < 2:\n",
    "            return 0.0\n",
    "        \n",
    "        similarity_score = 0.0\n",
    "        \n",
    "        # 1. Correlation similarity (40%)\n",
    "        try:\n",
    "            corr = np.corrcoef(resp_arr, gt_arr)[0, 1]\n",
    "            if not np.isnan(corr):\n",
    "                similarity_score += abs(corr) * 0.4\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        # 2. Directional accuracy (30%)\n",
    "        resp_directions = np.diff(resp_arr) > 0  # True for up, False for down\n",
    "        gt_directions = np.diff(gt_arr) > 0\n",
    "        if len(resp_directions) > 0:\n",
    "            directional_accuracy = np.mean(resp_directions == gt_directions)\n",
    "            similarity_score += directional_accuracy * 0.3\n",
    "        \n",
    "        # 3. Magnitude similarity (30%)\n",
    "        try:\n",
    "            # Normalize both arrays to compare relative changes\n",
    "            resp_norm = (resp_arr - np.mean(resp_arr)) / (np.std(resp_arr) + 1e-8)\n",
    "            gt_norm = (gt_arr - np.mean(gt_arr)) / (np.std(gt_arr) + 1e-8)\n",
    "            \n",
    "            # Calculate mean squared error and convert to similarity\n",
    "            mse = np.mean((resp_norm - gt_norm) ** 2)\n",
    "            magnitude_similarity = max(0, 1 - (mse / 4))  # Normalize MSE\n",
    "            similarity_score += magnitude_similarity * 0.3\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        return min(1.0, max(0.0, similarity_score))\n",
    "        \n",
    "    except Exception as e:\n",
    "        return 0.0\n",
    "\n",
    "print(\"‚úÖ Helper functions for structured output parsing defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6724234",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "def parse_trading_output(response_text):\n",
    "    \"\"\"\n",
    "    Parses a JSON object from a string, looking for the content between ```json and ```.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Find the JSON block within the response\n",
    "        match = re.search(r\"```json\\s*([\\s\\S]+?)\\s*```\", response_text)\n",
    "        if match:\n",
    "            json_str = match.group(1)\n",
    "            return json.loads(json_str)\n",
    "    except (json.JSONDecodeError, TypeError):\n",
    "        # Handle cases where parsing fails or input is not a string\n",
    "        pass\n",
    "    return None\n",
    "\n",
    "def calculate_forecast_similarity(predicted, actual):\n",
    "    \"\"\"\n",
    "    Calculates the similarity between two forecast arrays using Mean Absolute Percentage Error (MAPE).\n",
    "    A lower MAPE results in a higher similarity score (reward).\n",
    "    \"\"\"\n",
    "    if not predicted or not actual:\n",
    "        return 0.0\n",
    "    \n",
    "    # Ensure lists have the same length for comparison\n",
    "    min_len = min(len(predicted), len(actual))\n",
    "    if min_len == 0:\n",
    "        return 0.0\n",
    "        \n",
    "    predicted = predicted[:min_len]\n",
    "    actual = actual[:min_len]\n",
    "    \n",
    "    errors = []\n",
    "    for p, a in zip(predicted, actual):\n",
    "        if a > 0: # Avoid division by zero\n",
    "            errors.append(abs((p - a) / a))\n",
    "    \n",
    "    if not errors:\n",
    "        return 0.0\n",
    "        \n",
    "    mean_absolute_percentage_error = sum(errors) / len(errors)\n",
    "    \n",
    "    # Convert error to a similarity score (reward). 1.0 is perfect, 0.0 is high error.\n",
    "    # An error of 10% (0.1) would result in a 0.9 reward.\n",
    "    similarity = max(0, 1 - mean_absolute_percentage_error)\n",
    "    return similarity\n",
    "\n",
    "def calculate_price_prediction_reward(response, ground_truth):\n",
    "    \"\"\"\n",
    "    Calculates a reward based ONLY on the accuracy of numerical price predictions\n",
    "    (stop_loss, take_profit, and forecast_10d) against the ground truth.\n",
    "    \n",
    "    The final reward is a value between 0.0 and 1.0.\n",
    "    \"\"\"\n",
    "    response_json = parse_trading_output(response)\n",
    "    ground_truth_json = parse_trading_output(ground_truth)\n",
    "    \n",
    "    # If we don't have valid JSON in both, no reward can be calculated.\n",
    "    if not response_json or not ground_truth_json:\n",
    "        return 0.0\n",
    "        \n",
    "    price_rewards = []\n",
    "    \n",
    "    # 1. Price Level Accuracy (Stop Loss and Take Profit)\n",
    "    for price_field in ['stop_loss', 'take_profit']:\n",
    "        resp_price = response_json.get(price_field)\n",
    "        gt_price = ground_truth_json.get(price_field)\n",
    "        \n",
    "        # Ensure both are valid, positive numbers before calculating reward\n",
    "        if isinstance(resp_price, (int, float)) and isinstance(gt_price, (int, float)) and gt_price > 0:\n",
    "            # Calculate the percentage difference\n",
    "            price_diff_pct = abs((resp_price - gt_price) / gt_price)\n",
    "            \n",
    "            # Convert the percentage difference into a reward score (0 to 1)\n",
    "            # A perfect match gets 1.0. A 20% difference gets a 0.8 reward.\n",
    "            price_similarity = max(0, 1 - price_diff_pct)\n",
    "            price_rewards.append(price_similarity)\n",
    "            \n",
    "    # 2. Forecast Accuracy (10-day prediction array)\n",
    "    resp_forecast = response_json.get('forecast_10d')\n",
    "    gt_forecast = ground_truth_json.get('forecast_10d')\n",
    "    \n",
    "    # Ensure both are lists\n",
    "    if isinstance(resp_forecast, list) and isinstance(gt_forecast, list):\n",
    "        forecast_similarity = calculate_forecast_similarity(resp_forecast, gt_forecast)\n",
    "        price_rewards.append(forecast_similarity)\n",
    "        \n",
    "    # If no price fields were rewarded, the total reward is 0.\n",
    "    if not price_rewards:\n",
    "        return 0.0\n",
    "        \n",
    "    # The final reward is the average of all calculated price rewards.\n",
    "    total_reward = sum(price_rewards) / len(price_rewards)\n",
    "    \n",
    "    return total_reward\n",
    "\n",
    "### Example Usage ###\n",
    "\n",
    "# --- Test Case 1: Close match ---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6679bcec",
   "metadata": {},
   "source": [
    "## Enhanced Reward Function with Structured Output Parsing\n",
    "\n",
    "The reward function has been enhanced to handle structured JSON trading outputs with the following format:\n",
    "```json\n",
    "{\n",
    "  \"action\": \"SELL\",\n",
    "  \"confidence\": 99,\n",
    "  \"stop_loss\": 10668.23,\n",
    "  \"take_profit\": 9377.95,\n",
    "  \"forecast_10d\": [8830.75, 9174.91, 8277.01, 6955.27, 7754.00, 7621.30, 8265.59, 8736.98, 8621.90, 8129.97]\n",
    "}\n",
    "```\n",
    "\n",
    "### Reward Distribution:\n",
    "- **Structured Output Parsing (25%)**: Parses and validates JSON format, compares action/confidence/prices/forecast with ground truth\n",
    "- **Prediction Quality (20%)**: Keywords, length, comprehensive analysis indicators\n",
    "- **Technical Analysis (15%)**: Technical indicators, chart patterns, trading signals\n",
    "- **News Integration (15%)**: News impact assessment, multi-factor analysis\n",
    "- **Specificity (10%)**: Price targets, timeframes, confidence levels\n",
    "- **AI Assessment (10%)**: Conversational quality using reward model\n",
    "- **Bonuses (5%)**: Structure, disclaimers, professional formatting\n",
    "\n",
    "### Key Features:\n",
    "- **JSON Parsing**: Extracts structured trading data from model responses\n",
    "- **Action Matching**: Compares trading actions (BUY/SELL/HOLD) with ground truth\n",
    "- **Confidence Scoring**: Rewards confidence levels close to expected values\n",
    "- **Price Accuracy**: Evaluates stop_loss and take_profit price levels\n",
    "- **Forecast Similarity**: Multi-metric comparison of 10-day price predictions using correlation, directional accuracy, and magnitude similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "259ef602",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom GRPO Trainer with Enhanced Reward Integration\n",
    "class CustomGRPOTrainer(GRPOTrainer):\n",
    "    \"\"\"\n",
    "    Custom GRPO Trainer that integrates the enhanced reward function\n",
    "    with structured output parsing for Bitcoin trading predictions.\n",
    "    This version correctly processes batches and is structured robustly.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, reward_model=None, reward_tokenizer=None, **kwargs):\n",
    "        # Pass the reward function method to the parent class\n",
    "        kwargs['reward_funcs'] = [self._compute_reward_batch]\n",
    "        \n",
    "        super().__init__(**kwargs)\n",
    "        \n",
    "        # Now self.tokenizer and other attributes are safely initialized\n",
    "        self.reward_model = reward_model\n",
    "        self.reward_tokenizer = reward_tokenizer\n",
    "\n",
    "    def _compute_reward_batch(self, prompts=None, completions=None, **kwargs_inner):\n",
    "        \"\"\"\n",
    "        This is our main reward logic, now defined as a class method.\n",
    "        It correctly loops through a batch of completions and returns a list of rewards.\n",
    "        \"\"\"\n",
    "        rewards = []\n",
    "        try:\n",
    "            # 'completions' is a list of generated text strings from the model\n",
    "            for completion_text in completions:\n",
    "                # Calculate reward for EACH completion individually\n",
    "                reward_score = calculate_comprehensive_prediction_reward(\n",
    "                    response=completion_text,\n",
    "                    ground_truth=None,  # Correct for GRPO, no ground truth needed here\n",
    "                    reward_model=self.reward_model,\n",
    "                    reward_tokenizer=self.reward_tokenizer\n",
    "                )\n",
    "                rewards.append(reward_score)\n",
    "            \n",
    "            return rewards # Return the list of calculated rewards for the batch\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Error during batch reward computation: {e}\")\n",
    "            # If an error occurs, return a list of fallback rewards\n",
    "            # that matches the batch size to avoid crashing the trainer.\n",
    "            batch_size = len(completions) if completions is not None else 0\n",
    "            return [0.5] * batch_size\n",
    "    \n",
    "    def log_reward_details(self, response, reward_score):\n",
    "        \"\"\"\n",
    "        Log detailed reward breakdown for debugging and analysis.\n",
    "        (This method requires no changes)\n",
    "        \"\"\"\n",
    "        print(f\"\\n=== Reward Analysis ===\")\n",
    "        print(f\"Response length: {len(response)} chars\")\n",
    "        print(f\"Reward score: {reward_score:.4f}\")\n",
    "        \n",
    "        response_json = parse_trading_output(response)\n",
    "        \n",
    "        if response_json:\n",
    "            print(f\"Structured output found:\")\n",
    "            print(f\"  Action: {response_json.get('action', 'N/A')}\")\n",
    "            print(f\"  Confidence: {response_json.get('confidence', 'N/A')}\")\n",
    "            print(f\"  Stop Loss: {response_json.get('stop_loss', 'N/A')}\")\n",
    "            print(f\"  Take Profit: {response_json.get('take_profit', 'N/A')}\")\n",
    "            forecast = response_json.get('forecast_10d', [])\n",
    "            print(f\"  Forecast: {forecast[:3]}... ({len(forecast)} values)\")\n",
    "        else:\n",
    "            print(\"No structured output found - using text-based scoring\")\n",
    "        \n",
    "        print(\"=\" * 25)\n",
    "\n",
    "print(\"‚úÖ Corrected CustomGRPOTrainer created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b76c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Test Enhanced Reward Function with Structured Output\n",
    "# print(\"üß™ Testing Enhanced Reward Function with Structured Output\")\n",
    "# print(\"=\" * 60)\n",
    "\n",
    "# # Example model response with structured output\n",
    "# example_response = '''Based on my analysis of Bitcoin's current market conditions, technical indicators, and recent news sentiment, here is my prediction:\n",
    "\n",
    "# {\"action\":\"SELL\",\"confidence\":85,\"stop_loss\":11200.50,\"take_profit\":9500.75,\"forecast_10d\":[10450.20, 10100.85, 9750.40, 9500.75, 9200.30, 8950.10, 9150.60, 9400.25, 9300.80, 9150.45]}\n",
    "\n",
    "# This prediction is based on bearish divergence in RSI, declining institutional interest, and regulatory concerns affecting market sentiment.'''\n",
    "\n",
    "# # Example ground truth for comparison\n",
    "# example_ground_truth = '''{\"action\":\"SELL\",\"confidence\":90,\"stop_loss\":11000.00,\"take_profit\":9400.00,\"forecast_10d\":[10400.00, 10050.00, 9700.00, 9450.00, 9200.00, 8900.00, 9100.00, 9350.00, 9250.00, 9100.00]}'''\n",
    "\n",
    "# # Test parsing\n",
    "# print(\"üìä Testing JSON Parsing:\")\n",
    "# parsed_response = parse_trading_output(example_response)\n",
    "# parsed_gt = parse_trading_output(example_ground_truth)\n",
    "\n",
    "# print(\"Response JSON:\", parsed_response)\n",
    "# print(\"Ground Truth JSON:\", parsed_gt)\n",
    "\n",
    "# # Test reward calculation\n",
    "# print(f\"\\nüèÜ Testing Reward Calculation:\")\n",
    "# reward_score = calculate_comprehensive_prediction_reward(\n",
    "#     response=example_response,\n",
    "#     ground_truth=example_ground_truth,\n",
    "#     reward_model=reward_model,\n",
    "#     reward_tokenizer=reward_tokenizer\n",
    "# )\n",
    "\n",
    "# print(f\"Reward Score: {reward_score:.4f}\")\n",
    "\n",
    "# # Test forecast similarity\n",
    "# if parsed_response and parsed_gt:\n",
    "#     forecast_sim = calculate_forecast_similarity(\n",
    "#         parsed_response.get('forecast_10d', []),\n",
    "#         parsed_gt.get('forecast_10d', [])\n",
    "#     )\n",
    "#     print(f\"Forecast Similarity: {forecast_sim:.4f}\")\n",
    "\n",
    "# print(\"\\n‚úÖ Enhanced reward function testing completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5786cd7e",
   "metadata": {},
   "source": [
    "## Setup GRPO Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a072b0d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training arguments\n",
    "max_steps = SANITY_MAX_STEPS if SANITY_RUN else -1\n",
    "\n",
    "grpo_args = GRPOConfig(\n",
    "    # FIX 1: Corrected parameter name\n",
    "    output_dir=OUTPUT_DIR,\n",
    "\n",
    "    # FIX 2: Set max_steps and remove conditional logic for epochs\n",
    "    max_steps=max_steps,\n",
    "    num_train_epochs=NUM_TRAIN_EPOCHS,\n",
    "    bf16=True,  # You are enabling bfloat16 training\n",
    "\n",
    "    per_device_train_batch_size=PER_DEVICE_TRAIN_BATCH_SIZE,\n",
    "    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    logging_steps=10,\n",
    "    save_steps=0 if SANITY_RUN else 100,\n",
    "    save_strategy=\"no\" if SANITY_RUN else \"steps\",\n",
    "    warmup_ratio=0.1,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    optim=\"adamw_8bit\",\n",
    "    weight_decay=0.01,\n",
    "    max_grad_norm=1.0,\n",
    "        # max_new_tokens=1024,  # Limit to 50 new tokens generated\n",
    "\n",
    "    # fp16=not is_bfloat16_supported(),\n",
    "    # bf16=is_bfloat16_supported(),\n",
    "    remove_unused_columns=False,\n",
    "    dataloader_num_workers=2,\n",
    "    seed=SEED,\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "# Your print statements for verification are good and will still work.\n",
    "print(f\"üéØ Training Configuration:\")\n",
    "# ... (rest of your print statements)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf3fa56",
   "metadata": {},
   "source": [
    "## Initialize GRPO Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d44834be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize GRPO trainer with tokenized dataset\n",
    "print(\"üîß Initializing GRPO Trainer with pre-tokenized dataset...\")\n",
    "generation_kwargs = {\n",
    "    \"max_new_tokens\": 1024,  # <-- Set your generation length here\n",
    "    \"do_sample\": True,\n",
    "    \"top_k\": 50,\n",
    "    \"temperature\": 0.7,\n",
    "}\n",
    "\n",
    "if GRPO_AVAILABLE:\n",
    "    try:\n",
    "        # Note: Use the DataCollatorForLanguageModeling to pad tokenized batches correctly\n",
    "        from transformers import DataCollatorForLanguageModeling\n",
    "        \n",
    "        # This collator properly handles tokenized data (input_ids, attention_mask) for language modeling\n",
    "        data_collator = DataCollatorForLanguageModeling(\n",
    "            tokenizer=tokenizer,\n",
    "            mlm=False  # We're doing causal LM, not masked LM\n",
    "        )\n",
    "        \n",
    "        # Use the CustomGRPOTrainer with pre-tokenized data\n",
    "        grpo_trainer = CustomGRPOTrainer(\n",
    "            model=model,\n",
    "            tokenizer=tokenizer,\n",
    "            args=grpo_args,\n",
    "            train_dataset=tokenized_dataset,  # Now contains input_ids, attention_mask\n",
    "            reward_model=reward_model,\n",
    "            reward_tokenizer=reward_tokenizer,\n",
    "            max_length=MAX_LENGTH,\n",
    "            max_prompt_length=MAX_PROMPT_LENGTH,\n",
    "            beta=BETA,\n",
    "                # generation_kwargs=generation_kwargs, # <-- The correct argument\n",
    "\n",
    "            # Pass the appropriate collator for tokenized data\n",
    "            data_collator=data_collator\n",
    "        )\n",
    "        print(\"‚úÖ CustomGRPOTrainer initialized successfully\")\n",
    "        print(f\"üìä Dataset columns: {formatted_dataset.column_names}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Failed to initialize GRPO trainer: {e}\")\n",
    "        print(\"üîÑ Falling back to standard Trainer\")\n",
    "        \n",
    "        # For fallback, we can use the same tokenized dataset\n",
    "        from transformers import Trainer, DataCollatorForLanguageModeling\n",
    "        \n",
    "        data_collator = DataCollatorForLanguageModeling(\n",
    "            tokenizer=tokenizer,\n",
    "            mlm=False  # We're doing causal LM, not masked LM\n",
    "        )\n",
    "        \n",
    "        grpo_trainer = Trainer(\n",
    "            model=model,\n",
    "            tokenizer=tokenizer,\n",
    "            args=grpo_args,\n",
    "            train_dataset=formatted_dataset,\n",
    "            data_collator=data_collator,\n",
    "        )\n",
    "        print(\"‚úÖ Fallback Trainer initialized with tokenized data\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è GRPO not available, using standard Trainer\")\n",
    "    \n",
    "    from transformers import Trainer, DataCollatorForLanguageModeling\n",
    "    \n",
    "    data_collator = DataCollatorForLanguageModeling(\n",
    "        tokenizer=tokenizer,\n",
    "        mlm=False  # We're doing causal LM, not masked LM\n",
    "    )\n",
    "    \n",
    "    grpo_trainer = Trainer(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        args=grpo_args,\n",
    "        train_dataset=tokenized_dataset,\n",
    "        data_collator=data_collator,\n",
    "    )\n",
    "    print(\"‚úÖ Standard Trainer initialized with tokenized data\")\n",
    "\n",
    "print(f\"üéØ Training ready with {type(grpo_trainer).__name__}\")\n",
    "print(f\"üìä Training dataset: {len(formatted_dataset):,} samples\")\n",
    "print(f\"üîß Trainer configuration:\")\n",
    "print(f\"  ‚Ä¢ Effective batch size: {PER_DEVICE_TRAIN_BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS}\")\n",
    "print(f\"  ‚Ä¢ Total training steps: {len(formatted_dataset) // (PER_DEVICE_TRAIN_BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS) * (0 if SANITY_RUN else NUM_TRAIN_EPOCHS)}\")\n",
    "print(f\"  ‚Ä¢ Learning rate: {LEARNING_RATE}\")\n",
    "print(f\"  ‚Ä¢ Dataloader workers: 0 (avoiding multiprocessing issues)\")\n",
    "print(f\"  ‚Ä¢ Sanity run: {SANITY_RUN}\")\n",
    "\n",
    "if SANITY_RUN:\n",
    "    print(f\"  ‚Ä¢ Sanity max steps: {SANITY_MAX_STEPS}\")\n",
    "    print(\"‚ö†Ô∏è SANITY_RUN is enabled - this will be a short test run\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a35fa2b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize GRPO trainer with enhanced reward function\n",
    "print(\"üîß Initializing GRPO Trainer...\")\n",
    "print(\"‚úÖ Final check: Columns being passed to trainer:\", tokenized_dataset.column_names)\n",
    "\n",
    "if GRPO_AVAILABLE:\n",
    "    try:\n",
    "        # Initialize CustomGRPOTrainer with enhanced reward integration\n",
    "        grpo_trainer = CustomGRPOTrainer(\n",
    "            model=model,\n",
    "            tokenizer=tokenizer,\n",
    "            args=grpo_args,  # Use GRPOConfig\n",
    "    train_dataset=tokenized_dataset,  # <--- CORRECTED\n",
    "            reward_model=reward_model,\n",
    "            reward_tokenizer=reward_tokenizer,\n",
    "            # GRPO-specific parameters\n",
    "            max_length=MAX_LENGTH,\n",
    "            max_prompt_length=MAX_PROMPT_LENGTH,\n",
    "            beta=BETA,\n",
    "            # Let GRPO handle its own data collation for tokenized data\n",
    "        )\n",
    "        \n",
    "        print(\"‚úÖ CustomGRPOTrainer initialized successfully\")\n",
    "        print(f\"üìä Training dataset format: {formatted_dataset.column_names}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Failed to initialize GRPO trainer: {e}\")\n",
    "        print(\"üîÑ Falling back to standard trainer\")\n",
    "        \n",
    "        # Fallback initialization with correct data collator for tokenized data\n",
    "        from transformers import Trainer, DataCollatorWithPadding\n",
    "        \n",
    "        # Use DataCollatorWithPadding for pre-tokenized data\n",
    "        data_collator = DataCollatorWithPadding(\n",
    "            tokenizer=tokenizer,\n",
    "            padding=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        grpo_trainer = Trainer(\n",
    "            model=model,\n",
    "            tokenizer=tokenizer,\n",
    "            args=grpo_args,\n",
    "            train_dataset=tokenized_dataset,\n",
    "            data_collator=data_collator,\n",
    "        )\n",
    "        print(\"‚úÖ Fallback trainer initialized with DataCollatorWithPadding\")\n",
    "\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è GRPO not available, using standard training\")\n",
    "    from transformers import Trainer, DataCollatorWithPadding\n",
    "    \n",
    "    # Use DataCollatorWithPadding for pre-tokenized data\n",
    "    data_collator = DataCollatorWithPadding(\n",
    "        tokenizer=tokenizer,\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    \n",
    "    grpo_trainer = Trainer(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        args=grpo_args,\n",
    "        train_dataset=tokenized_dataset,\n",
    "        data_collator=data_collator,\n",
    "    )\n",
    "    print(\"‚úÖ Standard trainer initialized with DataCollatorWithPadding\")\n",
    "\n",
    "print(f\"üéØ Training ready with {type(grpo_trainer).__name__}\")\n",
    "print(f\"üìä Training dataset: {len(formatted_dataset):,} samples\")\n",
    "print(f\"üîß Trainer configuration:\")\n",
    "print(f\"  ‚Ä¢ Effective batch size: {PER_DEVICE_TRAIN_BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS}\")\n",
    "print(f\"  ‚Ä¢ Total training steps: {len(formatted_dataset) // (PER_DEVICE_TRAIN_BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS) * (0 if SANITY_RUN else NUM_TRAIN_EPOCHS)}\")\n",
    "print(f\"  ‚Ä¢ Learning rate: {LEARNING_RATE}\")\n",
    "print(f\"  ‚Ä¢ Sanity run: {SANITY_RUN}\")\n",
    "\n",
    "if SANITY_RUN:\n",
    "    print(f\"  ‚Ä¢ Sanity max steps: {SANITY_MAX_STEPS}\")\n",
    "    print(\"‚ö†Ô∏è SANITY_RUN is enabled - this will be a short test run\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "503f06f8",
   "metadata": {},
   "source": [
    "## Start GRPO Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f170b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training\n",
    "print(\"üöÄ Starting Unsloth GRPO Training...\")\n",
    "print(f\"Training {0 if SANITY_RUN else NUM_TRAIN_EPOCHS} epoch(s) on {len(formatted_dataset):,} samples\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Record start time\n",
    "start_time = datetime.now()\n",
    "print(f\"Training started at: {start_time.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "# Train the model\n",
    "trainer_stats = grpo_trainer.train()\n",
    "\n",
    "# Record end time\n",
    "end_time = datetime.now()\n",
    "training_duration = end_time - start_time\n",
    "\n",
    "# Extract safe stats\n",
    "final_loss = None\n",
    "steps_done = None\n",
    "try:\n",
    "    final_loss = getattr(trainer_stats, \"training_loss\", None)\n",
    "    steps_done = getattr(trainer_stats, \"global_step\", None)\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üéâ GRPO Training Completed!\")\n",
    "print(f\"Training finished at: {end_time.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"Total training time: {training_duration}\")\n",
    "print(f\"Final training loss: {final_loss if final_loss is not None else 'N/A'}\")\n",
    "print(f\"Training steps: {steps_done if steps_done is not None else 'N/A'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89bef97b",
   "metadata": {},
   "source": [
    "## Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d9c028",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the final model\n",
    "print(\"üíæ Saving trained model...\")\n",
    "\n",
    "# Save model and tokenizer\n",
    "model.save_pretrained(f\"{OUTPUT_DIR}/final_model\")\n",
    "tokenizer.save_pretrained(f\"{OUTPUT_DIR}/final_model\")\n",
    "\n",
    "print(f\"‚úÖ Model saved to: {OUTPUT_DIR}/final_model\")\n",
    "\n",
    "# Save training summary\n",
    "training_method = \"Unsloth GRPO\" if GRPO_AVAILABLE else \"Unsloth SFT/Basic\"\n",
    "training_summary = {\n",
    "    \"base_model_name\": chosen_model_name,\n",
    "    \"adapter_path\": f\"{ADAPTER_PATH}/{CHECKPOINT}\",\n",
    "    \"adapter_loaded\": adapter_loaded,\n",
    "    \"dataset\": DATASET_NAME,\n",
    "    \"training_method\": training_method,\n",
    "    \"grpo_available\": GRPO_AVAILABLE,\n",
    "    \"total_samples\": len(formatted_dataset),\n",
    "    \"training_config\": {\n",
    "        \"epochs\": training_config.num_train_epochs,\n",
    "        \"max_steps\": training_config.max_steps if training_config.max_steps > 0 else None,\n",
    "        \"learning_rate\": training_config.learning_rate,\n",
    "        \"batch_size\": training_config.per_device_train_batch_size,\n",
    "        \"gradient_accumulation_steps\": training_config.gradient_accumulation_steps,\n",
    "        \"sanity_run\": SANITY_RUN,\n",
    "        \"sanity_max_steps\": SANITY_MAX_STEPS if SANITY_RUN else None,\n",
    "        \"sanity_dataset_size\": SANITY_DATASET_SIZE if SANITY_RUN else None,\n",
    "    },\n",
    "    \"training_results\": {\n",
    "        \"final_loss\": final_loss,\n",
    "        \"total_steps\": steps_done,\n",
    "        \"training_duration\": str(training_duration),\n",
    "    },\n",
    "    \"timestamps\": {\n",
    "        \"start_time\": start_time.isoformat(),\n",
    "        \"end_time\": end_time.isoformat(),\n",
    "    },\n",
    "    \"model_path\": f\"{OUTPUT_DIR}/final_model\",\n",
    "}\n",
    "\n",
    "# Add GRPO-specific config if available\n",
    "if GRPO_AVAILABLE and hasattr(training_config, 'beta'):\n",
    "    training_summary[\"training_config\"][\"grpo_beta\"] = training_config.beta\n",
    "    training_summary[\"training_config\"][\"max_length\"] = training_config.max_length\n",
    "    training_summary[\"training_config\"][\"max_prompt_length\"] = training_config.max_prompt_length\n",
    "\n",
    "# Save summary\n",
    "with open(f\"{OUTPUT_DIR}/training_summary.json\", \"w\") as f:\n",
    "    json.dump(training_summary, f, indent=2)\n",
    "\n",
    "print(f\"Training summary saved to: {OUTPUT_DIR}/training_summary.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9efd7251",
   "metadata": {},
   "source": [
    "## Test Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6984cec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_trained_model(model, tokenizer):\n",
    "    \"\"\"\n",
    "    Generates a response from the trained model to a test prompt.\n",
    "    \"\"\"\n",
    "    print(\"üß™ Testing the trained GRPO model...\")\n",
    "\n",
    "    # Prepare model for inference\n",
    "    FastLanguageModel.for_inference(model)\n",
    "\n",
    "    # Test sample\n",
    "    test_messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are an expert Bitcoin market analyst. Provide accurate and insightful analysis.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Based on recent market trends and news, what is your Bitcoin price prediction for the next week? Please provide detailed analysis.\"}\n",
    "    ]\n",
    "\n",
    "    # Format with chat template\n",
    "    test_prompt = tokenizer.apply_chat_template(\n",
    "        test_messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "\n",
    "    print(\"Test prompt:\")\n",
    "    print(test_prompt)\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "\n",
    "    # Generate response\n",
    "    inputs = tokenizer(test_prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=1024,\n",
    "            temperature=0.7,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "\n",
    "    # Decode response\n",
    "    response = tokenizer.decode(\n",
    "        outputs[0][len(inputs.input_ids[0]):],\n",
    "        skip_special_tokens=True\n",
    "    )\n",
    "\n",
    "    print(\"Model Response:\")\n",
    "    print(response)\n",
    "    print(\"\\n‚úÖ Model testing completed!\")\n",
    "\n",
    "# Run the test\n",
    "test_trained_model(model, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4446bded",
   "metadata": {},
   "source": [
    "## Training Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff117e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üìä Unsloth GRPO Training Summary\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"ü§ñ Model: {MODEL_NAME}\")\n",
    "print(f\"üìö Dataset: {DATASET_NAME}\")\n",
    "print(f\"üìà Training method: Unsloth GRPO (Group Relative Policy Optimization)\")\n",
    "print(f\"üìù Total samples: {len(formatted_dataset):,}\")\n",
    "print()\n",
    "print(\"üéØ Training Configuration:\")\n",
    "print(f\"  ‚Ä¢ Epochs: {NUM_TRAIN_EPOCHS}\")\n",
    "print(f\"  ‚Ä¢ Learning rate: {LEARNING_RATE}\")\n",
    "print(f\"  ‚Ä¢ Batch size: {PER_DEVICE_TRAIN_BATCH_SIZE}\")\n",
    "print(f\"  ‚Ä¢ Gradient accumulation: {GRADIENT_ACCUMULATION_STEPS}\")\n",
    "print(f\"  ‚Ä¢ Effective batch size: {PER_DEVICE_TRAIN_BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS}\")\n",
    "print(f\"  ‚Ä¢ Max length: {MAX_LENGTH}\")\n",
    "print(f\"  ‚Ä¢ Max prompt length: {MAX_PROMPT_LENGTH}\")\n",
    "print(f\"  ‚Ä¢ Beta (KL penalty): {BETA}\")\n",
    "print(f\"  ‚Ä¢ LoRA rank: {LORA_R}\")\n",
    "print()\n",
    "print(\"üìä Training Results:\")\n",
    "print(f\"  ‚Ä¢ Final loss: {trainer_stats.training_loss:.4f}\")\n",
    "print(f\"  ‚Ä¢ Training steps: {trainer_stats.global_step:,}\")\n",
    "print(f\"  ‚Ä¢ Training duration: {training_duration}\")\n",
    "print()\n",
    "print(\"üíæ Outputs:\")\n",
    "print(f\"  ‚Ä¢ Model saved to: {OUTPUT_DIR}/final_model\")\n",
    "print(f\"  ‚Ä¢ Summary saved to: {OUTPUT_DIR}/training_summary.json\")\n",
    "print()\n",
    "print(\"üî¨ Key Features:\")\n",
    "print(\"  ‚úÖ Unsloth-optimized GRPO training\")\n",
    "print(\"  ‚úÖ Memory-efficient 4-bit quantization\")\n",
    "print(\"  ‚úÖ LoRA parameter-efficient fine-tuning\")\n",
    "print(\"  ‚úÖ Preference learning for Bitcoin analysis\")\n",
    "print(\"  ‚úÖ Chat template formatting\")\n",
    "print(\"  ‚úÖ Gradient checkpointing for memory optimization\")\n",
    "print()\n",
    "print(\"üéâ Unsloth GRPO training completed successfully!\")\n",
    "print(\"üìà Model ready for Bitcoin prediction tasks!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
