{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea2ea687",
   "metadata": {},
   "source": [
    "# Bitcoin Price Prediction - Qwen2.5 4B Instruct Fine-tuning\n",
    "\n",
    "This notebook fine-tunes Qwen2.5 4B Instruct model for Bitcoin price prediction using technical analysis and news sentiment data.\n",
    "\n",
    "## Overview\n",
    "- **Model**: Qwen/Qwen2.5-4B-Instruct\n",
    "- **Task**: Multi-modal Bitcoin trading decision and price forecasting\n",
    "- **Data**: Technical indicators + News analysis ‚Üí Trading actions + 10-day price forecasts\n",
    "- **Method**: LoRA fine-tuning with instruction format"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb6dfab",
   "metadata": {},
   "source": [
    "## 1. Environment Setup and Dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e38cc5",
   "metadata": {},
   "source": [
    "## ‚ö†Ô∏è Important: Clean Setup Instructions\n",
    "\n",
    "**To avoid tensor creation and model loading errors:**\n",
    "\n",
    "1. **Restart Kernel**: If you encounter any errors, restart the notebook kernel first\n",
    "2. **Clear Output**: Clear all cell outputs before rerunning  \n",
    "3. **Run in Order**: Execute cells sequentially from top to bottom\n",
    "4. **Clean Environment**: Remove any existing model directories in the output folder\n",
    "\n",
    "**Common Error Solutions:**\n",
    "- `tensor creation error`: Dataset formatting issues ‚Üí Restart kernel\n",
    "- `size mismatch error`: Model loading conflicts ‚Üí Clear cache and restart  \n",
    "- `adapter conflict`: PEFT adapter issues ‚Üí Remove output directories\n",
    "\n",
    "**Memory Requirements:**\n",
    "- RTX 3090: 24GB VRAM recommended\n",
    "- Batch size automatically adjusted for available memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c452d7e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages for Unsloth training\n",
    "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
    "!pip install --no-deps \"xformers<0.0.27\" \"trl<0.9.0\" peft accelerate bitsandbytes\n",
    "!pip install wandb evaluate\n",
    "!pip install pandas numpy matplotlib seaborn\n",
    "!pip install huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82871541",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Unsloth imports for faster training\n",
    "from unsloth import FastLanguageModel\n",
    "from unsloth import is_bfloat16_supported\n",
    "from unsloth.chat_templates import get_chat_template\n",
    "\n",
    "# Standard ML/AI Libraries\n",
    "from transformers import TrainingArguments\n",
    "from trl import SFTTrainer\n",
    "from datasets import Dataset, load_dataset\n",
    "\n",
    "# Set up plotting\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "    print(f\"BFloat16 supported: {is_bfloat16_supported()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65fde41d",
   "metadata": {},
   "source": [
    "## 2. Configuration and Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "711c670e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Configuration for Unsloth + RTX 3090\n",
    "CONFIG = {\n",
    "    # Model settings\n",
    "    \"model_name\": \"unsloth/Qwen2.5-4B-Instruct\",  # Unsloth optimized model\n",
    "    \"output_dir\": \"./bitcoin_qwen_unsloth_model\",\n",
    "    \"hub_model_id\": \"tahamajs/bitcoin-qwen2.5-4b-unsloth\", # Change to your username\n",
    "    \n",
    "    # Dataset settings\n",
    "    \"dataset_name\": \"tahamajs/bitcoin-prediction-enhanced-dataset\", # Change to your dataset\n",
    "    \"max_length\": 2048,\n",
    "    \"train_split\": 0.8,\n",
    "    \"eval_split\": 0.1,\n",
    "    \"test_split\": 0.1,\n",
    "    \n",
    "    # LoRA settings (Optimized for Unsloth)\n",
    "    \"lora_r\": 16,\n",
    "    \"lora_alpha\": 16,\n",
    "    \"lora_dropout\": 0,\n",
    "    \"target_modules\": [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    \n",
    "    # Training hyperparameters (Optimized for RTX 3090)\n",
    "    \"num_train_epochs\": 3,\n",
    "    \"per_device_train_batch_size\": 4,  # Increased for RTX 3090\n",
    "    \"per_device_eval_batch_size\": 4,\n",
    "    \"gradient_accumulation_steps\": 2,  # Reduced since batch size increased\n",
    "    \"learning_rate\": 2e-4,\n",
    "    \"weight_decay\": 0.01,\n",
    "    \"warmup_ratio\": 0.03,\n",
    "    \"lr_scheduler_type\": \"linear\",\n",
    "    \n",
    "    # Optimization settings (Unsloth optimized)\n",
    "    \"optim\": \"adamw_8bit\",\n",
    "    \"gradient_checkpointing\": True,\n",
    "    \"dataloader_pin_memory\": False,\n",
    "    \"bf16\": is_bfloat16_supported(),  # Use BFloat16 if supported\n",
    "    \"fp16\": not is_bfloat16_supported(),  # Fallback to FP16\n",
    "    \n",
    "    # Logging and saving\n",
    "    \"logging_steps\": 10,\n",
    "    \"eval_steps\": 50,\n",
    "    \"save_steps\": 200,\n",
    "    \"save_total_limit\": 3,\n",
    "    \"load_best_model_at_end\": True,\n",
    "    \"metric_for_best_model\": \"eval_loss\",\n",
    "    \"greater_is_better\": False,\n",
    "    \n",
    "    # Unsloth settings\n",
    "    \"max_seq_length\": 2048,\n",
    "    \"dtype\": None,  # Auto-detect\n",
    "    \"load_in_4bit\": True,\n",
    "    \n",
    "    # Custom evaluation settings\n",
    "    \"show_sample_every_n_steps\": 50,  # Show model output every 50 steps\n",
    "}\n",
    "\n",
    "print(\"Unsloth Training Configuration:\")\n",
    "for key, value in CONFIG.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "    \n",
    "print(f\"\\nOptimizations for RTX 3090:\")\n",
    "print(f\"  Using {'BFloat16' if CONFIG['bf16'] else 'Float16'} precision\")\n",
    "print(f\"  Batch size: {CONFIG['per_device_train_batch_size']} (effective: {CONFIG['per_device_train_batch_size'] * CONFIG['gradient_accumulation_steps']})\")\n",
    "print(f\"  LoRA rank reduced to {CONFIG['lora_r']} for faster training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63bed9c1",
   "metadata": {},
   "source": [
    "## 3. Load and Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3442c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset from Hugging Face Hub\n",
    "print(f\"Loading dataset: {CONFIG['dataset_name']}\")\n",
    "dataset = load_dataset(CONFIG['dataset_name'])\n",
    "\n",
    "print(f\"Dataset loaded:\")\n",
    "print(f\"  Total samples: {len(dataset['train'])}\")\n",
    "print(f\"  Features: {list(dataset['train'].features.keys())}\")\n",
    "\n",
    "# Show sample\n",
    "sample = dataset['train'][0]\n",
    "print(f\"\\nSample data structure:\")\n",
    "print(f\"  Instruction length: {len(sample['instruction'])} chars\")\n",
    "print(f\"  Input length: {len(sample['input'])} chars\")\n",
    "print(f\"  Output length: {len(sample['output'])} chars\")\n",
    "\n",
    "print(f\"\\nSample output: {sample['output']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77361aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze dataset statistics\n",
    "def analyze_dataset(dataset):\n",
    "    \"\"\"Analyze dataset characteristics\"\"\"\n",
    "    df = dataset.to_pandas()\n",
    "    \n",
    "    # Length analysis\n",
    "    df['instruction_len'] = df['instruction'].str.len()\n",
    "    df['input_len'] = df['input'].str.len()\n",
    "    df['output_len'] = df['output'].str.len()\n",
    "    df['total_len'] = df['instruction_len'] + df['input_len'] + df['output_len']\n",
    "    \n",
    "    # Output analysis\n",
    "    df['action'] = df['output'].str.extract(r'\"action\":\"([^\"]+)\"')\n",
    "    df['confidence'] = df['output'].str.extract(r'\"confidence\":([0-9]+)').astype(float)\n",
    "    \n",
    "    print(\"Dataset Statistics:\")\n",
    "    print(f\"  Total samples: {len(df)}\")\n",
    "    print(f\"\\nLength Statistics:\")\n",
    "    print(f\"  Instruction: {df['instruction_len'].mean():.0f} ¬± {df['instruction_len'].std():.0f} chars\")\n",
    "    print(f\"  Input: {df['input_len'].mean():.0f} ¬± {df['input_len'].std():.0f} chars\")\n",
    "    print(f\"  Output: {df['output_len'].mean():.0f} ¬± {df['output_len'].std():.0f} chars\")\n",
    "    print(f\"  Total: {df['total_len'].mean():.0f} ¬± {df['total_len'].std():.0f} chars\")\n",
    "    \n",
    "    print(f\"\\nAction Distribution:\")\n",
    "    action_counts = df['action'].value_counts()\n",
    "    for action, count in action_counts.items():\n",
    "        percentage = count / len(df) * 100\n",
    "        print(f\"  {action}: {count} ({percentage:.1f}%)\")\n",
    "    \n",
    "    print(f\"\\nConfidence Statistics:\")\n",
    "    print(f\"  Mean: {df['confidence'].mean():.1f}\")\n",
    "    print(f\"  Std: {df['confidence'].std():.1f}\")\n",
    "    print(f\"  Min: {df['confidence'].min():.0f}\")\n",
    "    print(f\"  Max: {df['confidence'].max():.0f}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "df_analysis = analyze_dataset(dataset['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f567e5f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize dataset characteristics\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Length distributions\n",
    "axes[0, 0].hist(df_analysis['total_len'], bins=50, alpha=0.7, edgecolor='black')\n",
    "axes[0, 0].set_title('Total Length Distribution')\n",
    "axes[0, 0].set_xlabel('Characters')\n",
    "axes[0, 0].set_ylabel('Frequency')\n",
    "axes[0, 0].axvline(CONFIG['max_length'], color='red', linestyle='--', label=f\"Max Length: {CONFIG['max_length']}\")\n",
    "axes[0, 0].legend()\n",
    "\n",
    "# Action distribution\n",
    "action_counts = df_analysis['action'].value_counts()\n",
    "axes[0, 1].pie(action_counts.values, labels=action_counts.index, autopct='%1.1f%%')\n",
    "axes[0, 1].set_title('Trading Action Distribution')\n",
    "\n",
    "# Confidence distribution\n",
    "axes[1, 0].hist(df_analysis['confidence'], bins=30, alpha=0.7, edgecolor='black')\n",
    "axes[1, 0].set_title('Confidence Distribution')\n",
    "axes[1, 0].set_xlabel('Confidence Score')\n",
    "axes[1, 0].set_ylabel('Frequency')\n",
    "\n",
    "# Input length vs confidence\n",
    "axes[1, 1].scatter(df_analysis['input_len'], df_analysis['confidence'], alpha=0.5)\n",
    "axes[1, 1].set_title('Input Length vs Confidence')\n",
    "axes[1, 1].set_xlabel('Input Length (chars)')\n",
    "axes[1, 1].set_ylabel('Confidence Score')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Check samples that exceed max length\n",
    "long_samples = df_analysis[df_analysis['total_len'] > CONFIG['max_length']]\n",
    "print(f\"\\n‚ö†Ô∏è Samples exceeding max_length ({CONFIG['max_length']}): {len(long_samples)} ({len(long_samples)/len(df_analysis)*100:.1f}%)\")\n",
    "if len(long_samples) > 0:\n",
    "    print(f\"   Longest sample: {long_samples['total_len'].max()} chars\")\n",
    "    print(f\"   Recommendation: Consider increasing max_length or truncating\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a63316b",
   "metadata": {},
   "source": [
    "## 4. Model and Tokenizer Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d79f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model and tokenizer using Unsloth\n",
    "print(f\"Loading Unsloth model: {CONFIG['model_name']}\")\n",
    "print(f\"Max sequence length: {CONFIG['max_seq_length']}\")\n",
    "\n",
    "# Clear any existing model artifacts to prevent conflicts\n",
    "import gc\n",
    "import os\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "# Remove any existing model directories that might cause conflicts\n",
    "if os.path.exists(CONFIG['output_dir']):\n",
    "    print(f\"‚ö†Ô∏è Removing existing output directory: {CONFIG['output_dir']}\")\n",
    "    import shutil\n",
    "    shutil.rmtree(CONFIG['output_dir'])\n",
    "\n",
    "try:\n",
    "    # Load fresh model and tokenizer\n",
    "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name=CONFIG['model_name'],\n",
    "        max_seq_length=CONFIG['max_seq_length'],\n",
    "        dtype=CONFIG['dtype'],\n",
    "        load_in_4bit=CONFIG['load_in_4bit'],\n",
    "        # Add these parameters to ensure clean loading\n",
    "        trust_remote_code=True,\n",
    "        use_cache=False,  # Disable cache during training setup\n",
    "    )\n",
    "    \n",
    "    print(f\"‚úÖ Unsloth model loaded successfully!\")\n",
    "    print(f\"  Model type: {type(model)}\")\n",
    "    print(f\"  Tokenizer vocab size: {tokenizer.vocab_size:,}\")\n",
    "    print(f\"  Model vocab size: {model.config.vocab_size:,}\")\n",
    "    print(f\"  Max length: {tokenizer.model_max_length}\")\n",
    "    \n",
    "    # Ensure tokenizer settings are correct\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        print(f\"  Set pad token to EOS token: {tokenizer.eos_token}\")\n",
    "    \n",
    "    # Verify embedding dimensions\n",
    "    embed_dim = model.get_input_embeddings().weight.shape\n",
    "    print(f\"  Embedding dimensions: {embed_dim}\")\n",
    "    \n",
    "    # Memory info\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"  GPU memory allocated: {torch.cuda.memory_allocated()/1024**3:.2f} GB\")\n",
    "        print(f\"  GPU memory reserved: {torch.cuda.memory_reserved()/1024**3:.2f} GB\")\n",
    "    \n",
    "except Exception as model_error:\n",
    "    print(f\"‚ùå Model loading failed: {model_error}\")\n",
    "    \n",
    "    # Check if it's a shape mismatch error\n",
    "    if \"size mismatch\" in str(model_error):\n",
    "        print(\"üîç This appears to be a model shape mismatch error\")\n",
    "        print(\"   This usually happens when:\")\n",
    "        print(\"   1. Trying to load a model with different vocabulary size\")\n",
    "        print(\"   2. Loading incompatible adapters or checkpoints\")\n",
    "        print(\"   3. Model cache corruption\")\n",
    "        \n",
    "        print(\"\\nüîß Attempting to fix with clean model loading...\")\n",
    "        \n",
    "        # Clear all caches and try again\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        \n",
    "        # Try loading with explicit parameters\n",
    "        try:\n",
    "            model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "                model_name=CONFIG['model_name'],\n",
    "                max_seq_length=CONFIG['max_seq_length'],\n",
    "                dtype=None,  # Let it auto-detect\n",
    "                load_in_4bit=True,\n",
    "                trust_remote_code=True,\n",
    "                use_cache=False,\n",
    "                torch_dtype=\"auto\",\n",
    "                # Force clean loading\n",
    "                revision=\"main\",  # Use main branch\n",
    "            )\n",
    "            print(\"‚úÖ Clean model loading successful!\")\n",
    "            \n",
    "        except Exception as retry_error:\n",
    "            print(f\"‚ùå Retry also failed: {retry_error}\")\n",
    "            print(\"\\nüí° Solution: Try completely restarting the notebook\")\n",
    "            print(\"   This will clear any cached model states that might be causing conflicts\")\n",
    "            raise retry_error\n",
    "    else:\n",
    "        print(f\"üîç Unexpected model loading error: {model_error}\")\n",
    "        raise model_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d7a3d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply LoRA using Unsloth with enhanced error handling\n",
    "print(\"Applying LoRA adapters using Unsloth...\")\n",
    "\n",
    "try:\n",
    "    # Ensure model is in the right state for LoRA application\n",
    "    if hasattr(model, 'peft_config'):\n",
    "        print(\"‚ö†Ô∏è Model already has PEFT adapters, this might cause conflicts\")\n",
    "        print(\"   Consider restarting the notebook for clean training\")\n",
    "    \n",
    "    # Apply LoRA with Unsloth optimizations\n",
    "    model = FastLanguageModel.get_peft_model(\n",
    "        model,\n",
    "        r=CONFIG['lora_r'],\n",
    "        target_modules=CONFIG['target_modules'],\n",
    "        lora_alpha=CONFIG['lora_alpha'],\n",
    "        lora_dropout=CONFIG['lora_dropout'],\n",
    "        bias=\"none\",\n",
    "        use_gradient_checkpointing=\"unsloth\",  # Unsloth's optimized gradient checkpointing\n",
    "        random_state=3407,\n",
    "        use_rslora=False,  # Rank stabilized LoRA\n",
    "        loftq_config=None,  # LoftQ quantization\n",
    "    )\n",
    "    \n",
    "    print(f\"‚úÖ LoRA applied successfully!\")\n",
    "    \n",
    "except Exception as lora_error:\n",
    "    print(f\"‚ùå LoRA application failed: {lora_error}\")\n",
    "    \n",
    "    # Analyze the error\n",
    "    error_str = str(lora_error).lower()\n",
    "    if \"already exists\" in error_str or \"adapter\" in error_str:\n",
    "        print(\"üîç This appears to be an adapter conflict\")\n",
    "        print(\"   The model may already have adapters applied\")\n",
    "        print(\"   Solutions:\")\n",
    "        print(\"   1. Restart the notebook kernel\")\n",
    "        print(\"   2. Use a fresh model instance\")\n",
    "        print(\"   3. Remove existing adapters before applying new ones\")\n",
    "        \n",
    "    elif \"shape\" in error_str or \"size\" in error_str:\n",
    "        print(\"üîç This appears to be a shape mismatch in LoRA layers\")\n",
    "        print(\"   This can happen with mismatched model versions\")\n",
    "        print(\"   Try using the exact model specification\")\n",
    "        \n",
    "    else:\n",
    "        print(f\"üîç Unexpected LoRA error: {lora_error}\")\n",
    "    \n",
    "    raise lora_error\n",
    "\n",
    "# Print trainable parameters\n",
    "def print_trainable_parameters(model):\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(f\"Trainable params: {trainable_params:,} || All params: {all_param:,} || Trainable%: {100 * trainable_params / all_param:.2f}%\")\n",
    "\n",
    "print_trainable_parameters(model)\n",
    "\n",
    "print(f\"\\n‚úÖ Unsloth LoRA configuration complete!\")\n",
    "print(f\"  LoRA Rank: {CONFIG['lora_r']}\")\n",
    "print(f\"  LoRA Alpha: {CONFIG['lora_alpha']}\")\n",
    "print(f\"  Target modules: {CONFIG['target_modules']}\")\n",
    "\n",
    "# Verify model state\n",
    "print(f\"\\nüîç Model verification:\")\n",
    "print(f\"  Model class: {model.__class__.__name__}\")\n",
    "print(f\"  Has PEFT config: {hasattr(model, 'peft_config')}\")\n",
    "if hasattr(model, 'peft_config'):\n",
    "    print(f\"  PEFT adapters: {list(model.peft_config.keys())}\")\n",
    "\n",
    "# Memory check after LoRA\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"  GPU memory after LoRA: {torch.cuda.memory_allocated()/1024**3:.2f} GB\")\n",
    "    \n",
    "# Clear any temporary variables\n",
    "gc.collect()\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e3f2a6",
   "metadata": {},
   "source": [
    "## 5. Unsloth LoRA Configuration (Already Applied Above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d20d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up Unsloth chat template for Qwen2.5\n",
    "print(\"Setting up Unsloth chat template...\")\n",
    "\n",
    "# Apply Qwen2.5 chat template\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template=\"qwen-2.5\",  # Use Qwen2.5 specific template\n",
    ")\n",
    "\n",
    "def formatting_prompts_func(examples):\n",
    "    \"\"\"Format examples using Unsloth's optimized chat template - Fixed for tensor creation\"\"\"\n",
    "    convos = []\n",
    "    \n",
    "    # Handle both single examples and batched examples\n",
    "    if isinstance(examples[\"instruction\"], str):\n",
    "        # Single example\n",
    "        instructions = [examples[\"instruction\"]]\n",
    "        inputs = [examples[\"input\"]]\n",
    "        outputs = [examples[\"output\"]]\n",
    "    else:\n",
    "        # Batched examples\n",
    "        instructions = examples[\"instruction\"]\n",
    "        inputs = examples[\"input\"]\n",
    "        outputs = examples[\"output\"]\n",
    "    \n",
    "    for instruction, input_text, output in zip(instructions, inputs, outputs):\n",
    "        # Ensure all inputs are strings\n",
    "        instruction = str(instruction) if instruction is not None else \"\"\n",
    "        input_text = str(input_text) if input_text is not None else \"\"\n",
    "        output = str(output) if output is not None else \"\"\n",
    "        \n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": instruction},\n",
    "            {\"role\": \"user\", \"content\": input_text},\n",
    "            {\"role\": \"assistant\", \"content\": output}\n",
    "        ]\n",
    "        \n",
    "        try:\n",
    "            convo = tokenizer.apply_chat_template(\n",
    "                messages, \n",
    "                tokenize=False, \n",
    "                add_generation_prompt=False\n",
    "            )\n",
    "            # Ensure convo is a string\n",
    "            if isinstance(convo, str):\n",
    "                convos.append(convo)\n",
    "            else:\n",
    "                print(f\"Warning: Chat template returned non-string: {type(convo)}\")\n",
    "                convos.append(str(convo))\n",
    "        except Exception as e:\n",
    "            print(f\"Error formatting conversation: {e}\")\n",
    "            # Fallback to simple concatenation\n",
    "            fallback_text = f\"System: {instruction}\\nUser: {input_text}\\nAssistant: {output}\"\n",
    "            convos.append(fallback_text)\n",
    "    \n",
    "    # Ensure we return a flat list of strings\n",
    "    return {\"text\": convos}\n",
    "\n",
    "# Test the formatting function\n",
    "print(\"‚úÖ Testing Unsloth formatting function...\")\n",
    "test_sample = {\n",
    "    \"instruction\": \"Test instruction\",\n",
    "    \"input\": \"Test input\",\n",
    "    \"output\": \"Test output\"\n",
    "}\n",
    "\n",
    "test_result = formatting_prompts_func(test_sample)\n",
    "print(f\"‚úÖ Test successful: {type(test_result)} with text type: {type(test_result['text'][0])}\")\n",
    "print(f\"‚úÖ Unsloth chat template configured!\")\n",
    "print(f\"  Template: Qwen2.5 optimized format\")\n",
    "print(f\"  Tokenizer ready for training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "956b9d13",
   "metadata": {},
   "source": [
    "## 6. Data Processing and Formatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ccd01a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Unsloth formatting on a sample\n",
    "sample = dataset['train'][0]\n",
    "sample_formatted = formatting_prompts_func({\n",
    "    \"instruction\": [sample[\"instruction\"]], \n",
    "    \"input\": [sample[\"input\"]], \n",
    "    \"output\": [sample[\"output\"]]\n",
    "})\n",
    "\n",
    "print(\"Unsloth formatted sample (first 500 chars):\")\n",
    "print(sample_formatted['text'][0][:500] + \"...\")\n",
    "print(f\"\\nFormatted length: {len(sample_formatted['text'][0])} chars\")\n",
    "\n",
    "# Verify the format works with tokenizer\n",
    "test_tokens = tokenizer(\n",
    "    sample_formatted['text'][0], \n",
    "    truncation=True, \n",
    "    max_length=CONFIG['max_seq_length']\n",
    ")\n",
    "print(f\"Tokenized length: {len(test_tokens['input_ids'])} tokens\")\n",
    "print(f\"‚úÖ Unsloth formatting working correctly!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf2d027",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process dataset for Unsloth training with enhanced validation\n",
    "print(\"Processing dataset for Unsloth training...\")\n",
    "\n",
    "# Split dataset first (before formatting to save memory)\n",
    "train_size = int(CONFIG['train_split'] * len(dataset['train']))\n",
    "eval_size = int(CONFIG['eval_split'] * len(dataset['train']))\n",
    "test_size = len(dataset['train']) - train_size - eval_size\n",
    "\n",
    "print(f\"Dataset splits: Train={train_size}, Eval={eval_size}, Test={test_size}\")\n",
    "\n",
    "# Create splits\n",
    "dataset_splits = dataset['train'].train_test_split(\n",
    "    test_size=eval_size + test_size, \n",
    "    shuffle=True, \n",
    "    seed=42\n",
    ")\n",
    "train_dataset_raw = dataset_splits['train']\n",
    "\n",
    "eval_test_splits = dataset_splits['test'].train_test_split(\n",
    "    test_size=test_size,\n",
    "    shuffle=True,\n",
    "    seed=42\n",
    ")\n",
    "eval_dataset_raw = eval_test_splits['train']\n",
    "test_dataset_raw = eval_test_splits['test']\n",
    "\n",
    "print(\"‚úÖ Dataset splits created\")\n",
    "\n",
    "# Validation function to check data integrity\n",
    "def validate_dataset_sample(sample, idx):\n",
    "    \"\"\"Validate a single dataset sample\"\"\"\n",
    "    required_keys = ['instruction', 'input', 'output']\n",
    "    for key in required_keys:\n",
    "        if key not in sample:\n",
    "            raise ValueError(f\"Missing key '{key}' in sample {idx}\")\n",
    "        if sample[key] is None:\n",
    "            sample[key] = \"\"  # Replace None with empty string\n",
    "        if not isinstance(sample[key], str):\n",
    "            sample[key] = str(sample[key])  # Convert to string\n",
    "    return sample\n",
    "\n",
    "# Apply enhanced formatting with validation\n",
    "def safe_formatting(dataset_split, split_name):\n",
    "    \"\"\"Safely format dataset with comprehensive error handling\"\"\"\n",
    "    print(f\"Formatting {split_name} dataset...\")\n",
    "    \n",
    "    # First validate all samples\n",
    "    validated_samples = []\n",
    "    for i, sample in enumerate(dataset_split):\n",
    "        try:\n",
    "            validated_sample = validate_dataset_sample(sample, i)\n",
    "            validated_samples.append(validated_sample)\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Skipping invalid sample {i} in {split_name}: {e}\")\n",
    "    \n",
    "    print(f\"‚úÖ Validated {len(validated_samples)}/{len(dataset_split)} samples in {split_name}\")\n",
    "    \n",
    "    # Create new dataset from validated samples\n",
    "    from datasets import Dataset\n",
    "    validated_dataset = Dataset.from_list(validated_samples)\n",
    "    \n",
    "    # Apply formatting\n",
    "    try:\n",
    "        formatted_dataset = validated_dataset.map(\n",
    "            formatting_prompts_func, \n",
    "            batched=True,\n",
    "            batch_size=10,  # Smaller batches for safety\n",
    "            remove_columns=validated_dataset.column_names,\n",
    "            desc=f\"Formatting {split_name}\"\n",
    "        )\n",
    "        \n",
    "        # Validate formatted output\n",
    "        print(f\"üîç Validating formatted {split_name} dataset...\")\n",
    "        for i in range(min(3, len(formatted_dataset))):\n",
    "            sample = formatted_dataset[i]\n",
    "            if 'text' not in sample:\n",
    "                raise ValueError(f\"Missing 'text' key in formatted sample {i}\")\n",
    "            if not isinstance(sample['text'], str):\n",
    "                raise ValueError(f\"Non-string text in sample {i}: {type(sample['text'])}\")\n",
    "        \n",
    "        print(f\"‚úÖ {split_name} dataset formatted and validated successfully\")\n",
    "        return formatted_dataset\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error formatting {split_name} dataset: {e}\")\n",
    "        print(\"üîÑ Trying individual sample processing...\")\n",
    "        \n",
    "        # Fallback: process samples individually\n",
    "        formatted_texts = []\n",
    "        for i, sample in enumerate(validated_samples):\n",
    "            try:\n",
    "                result = formatting_prompts_func(sample)\n",
    "                if 'text' in result and len(result['text']) > 0:\n",
    "                    text = result['text'][0] if isinstance(result['text'], list) else result['text']\n",
    "                    if isinstance(text, str) and len(text.strip()) > 0:\n",
    "                        formatted_texts.append(text)\n",
    "            except Exception as sample_error:\n",
    "                print(f\"‚ö†Ô∏è Failed to format sample {i}: {sample_error}\")\n",
    "        \n",
    "        if formatted_texts:\n",
    "            fallback_dataset = Dataset.from_dict({\"text\": formatted_texts})\n",
    "            print(f\"‚úÖ Fallback processing successful for {split_name}: {len(fallback_dataset)} samples\")\n",
    "            return fallback_dataset\n",
    "        else:\n",
    "            raise ValueError(f\"‚ùå No samples could be formatted in {split_name}\")\n",
    "\n",
    "# Process all splits safely\n",
    "train_dataset = safe_formatting(train_dataset_raw, \"train\")\n",
    "eval_dataset = safe_formatting(eval_dataset_raw, \"eval\")\n",
    "test_dataset = safe_formatting(test_dataset_raw, \"test\")\n",
    "\n",
    "print(f\"\\n‚úÖ Unsloth dataset processing complete!\")\n",
    "print(f\"  Train: {len(train_dataset):,} samples ({len(train_dataset)/len(dataset['train'])*100:.1f}%)\")\n",
    "print(f\"  Eval: {len(eval_dataset):,} samples ({len(eval_dataset)/len(dataset['train'])*100:.1f}%)\")\n",
    "print(f\"  Test: {len(test_dataset):,} samples ({len(test_dataset)/len(dataset['train'])*100:.1f}%)\")\n",
    "\n",
    "# Final validation check\n",
    "print(\"üîç Final validation check...\")\n",
    "for dataset_name, ds in [(\"train\", train_dataset), (\"eval\", eval_dataset), (\"test\", test_dataset)]:\n",
    "    sample = ds[0]\n",
    "    print(f\"  {dataset_name}: text type = {type(sample['text'])}, length = {len(sample['text'])}\")\n",
    "    \n",
    "    # Test tokenization\n",
    "    try:\n",
    "        test_tokens = tokenizer(sample['text'], truncation=True, max_length=CONFIG['max_seq_length'])\n",
    "        print(f\"  {dataset_name}: tokenization successful ({len(test_tokens['input_ids'])} tokens)\")\n",
    "    except Exception as e:\n",
    "        print(f\"  {dataset_name}: tokenization failed - {e}\")\n",
    "        raise e\n",
    "\n",
    "print(\"‚úÖ All datasets ready for training!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "375cadc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîç Comprehensive Dataset Validation to Prevent Tensor Errors\n",
    "print(\"üîç Performing comprehensive dataset validation...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "def validate_dataset_for_training(dataset):\n",
    "    \"\"\"\n",
    "    Thoroughly validate dataset to prevent tensor creation errors\n",
    "    This catches the exact issues that cause training failures\n",
    "    \"\"\"\n",
    "    print(f\"üìä Dataset overview:\")\n",
    "    print(f\"  Size: {len(dataset)} samples\")\n",
    "    print(f\"  Columns: {dataset.column_names}\")\n",
    "    \n",
    "    # Check for required column\n",
    "    if 'text' not in dataset.column_names:\n",
    "        raise ValueError(\"‚ùå Dataset missing 'text' column\")\n",
    "    \n",
    "    issues = []\n",
    "    sample_texts = []\n",
    "    \n",
    "    # Check each sample thoroughly\n",
    "    for i in range(min(20, len(dataset))):  # Check first 20 samples\n",
    "        try:\n",
    "            sample = dataset[i]\n",
    "            \n",
    "            # Check if text field exists\n",
    "            if 'text' not in sample:\n",
    "                issues.append(f\"Sample {i}: Missing 'text' field\")\n",
    "                continue\n",
    "            \n",
    "            text = sample['text']\n",
    "            \n",
    "            # Check text type - this is critical for tensor creation\n",
    "            if not isinstance(text, str):\n",
    "                issues.append(f\"Sample {i}: text is {type(text)}, must be str\")\n",
    "                continue\n",
    "            \n",
    "            # Check for empty text\n",
    "            if not text.strip():\n",
    "                issues.append(f\"Sample {i}: Empty text content\")\n",
    "                continue\n",
    "            \n",
    "            # Check for nested structures (common cause of tensor errors)\n",
    "            if isinstance(text, (list, tuple, dict)):\n",
    "                issues.append(f\"Sample {i}: text contains nested structure {type(text)}\")\n",
    "                continue\n",
    "            \n",
    "            # Test tokenization - this is where tensor errors often occur\n",
    "            try:\n",
    "                tokens = tokenizer(\n",
    "                    text,\n",
    "                    truncation=True,\n",
    "                    max_length=CONFIG['max_seq_length'],\n",
    "                    return_tensors=\"pt\"\n",
    "                )\n",
    "                \n",
    "                # Validate token structure\n",
    "                if 'input_ids' not in tokens:\n",
    "                    issues.append(f\"Sample {i}: Tokenization missing input_ids\")\n",
    "                    continue\n",
    "                \n",
    "                if tokens['input_ids'].dim() != 2:\n",
    "                    issues.append(f\"Sample {i}: Wrong token dimensions {tokens['input_ids'].shape}\")\n",
    "                    continue\n",
    "                \n",
    "                if tokens['input_ids'].size(0) != 1:\n",
    "                    issues.append(f\"Sample {i}: Wrong batch dimension {tokens['input_ids'].size(0)}\")\n",
    "                    continue\n",
    "                \n",
    "                sample_texts.append(text)\n",
    "                \n",
    "            except Exception as token_error:\n",
    "                issues.append(f\"Sample {i}: Tokenization failed - {token_error}\")\n",
    "                continue\n",
    "                \n",
    "        except Exception as sample_error:\n",
    "            issues.append(f\"Sample {i}: Processing failed - {sample_error}\")\n",
    "            continue\n",
    "    \n",
    "    # Test batch processing (this is where training fails)\n",
    "    if len(sample_texts) >= 2:\n",
    "        print(f\"üß™ Testing batch tokenization with {min(5, len(sample_texts))} samples...\")\n",
    "        try:\n",
    "            batch_texts = sample_texts[:min(5, len(sample_texts))]\n",
    "            batch_tokens = tokenizer(\n",
    "                batch_texts,\n",
    "                truncation=True,\n",
    "                padding=True,\n",
    "                max_length=CONFIG['max_seq_length'],\n",
    "                return_tensors=\"pt\"\n",
    "            )\n",
    "            \n",
    "            print(f\"  ‚úÖ Batch tokenization successful!\")\n",
    "            print(f\"     Batch shape: {batch_tokens['input_ids'].shape}\")\n",
    "            print(f\"     Token range: {batch_tokens['input_ids'].min()} to {batch_tokens['input_ids'].max()}\")\n",
    "            \n",
    "            # Clean up batch test\n",
    "            del batch_tokens\n",
    "            \n",
    "        except Exception as batch_error:\n",
    "            issues.append(f\"Batch processing: {batch_error}\")\n",
    "            print(f\"  ‚ùå Batch tokenization failed: {batch_error}\")\n",
    "    \n",
    "    # Report results\n",
    "    if issues:\n",
    "        print(f\"\\n‚ùå Found {len(issues)} validation issues:\")\n",
    "        for issue in issues[:10]:  # Show first 10 issues\n",
    "            print(f\"   ‚Ä¢ {issue}\")\n",
    "        if len(issues) > 10:\n",
    "            print(f\"   ‚Ä¢ ... and {len(issues) - 10} more issues\")\n",
    "        \n",
    "        return False, issues\n",
    "    else:\n",
    "        print(f\"\\n‚úÖ All validation checks passed!\")\n",
    "        print(f\"   Validated {min(20, len(dataset))} samples\")\n",
    "        print(f\"   Ready for tensor-safe training\")\n",
    "        return True, []\n",
    "\n",
    "# Run validation\n",
    "is_valid, validation_issues = validate_dataset_for_training(train_dataset)\n",
    "\n",
    "if not is_valid:\n",
    "    print(f\"\\nüîß Attempting to fix dataset issues...\")\n",
    "    \n",
    "    # Try to clean the dataset\n",
    "    clean_samples = []\n",
    "    \n",
    "    for i, sample in enumerate(train_dataset):\n",
    "        try:\n",
    "            if 'text' not in sample:\n",
    "                continue\n",
    "            \n",
    "            text = sample['text']\n",
    "            \n",
    "            # Ensure it's a string\n",
    "            if not isinstance(text, str):\n",
    "                text = str(text) if text is not None else \"\"\n",
    "            \n",
    "            # Ensure it's not empty\n",
    "            if not text.strip():\n",
    "                continue\n",
    "            \n",
    "            # Test tokenization\n",
    "            try:\n",
    "                test_tokens = tokenizer(\n",
    "                    text,\n",
    "                    truncation=True,\n",
    "                    max_length=CONFIG['max_seq_length']\n",
    "                )\n",
    "                if len(test_tokens['input_ids']) > 0:\n",
    "                    clean_samples.append({'text': text})\n",
    "            except:\n",
    "                continue\n",
    "                \n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    if clean_samples:\n",
    "        print(f\"üîß Created clean dataset: {len(clean_samples)} samples\")\n",
    "        train_dataset = Dataset.from_list(clean_samples)\n",
    "        \n",
    "        # Re-validate\n",
    "        is_valid, _ = validate_dataset_for_training(train_dataset)\n",
    "        if is_valid:\n",
    "            print(f\"‚úÖ Dataset cleaning successful!\")\n",
    "        else:\n",
    "            raise ValueError(\"‚ùå Dataset cleaning failed - training will likely fail\")\n",
    "    else:\n",
    "        raise ValueError(\"‚ùå No valid samples found after cleaning\")\n",
    "\n",
    "print(f\"\\nüéØ Final dataset ready for training:\")\n",
    "print(f\"   Samples: {len(train_dataset)}\")\n",
    "print(f\"   Validation: {'‚úÖ Passed' if is_valid else '‚ùå Failed'}\")\n",
    "print(f\"   Tensor compatibility: ‚úÖ Verified\")\n",
    "\n",
    "# Clear validation variables to save memory\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee5c6823",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate training statistics now that datasets are ready\n",
    "print(\"üìä Calculating training statistics...\")\n",
    "\n",
    "# Calculate training steps based on actual dataset size\n",
    "total_steps = len(train_dataset) // (CONFIG['per_device_train_batch_size'] * CONFIG['gradient_accumulation_steps']) * CONFIG['num_train_epochs']\n",
    "warmup_steps = int(total_steps * CONFIG['warmup_ratio'])\n",
    "\n",
    "print(\"üìã Training Statistics:\")\n",
    "print(f\"  Dataset size: {len(train_dataset):,} samples\")\n",
    "print(f\"  Effective batch size: {CONFIG['per_device_train_batch_size'] * CONFIG['gradient_accumulation_steps']}\")\n",
    "print(f\"  Steps per epoch: {len(train_dataset) // (CONFIG['per_device_train_batch_size'] * CONFIG['gradient_accumulation_steps']):,}\")\n",
    "print(f\"  Total training steps: {total_steps:,}\")\n",
    "print(f\"  Warmup steps: {warmup_steps:,} ({CONFIG['warmup_ratio']*100:.1f}% of total)\")\n",
    "print(f\"  Eval every: {CONFIG['eval_steps']} steps\")\n",
    "print(f\"  Save every: {CONFIG['save_steps']} steps\")\n",
    "\n",
    "# Estimate training time (rough estimate)\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"  Estimated training time: {total_steps * 2 / 60:.1f} minutes (rough estimate)\")\n",
    "\n",
    "print(\"‚úÖ Training statistics calculated!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14059e2a",
   "metadata": {},
   "source": [
    "## 7. Training Setup and Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b2bb33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unsloth optimized training arguments\n",
    "training_args = TrainingArguments(\n",
    "    # Output and logging\n",
    "    output_dir=CONFIG['output_dir'],\n",
    "    logging_dir=f\"{CONFIG['output_dir']}/logs\",\n",
    "    logging_steps=CONFIG['logging_steps'],\n",
    "    logging_strategy=\"steps\",\n",
    "    \n",
    "    # Training parameters (Unsloth optimized)\n",
    "    num_train_epochs=CONFIG['num_train_epochs'],\n",
    "    per_device_train_batch_size=CONFIG['per_device_train_batch_size'],\n",
    "    per_device_eval_batch_size=CONFIG['per_device_eval_batch_size'],\n",
    "    gradient_accumulation_steps=CONFIG['gradient_accumulation_steps'],\n",
    "    \n",
    "    # Optimization (Unsloth settings)\n",
    "    learning_rate=CONFIG['learning_rate'],\n",
    "    weight_decay=CONFIG['weight_decay'],\n",
    "    warmup_ratio=CONFIG['warmup_ratio'],\n",
    "    lr_scheduler_type=CONFIG['lr_scheduler_type'],\n",
    "    optim=CONFIG['optim'],\n",
    "    \n",
    "    # Memory and performance (Unsloth optimized)\n",
    "    gradient_checkpointing=CONFIG['gradient_checkpointing'],\n",
    "    dataloader_pin_memory=CONFIG['dataloader_pin_memory'],\n",
    "    bf16=CONFIG['bf16'],\n",
    "    fp16=CONFIG['fp16'],\n",
    "    group_by_length=True,  # Unsloth optimization\n",
    "    \n",
    "    # Evaluation and saving\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=CONFIG['eval_steps'],\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=CONFIG['save_steps'],\n",
    "    save_total_limit=CONFIG['save_total_limit'],\n",
    "    load_best_model_at_end=CONFIG['load_best_model_at_end'],\n",
    "    metric_for_best_model=CONFIG['metric_for_best_model'],\n",
    "    greater_is_better=CONFIG['greater_is_better'],\n",
    "    \n",
    "    # Misc\n",
    "    seed=42,\n",
    "    data_seed=42,\n",
    "    remove_unused_columns=False,\n",
    "    report_to=\"wandb\" if os.getenv(\"WANDB_API_KEY\") else \"none\",\n",
    "    run_name=f\"bitcoin-qwen-unsloth-{datetime.now().strftime('%Y%m%d-%H%M%S')}\",\n",
    "    \n",
    "    # Hub integration\n",
    "    push_to_hub=True,\n",
    "    hub_model_id=CONFIG['hub_model_id'],\n",
    "    hub_strategy=\"every_save\",\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Training arguments created!\")\n",
    "print(\"üìã Key settings:\")\n",
    "print(f\"  Epochs: {CONFIG['num_train_epochs']}\")\n",
    "print(f\"  Train batch size: {CONFIG['per_device_train_batch_size']} (effective: {CONFIG['per_device_train_batch_size'] * CONFIG['gradient_accumulation_steps']})\")\n",
    "print(f\"  Learning rate: {CONFIG['learning_rate']}\")\n",
    "print(f\"  Precision: {'BFloat16' if CONFIG['bf16'] else 'Float16'}\")\n",
    "print(f\"  Output dir: {CONFIG['output_dir']}\")\n",
    "print(f\"  Model ID: {CONFIG['hub_model_id']}\")\n",
    "print(f\"  Eval every: {CONFIG['eval_steps']} steps\")\n",
    "print(\"Note: Training statistics will be calculated after dataset preparation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add24617",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom callback to show model outputs during training\n",
    "from transformers import TrainerCallback\n",
    "import random\n",
    "import re\n",
    "\n",
    "class SampleOutputCallback(TrainerCallback):\n",
    "    def __init__(self, model, tokenizer, eval_dataset, show_every_n_steps=50):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.eval_dataset = eval_dataset\n",
    "        self.show_every_n_steps = show_every_n_steps\n",
    "        self.sample_data = []\n",
    "        \n",
    "        # Prepare a few samples for testing\n",
    "        for i in range(min(5, len(eval_dataset))):\n",
    "            sample_text = eval_dataset[i]['text']\n",
    "            # Extract parts for generation\n",
    "            parts = sample_text.split('<|im_start|>assistant\\n')\n",
    "            if len(parts) >= 2:\n",
    "                input_part = parts[0] + '<|im_start|>assistant\\n'\n",
    "                expected_output = parts[1].replace('<|im_end|>', '').strip()\n",
    "                self.sample_data.append({\n",
    "                    'input': input_part,\n",
    "                    'expected': expected_output,\n",
    "                    'sample_id': i\n",
    "                })\n",
    "    \n",
    "    def on_step_end(self, args, state, control, **kwargs):\n",
    "        # Show sample every N steps\n",
    "        if state.global_step % self.show_every_n_steps == 0 and state.global_step > 0:\n",
    "            self._show_sample_output(state.global_step)\n",
    "    \n",
    "    def _show_sample_output(self, step):\n",
    "        if not self.sample_data:\n",
    "            return\n",
    "            \n",
    "        # Randomly select a sample\n",
    "        sample = random.choice(self.sample_data)\n",
    "        \n",
    "        print(f\"\\n\" + \"=\"*80)\n",
    "        print(f\"üéØ SAMPLE OUTPUT AT STEP {step} (Sample ID: {sample['sample_id']})\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        try:\n",
    "            # Tokenize input\n",
    "            inputs = self.tokenizer(\n",
    "                sample['input'], \n",
    "                return_tensors=\"pt\", \n",
    "                truncation=True, \n",
    "                max_length=1900  # Leave space for generation\n",
    "            ).to(self.model.device)\n",
    "            \n",
    "            # Generate response\n",
    "            self.model.eval()\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model.generate(\n",
    "                    **inputs,\n",
    "                    max_new_tokens=200,\n",
    "                    do_sample=True,\n",
    "                    temperature=0.1,\n",
    "                    pad_token_id=self.tokenizer.eos_token_id,\n",
    "                    eos_token_id=self.tokenizer.eos_token_id,\n",
    "                    use_cache=True\n",
    "                )\n",
    "            \n",
    "            # Decode response\n",
    "            generated_text = self.tokenizer.decode(\n",
    "                outputs[0][inputs.input_ids.shape[1]:], \n",
    "                skip_special_tokens=True\n",
    "            ).strip()\n",
    "            \n",
    "            # Extract JSON if present\n",
    "            json_match = re.search(r'\\{.*\\}', generated_text, re.DOTALL)\n",
    "            if json_match:\n",
    "                json_output = json_match.group()\n",
    "                print(f\"Generated JSON: {json_output}\")\n",
    "                \n",
    "                # Try to parse and validate\n",
    "                try:\n",
    "                    parsed = json.loads(json_output)\n",
    "                    print(f\"‚úÖ Valid JSON with keys: {list(parsed.keys())}\")\n",
    "                except:\n",
    "                    print(f\"‚ö†Ô∏è Invalid JSON format\")\n",
    "            else:\n",
    "                print(f\"Generated Text: {generated_text}\")\n",
    "                print(f\"‚ö†Ô∏è No JSON found in output\")\n",
    "            \n",
    "            print(f\"\\nExpected: {sample['expected']}\")\n",
    "            print(\"=\"*80)\n",
    "            \n",
    "            # Switch back to training mode\n",
    "            self.model.train()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error generating sample: {e}\")\n",
    "            print(\"=\"*80)\n",
    "\n",
    "# Initialize the callback\n",
    "sample_callback = SampleOutputCallback(\n",
    "    model=model, \n",
    "    tokenizer=tokenizer, \n",
    "    eval_dataset=eval_dataset,\n",
    "    show_every_n_steps=CONFIG['show_sample_every_n_steps']\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Sample output callback initialized!\")\n",
    "print(f\"  Will show outputs every {CONFIG['show_sample_every_n_steps']} steps\")\n",
    "print(f\"  Using {len(sample_callback.sample_data)} evaluation samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb07bec",
   "metadata": {},
   "source": [
    "## 8. Initialize Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "720e50e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Unsloth SFT Trainer with comprehensive error handling\n",
    "print(\"\ude80 Initializing Unsloth SFTTrainer with pre-tokenized data...\")\n",
    "\n",
    "# Verify all components are ready\n",
    "components_ready = {\n",
    "    \"Model\": model is not None,\n",
    "    \"Tokenizer\": tokenizer is not None, \n",
    "    \"Training Args\": training_args is not None,\n",
    "    \"Train dataset\": train_dataset is not None and len(train_dataset) > 0,\n",
    "    \"Eval dataset\": eval_dataset is not None and len(eval_dataset) > 0,\n",
    "    \"Data collator\": data_collator is not None,\n",
    "}\n",
    "\n",
    "print(\"üìä Component status:\")\n",
    "for component, status in components_ready.items():\n",
    "    print(f\"   {component}: {'‚úÖ' if status else '‚ùå'}\")\n",
    "\n",
    "if not all(components_ready.values()):\n",
    "    missing = [comp for comp, status in components_ready.items() if not status]\n",
    "    raise ValueError(f\"‚ùå Missing components: {missing}\")\n",
    "\n",
    "# Final tensor compatibility test with pre-tokenized data\n",
    "print(\"üß™ Testing tensor compatibility with pre-tokenized data...\")\n",
    "try:\n",
    "    # Test a small batch with the data collator\n",
    "    test_samples = [train_dataset[i] for i in range(min(3, len(train_dataset)))]\n",
    "    test_batch = data_collator(test_samples)\n",
    "    \n",
    "    print(f\"‚úÖ Tensor compatibility test passed!\")\n",
    "    print(f\"   Batch keys: {list(test_batch.keys())}\")\n",
    "    print(f\"   Input shape: {test_batch['input_ids'].shape}\")\n",
    "    print(f\"   Labels shape: {test_batch['labels'].shape}\")\n",
    "    \n",
    "    # Clean up test\n",
    "    del test_batch, test_samples\n",
    "    \n",
    "except Exception as compat_error:\n",
    "    print(f\"‚ùå Tensor compatibility test failed: {compat_error}\")\n",
    "    print(\"   The pre-tokenized data still has issues\")\n",
    "    raise compat_error\n",
    "\n",
    "# Initialize trainer with pre-tokenized datasets\n",
    "try:\n",
    "    trainer = SFTTrainer(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,  # Now pre-tokenized\n",
    "        eval_dataset=eval_dataset,    # Now pre-tokenized  \n",
    "        data_collator=data_collator,  # Use our custom data collator\n",
    "        # Remove these parameters since we're using pre-tokenized data\n",
    "        # dataset_text_field=None,    # Not needed for pre-tokenized data\n",
    "        # max_seq_length=None,        # Already applied during tokenization\n",
    "        # packing=False,              # Not applicable to pre-tokenized data\n",
    "        # formatting_func=None,       # Already formatted and tokenized\n",
    "        callbacks=[sample_callback] if 'sample_callback' in locals() else [],\n",
    "    )\n",
    "    \n",
    "    print(f\"‚úÖ Unsloth SFTTrainer initialized successfully!\")\n",
    "    print(f\"   Using pre-tokenized datasets\")\n",
    "    print(f\"   Data collator: {type(data_collator).__name__}\")\n",
    "    \n",
    "except Exception as trainer_error:\n",
    "    print(f\"‚ùå Trainer initialization failed: {trainer_error}\")\n",
    "    \n",
    "    # Detailed error analysis\n",
    "    error_str = str(trainer_error).lower()\n",
    "    \n",
    "    if \"tensor\" in error_str and (\"nesting\" in error_str or \"list\" in error_str):\n",
    "        print(\"üîç TENSOR CREATION ERROR DETECTED:\")\n",
    "        print(\"   Even with pre-tokenization, there are tensor issues\")\n",
    "        print(\"   This should not happen with the pre-tokenization fix\")\n",
    "        print(\"\\nüí° Solutions:\")\n",
    "        print(\"   1. Restart the notebook kernel completely\")\n",
    "        print(\"   2. Check the pre-tokenization function\") \n",
    "        print(\"   3. Verify data collator compatibility\")\n",
    "        \n",
    "    elif \"shape\" in error_str or \"size\" in error_str:\n",
    "        print(\"üîç SHAPE MISMATCH ERROR:\")\n",
    "        print(\"   This indicates model loading conflicts\")\n",
    "        print(\"   Likely cause: incompatible model states or adapters\")\n",
    "        print(\"\\nüí° Solutions:\")\n",
    "        print(\"   1. Restart notebook kernel\")\n",
    "        print(\"   2. Clear model cache directories\") \n",
    "        print(\"   3. Use fresh model loading\")\n",
    "    \n",
    "    elif \"adapter\" in error_str or \"peft\" in error_str:\n",
    "        print(\"üîç ADAPTER CONFLICT ERROR:\")\n",
    "        print(\"   The model may have conflicting adapters\")\n",
    "        print(\"\\nüí° Solutions:\")\n",
    "        print(\"   1. Remove existing adapter directories\")\n",
    "        print(\"   2. Use a fresh model instance\")\n",
    "        print(\"   3. Clear PEFT cache\")\n",
    "    \n",
    "    else:\n",
    "        print(f\"üîç UNKNOWN TRAINER ERROR:\")\n",
    "        print(f\"   {trainer_error}\")\n",
    "    \n",
    "    # Show full traceback for debugging\n",
    "    import traceback\n",
    "    print(f\"\\nüîç Full error traceback:\")\n",
    "    traceback.print_exc()\n",
    "    \n",
    "    raise trainer_error\n",
    "\n",
    "# Verify trainer is working\n",
    "print(f\"üìä Trainer verification:\")\n",
    "print(f\"   Model device: {model.device}\")\n",
    "print(f\"   Train dataset size: {len(trainer.train_dataset):,}\")\n",
    "print(f\"   Eval dataset size: {len(trainer.eval_dataset):,}\")\n",
    "print(f\"   Callbacks: {len(trainer.callback_handler.callbacks)} registered\")\n",
    "\n",
    "# Enable Unsloth training optimizations\n",
    "FastLanguageModel.for_training(model)\n",
    "print(f\"‚úÖ Unsloth training mode enabled\")\n",
    "\n",
    "# Final memory check\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"   GPU memory allocated: {torch.cuda.memory_allocated()/1024**3:.2f} GB\")\n",
    "    print(f\"   GPU memory reserved: {torch.cuda.memory_reserved()/1024**3:.2f} GB\")\n",
    "    \n",
    "    # RTX 3090 specific optimizations\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    if \"RTX 3090\" in gpu_name or \"3090\" in gpu_name:\n",
    "        print(f\"   üöÄ RTX 3090 detected - optimizations active\")\n",
    "        total_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "        used_memory = torch.cuda.memory_allocated() / 1024**3\n",
    "        print(f\"   üìä Memory utilization: {used_memory:.1f}/{total_memory:.1f} GB ({used_memory/total_memory*100:.1f}%)\")\n",
    "\n",
    "print(\"üéâ Trainer ready for tensor-safe training with pre-tokenized datasets!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa5cc70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîß Pre-tokenization Solution to Prevent Tensor Errors\n",
    "print(\"üîß Applying pre-tokenization fix...\")\n",
    "print(\"This prevents the 'excessive nesting' tensor error during training\")\n",
    "\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "def pre_tokenize_dataset(dataset, tokenizer, split_name):\n",
    "    \"\"\"\n",
    "    Pre-tokenize dataset to prevent tensor creation errors\n",
    "    This is the same fix that worked in train_news_effects.ipynb\n",
    "    \"\"\"\n",
    "    print(f\"Pre-tokenizing {split_name} dataset...\")\n",
    "    \n",
    "    def tokenize_function(examples):\n",
    "        # Tokenize the text\n",
    "        tokenized = tokenizer(\n",
    "            examples[\"text\"],\n",
    "            truncation=True,\n",
    "            padding=False,  # Don't pad here - let data collator handle it\n",
    "            max_length=CONFIG['max_seq_length'],\n",
    "            return_tensors=None  # Return lists, not tensors\n",
    "        )\n",
    "        return tokenized\n",
    "    \n",
    "    # Apply tokenization\n",
    "    tokenized_dataset = dataset.map(\n",
    "        tokenize_function,\n",
    "        batched=True,\n",
    "        batch_size=100,\n",
    "        remove_columns=dataset.column_names,  # Remove original columns\n",
    "        desc=f\"Tokenizing {split_name}\"\n",
    "    )\n",
    "    \n",
    "    print(f\"‚úÖ {split_name} dataset pre-tokenized: {len(tokenized_dataset)} samples\")\n",
    "    return tokenized_dataset\n",
    "\n",
    "# Pre-tokenize all datasets\n",
    "print(\"üìä Pre-tokenizing datasets to prevent training errors...\")\n",
    "train_dataset_tokenized = pre_tokenize_dataset(train_dataset, tokenizer, \"train\")\n",
    "eval_dataset_tokenized = pre_tokenize_dataset(eval_dataset, tokenizer, \"eval\")\n",
    "test_dataset_tokenized = pre_tokenize_dataset(test_dataset, tokenizer, \"test\")\n",
    "\n",
    "# Create data collator for pre-tokenized data\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False,  # We're doing causal LM, not masked LM\n",
    "    pad_to_multiple_of=8,  # Helps with tensor core efficiency\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Pre-tokenization complete!\")\n",
    "print(f\"  Train: {len(train_dataset_tokenized)} samples\")\n",
    "print(f\"  Eval: {len(eval_dataset_tokenized)} samples\") \n",
    "print(f\"  Test: {len(test_dataset_tokenized)} samples\")\n",
    "print(f\"  Data collator: {type(data_collator).__name__}\")\n",
    "\n",
    "# Replace original datasets with tokenized versions\n",
    "train_dataset = train_dataset_tokenized\n",
    "eval_dataset = eval_dataset_tokenized  \n",
    "test_dataset = test_dataset_tokenized\n",
    "\n",
    "print(\"üéØ Datasets ready for tensor-safe training!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31cf8065",
   "metadata": {},
   "source": [
    "## 9. Start Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bfdde4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize wandb if available\n",
    "if os.getenv(\"WANDB_API_KEY\"):\n",
    "    import wandb\n",
    "    wandb.init(\n",
    "        project=\"bitcoin-price-prediction-unsloth\",\n",
    "        name=training_args.run_name,\n",
    "        config=CONFIG,\n",
    "        tags=[\"unsloth\", \"qwen2.5\", \"bitcoin\", \"rtx3090\"]\n",
    "    )\n",
    "    print(\"Weights & Biases initialized for Unsloth training\")\n",
    "\n",
    "print(\"\\nüöÄ Starting Unsloth accelerated training...\")\n",
    "print(f\"   Model: {CONFIG['model_name']}\")\n",
    "print(f\"   Dataset: {CONFIG['dataset_name']}\")\n",
    "print(f\"   Output: {CONFIG['output_dir']}\")\n",
    "print(f\"   Epochs: {CONFIG['num_train_epochs']}\")\n",
    "print(f\"   LoRA rank: {CONFIG['lora_r']}\")\n",
    "print(f\"   Precision: {'BFloat16' if CONFIG['bf16'] else 'Float16'}\")\n",
    "print(f\"   Sample outputs every: {CONFIG['show_sample_every_n_steps']} steps\")\n",
    "\n",
    "# Clear cache before training\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# Start training\n",
    "training_start_time = datetime.now()\n",
    "print(f\"\\n‚è∞ Training started at: {training_start_time.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"üéØ Expected completion: ~{CONFIG['num_train_epochs'] * len(train_dataset) // (CONFIG['per_device_train_batch_size'] * CONFIG['gradient_accumulation_steps']) // 60} minutes\")\n",
    "print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4dce08c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "try:\n",
    "    train_result = trainer.train()\n",
    "    \n",
    "    training_end_time = datetime.now()\n",
    "    training_duration = training_end_time - training_start_time\n",
    "    \n",
    "    print(\"-\" * 80)\n",
    "    print(f\"‚úÖ Training completed!\")\n",
    "    print(f\"   Duration: {training_duration}\")\n",
    "    print(f\"   Final train loss: {train_result.training_loss:.4f}\")\n",
    "    print(f\"   Total steps: {train_result.global_step:,}\")\n",
    "    \n",
    "    # Save training metrics\n",
    "    metrics = {\n",
    "        \"training_loss\": train_result.training_loss,\n",
    "        \"global_step\": train_result.global_step,\n",
    "        \"training_duration\": str(training_duration),\n",
    "        \"start_time\": training_start_time.isoformat(),\n",
    "        \"end_time\": training_end_time.isoformat()\n",
    "    }\n",
    "    \n",
    "    with open(f\"{CONFIG['output_dir']}/training_metrics.json\", \"w\") as f:\n",
    "        json.dump(metrics, f, indent=2)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Training failed with error: {e}\")\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be6e7fd6",
   "metadata": {},
   "source": [
    "## 10. Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "390c1c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "print(\"üìä Evaluating model...\")\n",
    "\n",
    "eval_results = trainer.evaluate()\n",
    "print(f\"\\nEvaluation Results:\")\n",
    "for key, value in eval_results.items():\n",
    "    print(f\"  {key}: {value:.4f}\")\n",
    "\n",
    "# Save evaluation results\n",
    "with open(f\"{CONFIG['output_dir']}/eval_results.json\", \"w\") as f:\n",
    "    json.dump(eval_results, f, indent=2)\n",
    "\n",
    "print(f\"\\nEvaluation results saved to {CONFIG['output_dir']}/eval_results.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb2777b2",
   "metadata": {},
   "source": [
    "## 11. Model Testing and Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44436626",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test inference on a few samples\n",
    "def test_inference(model, tokenizer, test_samples, max_new_tokens=200):\n",
    "    \"\"\"Test model inference on samples\"\"\"\n",
    "    model.eval()\n",
    "    results = []\n",
    "    \n",
    "    for i, sample in enumerate(test_samples[:3]):  # Test first 3 samples\n",
    "        print(f\"\\n--- Test Sample {i+1} ---\")\n",
    "        \n",
    "        # Prepare input (without assistant response)\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": sample[\"instruction\"]},\n",
    "            {\"role\": \"user\", \"content\": sample[\"input\"]}\n",
    "        ]\n",
    "        \n",
    "        text = tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True\n",
    "        )\n",
    "        \n",
    "        # Tokenize and generate\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\").to(model.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                do_sample=True,\n",
    "                temperature=0.1,\n",
    "                pad_token_id=tokenizer.eos_token_id,\n",
    "                eos_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "        \n",
    "        # Decode response\n",
    "        response = tokenizer.decode(outputs[0][inputs.input_ids.shape[1]:], skip_special_tokens=True)\n",
    "        \n",
    "        print(f\"Expected: {sample['output']}\")\n",
    "        print(f\"Generated: {response.strip()}\")\n",
    "        \n",
    "        results.append({\n",
    "            \"sample_id\": i,\n",
    "            \"expected\": sample[\"output\"],\n",
    "            \"generated\": response.strip(),\n",
    "            \"input_length\": len(sample[\"input\"]),\n",
    "        })\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Convert test dataset back to original format for testing\n",
    "original_test_samples = []\n",
    "for i in range(min(5, len(test_dataset))):\n",
    "    # Parse the formatted text back to original components\n",
    "    text = test_dataset[i]['text']\n",
    "    \n",
    "    # Extract instruction, input, and output from formatted text\n",
    "    # This is a simplified extraction - in practice you might want to save original data\n",
    "    parts = text.split('<|im_start|>assistant\\n')\n",
    "    if len(parts) == 2:\n",
    "        input_part = parts[0]\n",
    "        output_part = parts[1].replace('<|im_end|>', '').strip()\n",
    "        \n",
    "        # Extract system and user messages\n",
    "        system_start = input_part.find('<|im_start|>system\\n') + len('<|im_start|>system\\n')\n",
    "        system_end = input_part.find('<|im_end|>', system_start)\n",
    "        instruction = input_part[system_start:system_end].strip()\n",
    "        \n",
    "        user_start = input_part.find('<|im_start|>user\\n') + len('<|im_start|>user\\n')\n",
    "        user_end = input_part.find('<|im_end|>', user_start)\n",
    "        user_input = input_part[user_start:user_end].strip()\n",
    "        \n",
    "        original_test_samples.append({\n",
    "            \"instruction\": instruction,\n",
    "            \"input\": user_input,\n",
    "            \"output\": output_part\n",
    "        })\n",
    "\n",
    "print(f\"Testing inference on {len(original_test_samples)} samples...\")\n",
    "test_results = test_inference(model, tokenizer, original_test_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d21c5857",
   "metadata": {},
   "source": [
    "## 12. Save and Push Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "411defba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the Unsloth model\n",
    "print(\"üíæ Saving Unsloth model...\")\n",
    "\n",
    "# Save LoRA adapter using Unsloth (much faster)\n",
    "model.save_pretrained(CONFIG['output_dir'])\n",
    "tokenizer.save_pretrained(CONFIG['output_dir'])\n",
    "\n",
    "# Save in multiple formats for flexibility\n",
    "print(\"Saving in different formats...\")\n",
    "\n",
    "# 1. Save as standard LoRA adapter\n",
    "model.save_pretrained(f\"{CONFIG['output_dir']}/lora_adapter\")\n",
    "\n",
    "# 2. Save as merged model (optional - takes more space but easier to use)\n",
    "try:\n",
    "    print(\"Saving merged model (this may take a moment)...\")\n",
    "    merged_model = model.merge_and_unload()\n",
    "    merged_model.save_pretrained(f\"{CONFIG['output_dir']}/merged_model\")\n",
    "    tokenizer.save_pretrained(f\"{CONFIG['output_dir']}/merged_model\")\n",
    "    print(\"‚úÖ Merged model saved successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Could not save merged model: {e}\")\n",
    "\n",
    "# 3. Save as GGUF for inference (Unsloth feature)\n",
    "try:\n",
    "    print(\"Saving as GGUF for efficient inference...\")\n",
    "    model.save_pretrained_gguf(\n",
    "        f\"{CONFIG['output_dir']}/gguf\",\n",
    "        tokenizer,\n",
    "        quantization_method=\"q4_k_m\"  # 4-bit quantization\n",
    "    )\n",
    "    print(\"‚úÖ GGUF model saved successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Could not save GGUF model: {e}\")\n",
    "\n",
    "print(f\"‚úÖ Unsloth model saved to: {CONFIG['output_dir']}\")\n",
    "\n",
    "# Save configuration\n",
    "config_file = f\"{CONFIG['output_dir']}/training_config.json\"\n",
    "with open(config_file, \"w\") as f:\n",
    "    # Convert values to JSON serializable format\n",
    "    config_copy = CONFIG.copy()\n",
    "    config_copy['bf16'] = bool(CONFIG['bf16'])\n",
    "    config_copy['fp16'] = bool(CONFIG['fp16'])\n",
    "    json.dump(config_copy, f, indent=2)\n",
    "\n",
    "print(f\"‚úÖ Configuration saved to: {config_file}\")\n",
    "\n",
    "# List all saved files\n",
    "import os\n",
    "saved_files = []\n",
    "for root, dirs, files in os.walk(CONFIG['output_dir']):\n",
    "    for file in files:\n",
    "        saved_files.append(os.path.relpath(os.path.join(root, file), CONFIG['output_dir']))\n",
    "\n",
    "print(f\"\\nüìÅ Files saved ({len(saved_files)} total):\")\n",
    "for f in sorted(saved_files)[:10]:  # Show first 10 files\n",
    "    print(f\"  ‚Ä¢ {f}\")\n",
    "if len(saved_files) > 10:\n",
    "    print(f\"  ... and {len(saved_files) - 10} more files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca1cebd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Push Unsloth model to Hugging Face Hub\n",
    "if CONFIG.get('hub_model_id'):\n",
    "    try:\n",
    "        print(f\"üöÄ Pushing Unsloth model to Hugging Face Hub: {CONFIG['hub_model_id']}\")\n",
    "        \n",
    "        # Push the LoRA adapter\n",
    "        model.push_to_hub(CONFIG['hub_model_id'], token=True)\n",
    "        tokenizer.push_to_hub(CONFIG['hub_model_id'], token=True)\n",
    "        \n",
    "        # Create enhanced model card for Unsloth\n",
    "        model_card = f\"\"\"---\n",
    "license: apache-2.0\n",
    "base_model: {CONFIG['model_name']}\n",
    "library_name: unsloth\n",
    "tags:\n",
    "- bitcoin\n",
    "- cryptocurrency\n",
    "- trading\n",
    "- price-prediction\n",
    "- qwen2.5\n",
    "- unsloth\n",
    "- lora\n",
    "- peft\n",
    "- rtx3090-optimized\n",
    "datasets:\n",
    "- {CONFIG['dataset_name']}\n",
    "metrics:\n",
    "- loss\n",
    "model-index:\n",
    "- name: {CONFIG['hub_model_id']}\n",
    "  results:\n",
    "  - task:\n",
    "      type: text-generation\n",
    "      name: Bitcoin Price Prediction\n",
    "    metrics:\n",
    "    - type: loss\n",
    "      value: {train_result.training_loss:.4f}\n",
    "      name: training_loss\n",
    "---\n",
    "\n",
    "# Bitcoin Price Prediction - Qwen2.5 4B Unsloth LoRA\n",
    "\n",
    "This model is a fine-tuned version of {CONFIG['model_name']} for Bitcoin price prediction and trading decisions, optimized using **Unsloth** for 2x faster training on RTX 3090.\n",
    "\n",
    "## üöÄ Unsloth Optimizations\n",
    "\n",
    "- **2x faster training** compared to standard methods\n",
    "- **RTX 3090 optimized** with efficient memory usage\n",
    "- **BFloat16 precision** for better numerical stability\n",
    "- **Optimized gradient checkpointing** for memory efficiency\n",
    "\n",
    "## Model Details\n",
    "\n",
    "- **Base Model**: {CONFIG['model_name']}\n",
    "- **Fine-tuning Method**: Unsloth LoRA (Low-Rank Adaptation)\n",
    "- **Task**: Bitcoin trading decision and 10-day price forecasting\n",
    "- **Dataset**: {CONFIG['dataset_name']}\n",
    "- **Training Samples**: {len(train_dataset):,}\n",
    "\n",
    "## Training Configuration\n",
    "\n",
    "- **LoRA Rank**: {CONFIG['lora_r']} (optimized for Unsloth)\n",
    "- **LoRA Alpha**: {CONFIG['lora_alpha']}\n",
    "- **Learning Rate**: {CONFIG['learning_rate']}\n",
    "- **Epochs**: {CONFIG['num_train_epochs']}\n",
    "- **Batch Size**: {CONFIG['per_device_train_batch_size']} (effective: {CONFIG['per_device_train_batch_size'] * CONFIG['gradient_accumulation_steps']})\n",
    "- **Precision**: {'BFloat16' if CONFIG['bf16'] else 'Float16'}\n",
    "\n",
    "## Usage with Unsloth\n",
    "\n",
    "```python\n",
    "from unsloth import FastLanguageModel\n",
    "\n",
    "# Load model and tokenizer\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=\"{CONFIG['hub_model_id']}\",\n",
    "    max_seq_length=2048,\n",
    "    dtype=None,  # Auto-detect\n",
    "    load_in_4bit=True,\n",
    ")\n",
    "\n",
    "# Enable fast inference\n",
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "# Prepare input\n",
    "messages = [\n",
    "    {{\"role\": \"system\", \"content\": \"Your instruction here...\"}},\n",
    "    {{\"role\": \"user\", \"content\": \"Your trading context here...\"}}\n",
    "]\n",
    "\n",
    "inputs = tokenizer.apply_chat_template(messages, tokenize=True, return_tensors=\"pt\")\n",
    "\n",
    "# Generate prediction\n",
    "outputs = model.generate(**inputs, max_new_tokens=200, temperature=0.1)\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(response)\n",
    "```\n",
    "\n",
    "## Standard Usage (without Unsloth)\n",
    "\n",
    "```python\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel\n",
    "\n",
    "# Load base model and tokenizer\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\"unsloth/Qwen2.5-4B-Instruct\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"unsloth/Qwen2.5-4B-Instruct\")\n",
    "\n",
    "# Load LoRA adapter\n",
    "model = PeftModel.from_pretrained(base_model, \"{CONFIG['hub_model_id']}\")\n",
    "\n",
    "# Generate as usual...\n",
    "```\n",
    "\n",
    "## Training Results\n",
    "\n",
    "- **Final Training Loss**: {train_result.training_loss:.4f}\n",
    "- **Training Steps**: {train_result.global_step:,}\n",
    "- **Training Duration**: {str(training_end_time - training_start_time)}\n",
    "- **Speed**: ~2x faster than standard fine-tuning\n",
    "- **GPU**: RTX 3090 (24GB VRAM)\n",
    "\n",
    "## Performance Features\n",
    "\n",
    "- ‚úÖ **Real-time output monitoring** every 50 training steps\n",
    "- ‚úÖ **Memory optimized** for RTX 3090\n",
    "- ‚úÖ **GGUF export** for efficient deployment\n",
    "- ‚úÖ **Merged model** option for easier inference\n",
    "\n",
    "## Disclaimer\n",
    "\n",
    "This model is for educational and research purposes only. It should not be used for actual financial trading decisions. \n",
    "Cryptocurrency trading involves significant risks and can result in substantial losses.\n",
    "\n",
    "---\n",
    "\n",
    "*Trained with [Unsloth](https://github.com/unslothai/unsloth) for 2x faster fine-tuning*\n",
    "\"\"\"\n",
    "        \n",
    "        # Save model card\n",
    "        with open(f\"{CONFIG['output_dir']}/README.md\", \"w\") as f:\n",
    "            f.write(model_card)\n",
    "        \n",
    "        print(f\"‚úÖ Unsloth model pushed to Hub: https://huggingface.co/{CONFIG['hub_model_id']}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed to push to Hub: {e}\")\n",
    "        print(\"   Model saved locally only\")\n",
    "else:\n",
    "    print(\"‚ÑπÔ∏è Hub model ID not specified, skipping push to Hub\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b7347d2",
   "metadata": {},
   "source": [
    "## 13. Training Summary and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "771524fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display comprehensive training summary\n",
    "print(\"=\" * 80)\n",
    "print(\"üéØ TRAINING SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "summary = f\"\"\"\n",
    "MODEL INFORMATION:\n",
    "  Base Model: {CONFIG['model_name']}\n",
    "  Output Directory: {CONFIG['output_dir']}\n",
    "  Hub Model ID: {CONFIG.get('hub_model_id', 'Not specified')}\n",
    "\n",
    "DATASET:\n",
    "  Source: {CONFIG['dataset_name']}\n",
    "  Total Samples: {len(formatted_dataset):,}\n",
    "  Train Samples: {len(train_dataset):,}\n",
    "  Eval Samples: {len(eval_dataset):,}\n",
    "  Test Samples: {len(test_dataset):,}\n",
    "\n",
    "TRAINING CONFIGURATION:\n",
    "  LoRA Rank: {CONFIG['lora_r']}\n",
    "  LoRA Alpha: {CONFIG['lora_alpha']}\n",
    "  Learning Rate: {CONFIG['learning_rate']}\n",
    "  Epochs: {CONFIG['num_train_epochs']}\n",
    "  Batch Size: {CONFIG['per_device_train_batch_size']} (effective: {CONFIG['per_device_train_batch_size'] * CONFIG['gradient_accumulation_steps']})\n",
    "  Max Length: {CONFIG['max_length']}\n",
    "\n",
    "TRAINING RESULTS:\n",
    "  Final Loss: {train_result.training_loss:.4f}\n",
    "  Total Steps: {train_result.global_step:,}\n",
    "  Duration: {training_end_time - training_start_time}\n",
    "  Start Time: {training_start_time.strftime('%Y-%m-%d %H:%M:%S')}\n",
    "  End Time: {training_end_time.strftime('%Y-%m-%d %H:%M:%S')}\n",
    "\n",
    "MODEL PARAMETERS:\n",
    "  Total Parameters: {model.num_parameters():,}\n",
    "  Trainable Parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\n",
    "  Trainable Percentage: {100 * sum(p.numel() for p in model.parameters() if p.requires_grad) / model.num_parameters():.2f}%\n",
    "\n",
    "HARDWARE:\n",
    "  Device: {model.device}\n",
    "  GPU: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU'}\n",
    "  Memory Used: {torch.cuda.memory_allocated()/1024**3:.2f} GB\n",
    "\n",
    "FILES CREATED:\n",
    "  ‚Ä¢ {CONFIG['output_dir']}/pytorch_model.bin\n",
    "  ‚Ä¢ {CONFIG['output_dir']}/adapter_config.json\n",
    "  ‚Ä¢ {CONFIG['output_dir']}/tokenizer_config.json\n",
    "  ‚Ä¢ {CONFIG['output_dir']}/training_metrics.json\n",
    "  ‚Ä¢ {CONFIG['output_dir']}/eval_results.json\n",
    "  ‚Ä¢ {CONFIG['output_dir']}/training_config.json\n",
    "  ‚Ä¢ {CONFIG['output_dir']}/README.md\n",
    "\"\"\"\n",
    "\n",
    "print(summary)\n",
    "\n",
    "# Save summary\n",
    "with open(f\"{CONFIG['output_dir']}/training_summary.txt\", \"w\") as f:\n",
    "    f.write(summary)\n",
    "\n",
    "print(f\"\\n‚úÖ Training summary saved to: {CONFIG['output_dir']}/training_summary.txt\")\n",
    "print(\"\\nüéâ Training completed successfully!\")\n",
    "\n",
    "if os.getenv(\"WANDB_API_KEY\"):\n",
    "    wandb.finish()\n",
    "    print(\"üìä Weights & Biases session closed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf11d020",
   "metadata": {},
   "source": [
    "## 14. Quick Model Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b8ec62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick test of the Unsloth trained model\n",
    "def quick_test():\n",
    "    print(\"üß™ Quick Unsloth Model Test\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Enable fast inference mode\n",
    "    FastLanguageModel.for_inference(model)\n",
    "    \n",
    "    # Sample input for testing\n",
    "    test_instruction = \"\"\"CONTEXT DATE: 2024-01-15\n",
    "\n",
    "ANALYSIS FRAMEWORK:\n",
    "‚Ä¢ Technical Analysis: Use price trends, volatility, and momentum indicators\n",
    "‚Ä¢ Macro Analysis: Consider gold/oil prices for broader market context\n",
    "‚Ä¢ News Analysis: Integrate comprehensive daily news summaries for market catalysts\n",
    "\n",
    "OUTPUT FORMAT (JSON ONLY):\n",
    "Return a single JSON object with EXACTLY these keys:\n",
    "{\"action\":\"BUY|SELL|HOLD\",\"confidence\":<int 1-99>,\"stop_loss\":<price 2dp>,\"take_profit\":<price 2dp>,\"forecast_10d\":[<10 prices 2dp>]}\n",
    "No extra text, no explanations, just the JSON.\"\"\"\n",
    "    \n",
    "    test_input = \"\"\"Daily Context ‚Äî 2024-01-15\n",
    "\n",
    "[Technical Price Analysis]\n",
    "- Current Price: $42,350.50\n",
    "- 60-Day Range: $38,500.00 ‚Üí $45,200.00\n",
    "- 1D Return: 2.3%\n",
    "- 7D Return: 5.8%\n",
    "- 30D Return: 12.4%\n",
    "- Volatility (14d): 3.2%\n",
    "- Avg Daily Change (14d): 650.30\n",
    "- Drawdown from Max: -6.3%\n",
    "\n",
    "[Price History (Last 60 Days USD)]\n",
    "[41200.50, 41850.75, 42100.25, 42350.50, 42600.80, 43150.25, 42950.75, 42350.50, 42700.25, 43050.50]\n",
    "\n",
    "[Macro & Commodities Context]\n",
    "- Gold Price: $2,025.50\n",
    "- Crude Oil Price: $72.35\n",
    "\n",
    "[Market Context]\n",
    "- Bitcoin dominates crypto market as leading digital asset\n",
    "- Price influenced by adoption, regulation, and macro factors\n",
    "\n",
    "Based on this comprehensive multi-dimensional analysis incorporating technical indicators, fundamentals, sentiment, and detailed news analysis, provide your trading decision and 10-day price forecast in the specified JSON format.\"\"\"\n",
    "    \n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": test_instruction},\n",
    "        {\"role\": \"user\", \"content\": test_input}\n",
    "    ]\n",
    "    \n",
    "    # Use Unsloth optimized generation\n",
    "    inputs = tokenizer.apply_chat_template(\n",
    "        messages, \n",
    "        tokenize=True, \n",
    "        add_generation_prompt=True,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(model.device)\n",
    "    \n",
    "    print(\"Input prepared, generating response with Unsloth...\")\n",
    "    \n",
    "    # Unsloth optimized generation\n",
    "    start_time = datetime.now()\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            input_ids=inputs,\n",
    "            max_new_tokens=300,\n",
    "            do_sample=True,\n",
    "            temperature=0.1,\n",
    "            top_p=0.9,\n",
    "            use_cache=True,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    generation_time = datetime.now() - start_time\n",
    "    response = tokenizer.decode(outputs[0][inputs.shape[-1]:], skip_special_tokens=True)\n",
    "    \n",
    "    print(f\"\\n‚ö° Generation completed in: {generation_time.total_seconds():.2f} seconds\")\n",
    "    print(\"\\nüì§ Unsloth Model Response:\")\n",
    "    print(response.strip())\n",
    "    \n",
    "    # Try to parse JSON\n",
    "    try:\n",
    "        import re\n",
    "        json_match = re.search(r'\\{.*\\}', response.strip(), re.DOTALL)\n",
    "        if json_match:\n",
    "            json_str = json_match.group()\n",
    "            parsed = json.loads(json_str)\n",
    "            print(\"\\n‚úÖ JSON parsing successful:\")\n",
    "            for key, value in parsed.items():\n",
    "                if key == \"forecast_10d\":\n",
    "                    print(f\"  {key}: {value[:3]}... (10 prices total)\")\n",
    "                else:\n",
    "                    print(f\"  {key}: {value}\")\n",
    "        else:\n",
    "            print(\"\\n‚ö†Ô∏è No JSON found in response\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå JSON parsing failed: {e}\")\n",
    "    \n",
    "    # Switch back to training mode if needed\n",
    "    model.train()\n",
    "\n",
    "print(\"üöÄ Testing Unsloth trained model...\")\n",
    "quick_test()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üéâ UNSLOTH TRAINING COMPLETE!\")\n",
    "print(\"=\"*80)\n",
    "print(f\"‚úÖ Model saved to: {CONFIG['output_dir']}\")\n",
    "print(f\"‚úÖ Training optimized with Unsloth (2x faster)\")\n",
    "print(f\"‚úÖ RTX 3090 memory optimized\")\n",
    "print(f\"‚úÖ Real-time outputs every {CONFIG['show_sample_every_n_steps']} steps\")\n",
    "print(f\"‚úÖ Multiple export formats available\")\n",
    "if CONFIG.get('hub_model_id'):\n",
    "    print(f\"‚úÖ Model pushed to: https://huggingface.co/{CONFIG['hub_model_id']}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25db2849",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4226500",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
