{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8ef331e",
   "metadata": {},
   "source": [
    "# GRPO Training for Bitcoin Enhanced Prediction (Final Output)\n",
    "\n",
    "This notebook implements Group Relative Policy Optimization (GRPO) for comprehensive Bitcoin prediction with enhanced datasets.\n",
    "\n",
    "**Dataset**: `bitcoin-enhanced-prediction-dataset-with-local-comprehensive-news`\n",
    "\n",
    "**Training Method**: Group Relative Policy Optimization (GRPO)\n",
    "- Preference learning through relative comparisons\n",
    "- Custom reward system for Bitcoin prediction quality\n",
    "- Multi-response generation and ranking\n",
    "- Enhanced final output model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b78f5b",
   "metadata": {},
   "source": [
    "## Install Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b5ccea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U unsloth\n",
    "# !pip install trl\n",
    "# !pip install accelerate\n",
    "# !pip install datasets\n",
    "# !pip install transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3758d3f7",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7463147d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel, is_bfloat16_supported\n",
    "from unsloth.chat_templates import get_chat_template\n",
    "from trl import GRPOTrainer, GRPOConfig\n",
    "from datasets import load_dataset, Dataset\n",
    "import torch, random, os\n",
    "from typing import Dict, List, Any\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from transformers import TrainingArguments\n",
    "from tqdm import tqdm\n",
    "\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27fd2ce9",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb52543",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model and training configuration\n",
    "MODEL_CONFIG = {\n",
    "    \"model_path\": \"./Qwen3-8B\",  # Base model path\n",
    "    # \"model_path\": \"qwen_bitcoin_final_sft_enhanced/lora_adapter\",  # Use this if loading from SFT checkpoint\n",
    "    \"max_seq_length\": 4096,\n",
    "    \"dtype\": torch.float16,\n",
    "    \"load_in_4bit\": True,\n",
    "    \"lora_r\": 32,\n",
    "    \"lora_alpha\": 32,\n",
    "    \"lora_dropout\": 0.0,\n",
    "    \"target_modules\": [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"]\n",
    "}\n",
    "\n",
    "GRPO_CONFIG = {\n",
    "    \"output_dir\": \"qwen_bitcoin_final_grpo_only_enhanced\",\n",
    "    \"num_train_epochs\": 3,\n",
    "    \"per_device_train_batch_size\": 1,\n",
    "    \"gradient_accumulation_steps\": 8,\n",
    "    \"learning_rate\": 5e-7,\n",
    "    \"logging_steps\": 10,\n",
    "    \"save_steps\": 50,\n",
    "    \"warmup_ratio\": 0.1,\n",
    "    \"max_length\": 1024,\n",
    "    \"max_prompt_length\": 512,\n",
    "    \"beta\": 0.1,\n",
    "    \"remove_unused_columns\": False,\n",
    "    \"fp16\": not is_bfloat16_supported(),\n",
    "    \"bf16\": is_bfloat16_supported(),\n",
    "}\n",
    "\n",
    "DATASET_NAME = \"tahamajs/bitcoin-enhanced-prediction-dataset-with-local-comprehensive-news\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad05c0e8",
   "metadata": {},
   "source": [
    "## Load Model and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d5568c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    MODEL_CONFIG[\"model_path\"],\n",
    "    max_seq_length=MODEL_CONFIG[\"max_seq_length\"],\n",
    "    dtype=MODEL_CONFIG[\"dtype\"],\n",
    "    load_in_4bit=MODEL_CONFIG[\"load_in_4bit\"],\n",
    ")\n",
    "\n",
    "# Apply chat template\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template=\"chatml\",  # Supports Qwen models\n",
    ")\n",
    "\n",
    "# Prepare model for training\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=MODEL_CONFIG[\"lora_r\"],\n",
    "    target_modules=MODEL_CONFIG[\"target_modules\"],\n",
    "    lora_alpha=MODEL_CONFIG[\"lora_alpha\"],\n",
    "    lora_dropout=MODEL_CONFIG[\"lora_dropout\"],\n",
    "    bias=\"none\",\n",
    "    use_gradient_checkpointing=\"unsloth\",\n",
    "    random_state=SEED,\n",
    "    use_rslora=False,\n",
    "    loftq_config=None,\n",
    ")\n",
    "\n",
    "print(f\"Model loaded: {MODEL_CONFIG['model_path']}\")\n",
    "print(f\"Total parameters: {model.num_parameters():,}\")\n",
    "print(f\"Trainable parameters: {model.num_parameters(only_trainable=True):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a3829cb",
   "metadata": {},
   "source": [
    "## Special Tokens Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e65473",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define special tokens for enhanced prediction\n",
    "SPECIAL_TOKENS = [\"<|response|>\", \"<|analysis|>\", \"<|forecast|>\", \"<|confidence|>\", \"<|reasoning|>\", \"<|thinking|>\"]\n",
    "num_added = tokenizer.add_special_tokens({\"additional_special_tokens\": SPECIAL_TOKENS})\n",
    "if num_added > 0:\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "    print(f\"Added {num_added} special tokens\")\n",
    "\n",
    "RESPONSE_TAG = \"<|response|>\"\n",
    "ANALYSIS_TAG = \"<|analysis|>\"\n",
    "FORECAST_TAG = \"<|forecast|>\"\n",
    "CONFIDENCE_TAG = \"<|confidence|>\"\n",
    "REASONING_TAG = \"<|reasoning|>\"\n",
    "THINKING_TAG = \"<|thinking|>\"\n",
    "\n",
    "response_token_id = tokenizer.convert_tokens_to_ids(RESPONSE_TAG)\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.truncation_side = \"left\"\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "print(f\"Special tokens: {SPECIAL_TOKENS}\")\n",
    "print(f\"Response token ID: {response_token_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16af12bd",
   "metadata": {},
   "source": [
    "## Load and Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d57615",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "raw_dataset = load_dataset(DATASET_NAME)\n",
    "print(f\"Loaded dataset: {DATASET_NAME}\")\n",
    "print(f\"Dataset structure: {raw_dataset}\")\n",
    "\n",
    "train_data = raw_dataset[\"train\"]\n",
    "print(f\"Total training samples: {len(train_data)}\")\n",
    "\n",
    "# Show sample data\n",
    "print(\"\\n=== Sample Data ===\")\n",
    "sample = train_data[0]\n",
    "for key, value in sample.items():\n",
    "    print(f\"{key}: {str(value)[:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd10ad04",
   "metadata": {},
   "source": [
    "## Data Processing for GRPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e91086",
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatting_prompts_func(examples):\n",
    "    \"\"\"Format examples for Unsloth GRPO training\"\"\"\n",
    "    instructions = examples.get(\"instruction\", [\"\"] * len(examples.get(\"input\", [])))\n",
    "    inputs = examples.get(\"input\", [])\n",
    "    outputs = examples.get(\"output\", [])\n",
    "    \n",
    "    conversations = []\n",
    "    for instruction, user_input, output in zip(instructions, inputs, outputs):\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": instruction or \"\"},\n",
    "            {\"role\": \"user\", \"content\": user_input or \"\"},\n",
    "            {\"role\": \"assistant\", \"content\": output or \"\"},\n",
    "        ]\n",
    "        conversations.append(messages)\n",
    "    \n",
    "    return {\"conversations\": conversations}\n",
    "\n",
    "# Format data for Unsloth GRPO\n",
    "formatted_dataset = train_data.map(\n",
    "    formatting_prompts_func,\n",
    "    batched=True,\n",
    "    remove_columns=train_data.column_names\n",
    ")\n",
    "\n",
    "print(f\"Formatted dataset for Unsloth GRPO: {len(formatted_dataset)} samples\")\n",
    "\n",
    "# Show sample formatted data\n",
    "print(\"\\n=== Sample Formatted Data ===\")\n",
    "sample_conversation = formatted_dataset[0][\"conversations\"]\n",
    "for message in sample_conversation:\n",
    "    print(f\"{message['role']}: {message['content'][:100]}...\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1dc9fd8",
   "metadata": {},
   "source": [
    "## Unsloth GRPO Training Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f311e17d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Unsloth GRPO Trainer\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=GRPO_CONFIG[\"output_dir\"],\n",
    "    num_train_epochs=GRPO_CONFIG[\"num_train_epochs\"],\n",
    "    per_device_train_batch_size=GRPO_CONFIG[\"per_device_train_batch_size\"],\n",
    "    gradient_accumulation_steps=GRPO_CONFIG[\"gradient_accumulation_steps\"],\n",
    "    learning_rate=GRPO_CONFIG[\"learning_rate\"],\n",
    "    logging_steps=GRPO_CONFIG[\"logging_steps\"],\n",
    "    save_steps=GRPO_CONFIG[\"save_steps\"],\n",
    "    warmup_ratio=GRPO_CONFIG[\"warmup_ratio\"],\n",
    "    fp16=GRPO_CONFIG[\"fp16\"],\n",
    "    bf16=GRPO_CONFIG[\"bf16\"],\n",
    "    optim=\"adamw_8bit\",\n",
    "    weight_decay=0.01,\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    seed=SEED,\n",
    "    remove_unused_columns=GRPO_CONFIG[\"remove_unused_columns\"],\n",
    "    dataloader_num_workers=2,\n",
    ")\n",
    "\n",
    "# Initialize GRPO trainer using Unsloth\n",
    "grpo_trainer = GRPOTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=formatted_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=GRPO_CONFIG[\"max_length\"],\n",
    "    max_prompt_length=GRPO_CONFIG[\"max_prompt_length\"],\n",
    "    beta=GRPO_CONFIG[\"beta\"],\n",
    "    formatting_func=formatting_prompts_func,\n",
    ")\n",
    "\n",
    "print(\"✅ Unsloth GRPO Trainer initialized\")\n",
    "print(f\"Model: {model.__class__.__name__}\")\n",
    "print(f\"Training dataset: {len(formatted_dataset)} samples\")\n",
    "print(f\"Max length: {GRPO_CONFIG['max_length']}\")\n",
    "print(f\"Max prompt length: {GRPO_CONFIG['max_prompt_length']}\")\n",
    "print(f\"Beta parameter: {GRPO_CONFIG['beta']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5faf6d27",
   "metadata": {},
   "source": [
    "## Unsloth GRPO Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b40196bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize enhanced GRPO trainer\n",
    "grpo_trainer = EnhancedGRPOTrainer(model, tokenizer, GRPO_CONFIG)\n",
    "\n",
    "# Setup optimizer for GRPO\n",
    "optimizer = AdamW(model.parameters(), lr=GRPO_CONFIG[\"learning_rate\"])\n",
    "\n",
    "print(\"🚀 Starting Enhanced GRPO training...\")\n",
    "print(f\"Training on {len(grpo_batches)} batches for {GRPO_CONFIG['num_train_epochs']} epochs\")\n",
    "\n",
    "# Training loop\n",
    "training_logs = []\n",
    "global_step = 0\n",
    "\n",
    "for epoch in range(GRPO_CONFIG[\"num_train_epochs\"]):\n",
    "    epoch_loss = 0.0\n",
    "    epoch_batches = 0\n",
    "    \n",
    "    progress_bar = tqdm(grpo_batches, desc=f\"Enhanced GRPO Epoch {epoch+1}/{GRPO_CONFIG['num_train_epochs']}\")\n",
    "    \n",
    "    for batch_queries, batch_ground_truths in progress_bar:\n",
    "        try:\n",
    "            # Perform enhanced training step\n",
    "            loss, responses, rewards = grpo_trainer.train_step(batch_queries, batch_ground_truths)\n",
    "            \n",
    "            if loss.requires_grad:\n",
    "                # Backward pass\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                \n",
    "                # Gradient clipping\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "                \n",
    "                optimizer.step()\n",
    "                \n",
    "                epoch_loss += loss.item()\n",
    "                epoch_batches += 1\n",
    "                global_step += 1\n",
    "                \n",
    "                # Update progress bar\n",
    "                progress_bar.set_postfix({\n",
    "                    'loss': f'{loss.item():.4f}',\n",
    "                    'avg_loss': f'{epoch_loss/epoch_batches:.4f}'\n",
    "                })\n",
    "                \n",
    "                # Enhanced logging\n",
    "                if global_step % GRPO_CONFIG[\"logging_steps\"] == 0:\n",
    "                    # Calculate reward statistics\n",
    "                    all_rewards = [r for batch_rewards in rewards for r in batch_rewards]\n",
    "                    avg_reward = sum(all_rewards) / len(all_rewards) if all_rewards else 0\n",
    "                    max_reward = max(all_rewards) if all_rewards else 0\n",
    "                    min_reward = min(all_rewards) if all_rewards else 0\n",
    "                    \n",
    "                    log_entry = {\n",
    "                        \"epoch\": epoch,\n",
    "                        \"step\": global_step,\n",
    "                        \"loss\": loss.item(),\n",
    "                        \"avg_epoch_loss\": epoch_loss / epoch_batches,\n",
    "                        \"avg_reward\": avg_reward,\n",
    "                        \"max_reward\": max_reward,\n",
    "                        \"min_reward\": min_reward,\n",
    "                        \"timestamp\": datetime.now().isoformat()\n",
    "                    }\n",
    "                    training_logs.append(log_entry)\n",
    "                    \n",
    "                    print(f\"\\nStep {global_step}: Loss = {loss.item():.4f}\")\n",
    "                    print(f\"Reward stats - Avg: {avg_reward:.3f}, Max: {max_reward:.3f}, Min: {min_reward:.3f}\")\n",
    "                    print(f\"Sample responses for batch:\")\n",
    "                    for i, (query_responses, query_rewards) in enumerate(zip(responses[:1], rewards[:1])):\n",
    "                        print(f\"  Query {i+1} responses:\")\n",
    "                        for j, (resp, rew) in enumerate(zip(query_responses[:2], query_rewards[:2])):\n",
    "                            print(f\"    Response {j+1} (reward: {rew:.3f}): {resp[:120]}...\")\n",
    "                \n",
    "                # Save checkpoint\n",
    "                if global_step % GRPO_CONFIG[\"save_steps\"] == 0:\n",
    "                    checkpoint_dir = f\"{GRPO_CONFIG['output_dir']}/checkpoint-{global_step}\"\n",
    "                    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "                    model.save_pretrained(f\"{checkpoint_dir}/lora_adapter\")\n",
    "                    tokenizer.save_pretrained(checkpoint_dir)\n",
    "                    print(f\"\\n💾 Enhanced checkpoint saved at step {global_step}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"\\n⚠️ Error in batch: {e}\")\n",
    "            continue\n",
    "    \n",
    "    avg_epoch_loss = epoch_loss / max(epoch_batches, 1)\n",
    "    print(f\"\\n✅ Enhanced Epoch {epoch+1} completed. Average loss: {avg_epoch_loss:.4f}\")\n",
    "\n",
    "print(\"\\n🎉 Enhanced GRPO training completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94bb13c4",
   "metadata": {},
   "source": [
    "## Save Final GRPO Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c9e2497",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save final enhanced GRPO model\n",
    "final_model_dir = f\"{GRPO_CONFIG['output_dir']}/final_model\"\n",
    "os.makedirs(final_model_dir, exist_ok=True)\n",
    "\n",
    "model.save_pretrained(f\"{final_model_dir}/lora_adapter\")\n",
    "tokenizer.save_pretrained(final_model_dir)\n",
    "\n",
    "print(f\"✅ Final Enhanced GRPO model saved to {final_model_dir}\")\n",
    "\n",
    "# Save enhanced GRPO training logs\n",
    "with open(f\"{GRPO_CONFIG['output_dir']}/enhanced_grpo_training_logs.json\", \"w\") as f:\n",
    "    json.dump(training_logs, f, indent=2)\n",
    "\n",
    "print(f\"Enhanced GRPO training logs saved to {GRPO_CONFIG['output_dir']}/enhanced_grpo_training_logs.json\")\n",
    "\n",
    "# Create comprehensive training summary\n",
    "training_summary = {\n",
    "    \"model_type\": \"Enhanced GRPO for Bitcoin Prediction\",\n",
    "    \"dataset\": DATASET_NAME,\n",
    "    \"model_config\": MODEL_CONFIG,\n",
    "    \"grpo_config\": GRPO_CONFIG,\n",
    "    \"total_samples\": len(train_data),\n",
    "    \"total_grpo_steps\": global_step,\n",
    "    \"training_completed\": datetime.now().isoformat(),\n",
    "    \"final_model_path\": final_model_dir,\n",
    "    \"enhancement_features\": [\n",
    "        \"Multi-dimensional reward system\",\n",
    "        \"Technical analysis integration\",\n",
    "        \"News sentiment analysis\",\n",
    "        \"Confidence scoring\",\n",
    "        \"Structured prediction format\",\n",
    "        \"Enhanced preference learning\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "with open(f\"{GRPO_CONFIG['output_dir']}/enhanced_training_summary.json\", \"w\") as f:\n",
    "    json.dump(training_summary, f, indent=2)\n",
    "\n",
    "print(f\"Enhanced training summary saved to {GRPO_CONFIG['output_dir']}/enhanced_training_summary.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad321334",
   "metadata": {},
   "source": [
    "## Comprehensive Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d27557",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the enhanced final model\n",
    "print(\"🧪 Testing the Enhanced GRPO model...\")\n",
    "\n",
    "def test_enhanced_model_generation(model, tokenizer, test_query, max_length=600):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        inputs = tokenizer(test_query, return_tensors=\"pt\", truncation=True, max_length=2048)\n",
    "        inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "        \n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_length=inputs[\"input_ids\"].shape[1] + max_length,\n",
    "            temperature=0.7,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "        \n",
    "        response = tokenizer.decode(\n",
    "            outputs[0][inputs[\"input_ids\"].shape[1]:], \n",
    "            skip_special_tokens=True\n",
    "        )\n",
    "        return response.strip()\n",
    "\n",
    "# Test multiple samples\n",
    "test_samples = grpo_formatted[:3]\n",
    "\n",
    "for i, test_sample in enumerate(test_samples):\n",
    "    print(f\"\\n=== Test Sample {i+1} ===\")\n",
    "    test_query = test_sample[\"query\"]\n",
    "    ground_truth = test_sample[\"ground_truth\"]\n",
    "    \n",
    "    print(\"Query:\")\n",
    "    print(test_query[:300] + \"...\")\n",
    "    \n",
    "    print(\"\\nGround Truth:\")\n",
    "    print(ground_truth[:200] + \"...\")\n",
    "    \n",
    "    print(\"\\nEnhanced Model Response:\")\n",
    "    response = test_enhanced_model_generation(model, tokenizer, test_query)\n",
    "    print(response)\n",
    "    \n",
    "    # Calculate reward for the response\n",
    "    reward = grpo_trainer._calculate_enhanced_prediction_reward(response, ground_truth)\n",
    "    print(f\"\\nResponse Quality Score: {reward:.3f}\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "print(\"\\n✅ Enhanced model evaluation completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c2efba6",
   "metadata": {},
   "source": [
    "## Final Training Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf48a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"🏆 Enhanced GRPO Training Summary\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Dataset: {DATASET_NAME}\")\n",
    "print(f\"Total samples: {len(train_data):,}\")\n",
    "print(f\"Training method: Enhanced Group Relative Policy Optimization (GRPO)\")\n",
    "print(\"\\n🎯 Training Configuration:\")\n",
    "print(f\"GRPO epochs: {GRPO_CONFIG['num_train_epochs']}\")\n",
    "print(f\"Total GRPO steps: {global_step}\")\n",
    "print(f\"Batch size: {GRPO_CONFIG['per_device_train_batch_size']}\")\n",
    "print(f\"Learning rate: {GRPO_CONFIG['learning_rate']}\")\n",
    "print(f\"Group size: {GRPO_CONFIG['group_size']}\")\n",
    "print(\"\\n💾 Model Output:\")\n",
    "print(f\"Final Enhanced GRPO model: {final_model_dir}\")\n",
    "print(\"\\n🔬 Advanced Features:\")\n",
    "print(\"✅ Enhanced Group Relative Policy Optimization\")\n",
    "print(\"✅ Multi-dimensional reward system for Bitcoin predictions\")\n",
    "print(\"✅ Technical analysis integration (RSI, MACD, Moving Averages)\")\n",
    "print(\"✅ News sentiment and impact analysis\")\n",
    "print(\"✅ Confidence and reasoning scoring\")\n",
    "print(\"✅ Structured prediction format with special tokens\")\n",
    "print(\"✅ Enhanced preference learning with margin scaling\")\n",
    "print(\"✅ Comprehensive quality assessment\")\n",
    "print(\"\\n🚀 Model Capabilities:\")\n",
    "print(\"• Advanced Bitcoin price prediction\")\n",
    "print(\"• Technical and fundamental analysis integration\")\n",
    "print(\"• News-driven market sentiment analysis\")\n",
    "print(\"• Confidence-aware forecasting\")\n",
    "print(\"• Structured reasoning and explanation\")\n",
    "print(\"\\n🎉 Enhanced GRPO training pipeline completed successfully!\")\n",
    "print(\"\\n📈 Ready for production Bitcoin prediction tasks!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
