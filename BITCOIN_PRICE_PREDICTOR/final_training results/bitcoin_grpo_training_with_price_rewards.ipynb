{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e564eec1",
   "metadata": {},
   "source": [
    "# Bitcoin Price Predictor: GRPO Training with Price Difference Rewards\n",
    "\n",
    "This notebook implements Group Relative Policy Optimization (GRPO) training for a Bitcoin price prediction model that has already been fine-tuned with SFT. GRPO is a reinforcement learning technique similar to PPO that further improves model performance by optimizing for specific rewards - in this case, minimizing price prediction errors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acec479c",
   "metadata": {},
   "source": [
    "## 1. Install Required Libraries for GRPO Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ca9bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required libraries for GRPO training\n",
    "!pip install trl==0.7.6 transformers>=4.38.0 datasets accelerate bitsandbytes\n",
    "!pip install wandb torch>=2.0.0 peft>=0.8.0 scipy evaluate\n",
    "!pip install deepspeed\n",
    "\n",
    "# Additional libraries for data processing and visualization\n",
    "!pip install pandas matplotlib seaborn numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a267c7a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Verify installation and check for compatibility issues\n",
    "# import sys\n",
    "# print(f\"Python version: {sys.version}\")\n",
    "\n",
    "# try:\n",
    "#     import torch\n",
    "#     print(f\"PyTorch version: {torch.__version__}\")\n",
    "#     print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "#     if torch.cuda.is_available():\n",
    "#         print(f\"CUDA version: {torch.version.cuda}\")\n",
    "#         print(f\"GPU count: {torch.cuda.device_count()}\")\n",
    "# except ImportError as e:\n",
    "#     print(f\"PyTorch import error: {e}\")\n",
    "\n",
    "# try:\n",
    "#     import torchvision\n",
    "#     print(f\"Torchvision version: {torchvision.__version__}\")\n",
    "# except ImportError as e:\n",
    "#     print(f\"Torchvision import error: {e}\")\n",
    "\n",
    "# try:\n",
    "#     import transformers\n",
    "#     print(f\"Transformers version: {transformers.__version__}\")\n",
    "# except ImportError as e:\n",
    "#     print(f\"Transformers import error: {e}\")\n",
    "\n",
    "# try:\n",
    "#     import peft\n",
    "#     print(f\"PEFT version: {peft.__version__}\")\n",
    "# except ImportError as e:\n",
    "#     print(f\"PEFT import error: {e}\")\n",
    "\n",
    "# print(\"\\n‚úÖ If all versions are displayed above without errors, you can proceed to the next cell.\")\n",
    "# print(\"‚ùå If you see import errors, please restart your kernel and re-run the installation cell.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "266966f9",
   "metadata": {},
   "source": [
    "## 2. Load SFT-Trained Model and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7fe13f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "import json\n",
    "import re\n",
    "from scipy.stats import wilcoxon\n",
    "\n",
    "# Load the enhanced model (assuming it's the same base as your other model)\n",
    "base_model_id = './Qwen3-8B'\n",
    "adapter_path = './my-awesome-model_final_bitcoin-enhanced-prediction-dataset-with-local-comprehensive-news-v2/checkpoint-400'  # Adjust based on your checkpoint\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "import json\n",
    "import re\n",
    "from scipy.stats import wilcoxon\n",
    "from collections import defaultdict\n",
    "import warnings\n",
    "\n",
    "# Suppress warnings that might be caused by version mismatches\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "\n",
    "# Set torch backend to avoid potential conflicts\n",
    "import os\n",
    "os.environ['TORCH_USE_CUDA_DSA'] = '1'\n",
    "\n",
    "print(\"üîß Environment configured with compatibility settings...\")\n",
    "\n",
    "# Load the base model for individual news training\n",
    "base_model_id = './Qwen3_8B'\n",
    "\n",
    "print(\"üì¶ Loading base model and tokenizer...\")\n",
    "try:\n",
    "    # Load the base model and tokenizer with error handling\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        base_model_id,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True,\n",
    "        low_cpu_mem_usage=True  # Help with memory issues\n",
    "    )\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        base_model_id,\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    print(\"‚úÖ Individual News Model loaded successfully!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error loading model: {e}\")\n",
    "    print(\"üí° Try the following solutions:\")\n",
    "    print(\"1. Restart your kernel and re-run the installation cell\")\n",
    "    print(\"2. Check if the model path './Qwen3_8B' exists\")\n",
    "    print(\"3. Use the absolute path to your model\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "756dfca0",
   "metadata": {},
   "source": [
    "## 3. Setup Reward Function for Price Differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f6f68b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_prices_from_text(text):\n",
    "    \"\"\"Extract price predictions from model output\"\"\"\n",
    "    # Look for patterns like numbers separated by commas\n",
    "    price_pattern = r'(\\d+(?:\\.\\d+)?(?:,\\s*\\d+(?:\\.\\d+)?)*)'  \n",
    "    matches = re.findall(price_pattern, text)\n",
    "    \n",
    "    if matches:\n",
    "        # Take the first match and split by comma\n",
    "        prices_str = matches[0]\n",
    "        try:\n",
    "            prices = [float(p.strip()) for p in prices_str.split(',')]\n",
    "            return prices\n",
    "        except:\n",
    "            return []\n",
    "    return []\n",
    "\n",
    "def calculate_price_difference_reward(predicted_prices, actual_prices, max_len=10):\n",
    "    \"\"\"Calculate reward based on price difference accuracy\n",
    "    \n",
    "    Lower price differences = higher rewards\n",
    "    Correct price direction predictions = bonus rewards\n",
    "    \"\"\"\n",
    "    # Ensure we have valid predictions\n",
    "    if not predicted_prices or not actual_prices:\n",
    "        return -10.0  # Penalty for invalid predictions\n",
    "    \n",
    "    # Truncate to minimum length and max_len for fair comparison\n",
    "    min_len = min(len(predicted_prices), len(actual_prices), max_len)\n",
    "    if min_len <= 1:  # Need at least 2 prices to calculate direction\n",
    "        return -5.0  # Smaller penalty for partial predictions\n",
    "        \n",
    "    pred_truncated = np.array(predicted_prices[:min_len])\n",
    "    actual_truncated = np.array(actual_prices[:min_len])\n",
    "    \n",
    "    # Calculate absolute differences\n",
    "    abs_diffs = np.abs(pred_truncated - actual_truncated)\n",
    "    mean_abs_diff = np.mean(abs_diffs)\n",
    "    \n",
    "    # Calculate direction accuracy (up/down/same)\n",
    "    actual_direction = np.diff(actual_truncated)\n",
    "    pred_direction = np.diff(pred_truncated)\n",
    "    direction_correct = np.sign(actual_direction) == np.sign(pred_direction)\n",
    "    direction_accuracy = np.mean(direction_correct) if len(direction_correct) > 0 else 0\n",
    "    \n",
    "    # Convert price differences to rewards (lower difference = higher reward)\n",
    "    # Normalize by typical Bitcoin price volatility (e.g., $1000)\n",
    "    price_diff_reward = 10.0 * np.exp(-mean_abs_diff / 1000)\n",
    "    \n",
    "    # Add bonus for direction accuracy\n",
    "    direction_bonus = 5.0 * direction_accuracy\n",
    "    \n",
    "    # Combine rewards (price difference + direction bonus)\n",
    "    total_reward = price_diff_reward + direction_bonus\n",
    "    \n",
    "    return float(total_reward)\n",
    "\n",
    "def compute_rewards(predictions, references):\n",
    "    \"\"\"Compute rewards for a batch of predictions and references\"\"\"\n",
    "    rewards = []\n",
    "    \n",
    "    for pred, ref in zip(predictions, references):\n",
    "        # Extract predicted prices from model output\n",
    "        predicted_prices = extract_prices_from_text(pred)\n",
    "        \n",
    "        # Extract actual prices from reference output\n",
    "        actual_prices = extract_prices_from_text(ref)\n",
    "        \n",
    "        # Calculate reward based on price differences\n",
    "        reward = calculate_price_difference_reward(predicted_prices, actual_prices)\n",
    "        rewards.append(reward)\n",
    "    \n",
    "    return rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b58be8a3",
   "metadata": {},
   "source": [
    "## 4. Create GRPO Dataset from Bitcoin Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce3911a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "print(\"Loading Bitcoin prediction dataset...\")\n",
    "# Load the Bitcoin prediction dataset\n",
    "train_dataset = load_dataset('tahamajs/bitcoin-enhanced-prediction-dataset-with-local-comprehensive-news', split='train')\n",
    "test_dataset = load_dataset('tahamajs/bitcoin-enhanced-prediction-dataset-with-local-comprehensive-news', split='train')\n",
    "\n",
    "print(f\"Loaded {len(train_dataset)} training samples and {len(test_dataset)} test samples\")\n",
    "\n",
    "def format_prompt(example):\n",
    "    \"\"\"Format prompt for the model\"\"\"\n",
    "    instruction = example.get('instruction', '')\n",
    "    user_input = example.get('input', '')\n",
    "    \n",
    "    return f\"\"\"<|im_start|>system\n",
    "{instruction}<|im_end|>\n",
    "<|im_start|>user\n",
    "{user_input}<|im_end|>\n",
    "<|im_start|>assistant\n",
    "\"\"\"\n",
    "\n",
    "# Prepare a subset for GRPO training (GRPO training can be computationally expensive)\n",
    "num_samples = min(500, len(train_dataset))  # Adjust based on your computational resources\n",
    "grpo_train_dataset = train_dataset.select(range(num_samples))\n",
    "\n",
    "# Format prompts for GRPO training\n",
    "grpo_data = []\n",
    "for example in grpo_train_dataset:\n",
    "    prompt = format_prompt(example)\n",
    "    output = example.get('output', '')\n",
    "    \n",
    "    grpo_data.append({\n",
    "        \"prompt\": prompt,\n",
    "        \"chosen\": output,  # The ground truth output\n",
    "        \"rejected\": None,  # Will be generated by the model during training\n",
    "    })\n",
    "\n",
    "# Convert to Dataset format\n",
    "grpo_dataset = Dataset.from_pandas(pd.DataFrame(grpo_data))\n",
    "\n",
    "print(f\"Created GRPO dataset with {len(grpo_dataset)} samples\")\n",
    "print(\"Sample prompt:\")\n",
    "print(grpo_dataset[0]['prompt'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f0abb96",
   "metadata": {},
   "source": [
    "## 5. Configure GRPO Training Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57920ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "output_dir = \"./grpo_bitcoin_price_predictor\"\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    num_train_epochs=3,                 # Number of training epochs\n",
    "    per_device_train_batch_size=4,      # Batch size for training\n",
    "    gradient_accumulation_steps=4,      # Number of updates steps to accumulate before backward pass\n",
    "    learning_rate=1e-5,                 # Learning rate\n",
    "    weight_decay=0.01,                  # Weight decay\n",
    "    warmup_steps=100,                   # Number of warmup steps\n",
    "    logging_steps=10,                   # Log every X updates steps\n",
    "    eval_strategy=\"steps\",        # Evaluation strategy\n",
    "    eval_steps=100,                     # Evaluate every X steps\n",
    "    save_strategy=\"steps\",              # Save strategy\n",
    "    save_steps=100,                     # Save checkpoint every X updates steps\n",
    "    save_total_limit=3,                 # Maximum number of checkpoints to keep\n",
    "    load_best_model_at_end=True,        # Load the best model when training finishes\n",
    "    fp16=True,                          # Use FP16 precision\n",
    "    report_to=\"none\"                    # Disable wandb reporting (change to \"wandb\" to enable)\n",
    ")\n",
    "\n",
    "# GRPO specific hyperparameters\n",
    "grpo_config = {\n",
    "    \"num_rollouts\": 32,             # Number of rollouts per prompt\n",
    "    \"chunk_size\": 4,                # Number of chunks to split the batch into\n",
    "    \"beta\": 0.1,                    # KL penalty coefficient\n",
    "    \"lambda_coef\": 0.95,            # GAE lambda coefficient\n",
    "    \"gamma\": 0.99,                  # Discount factor\n",
    "    \"eps_clip\": 0.2,                # PPO clip range\n",
    "    \"value_clip\": 0.2,              # Value clip range\n",
    "    \"generate_during_eval\": True,   # Generate completions during evaluation\n",
    "    \"max_new_tokens\": 256,          # Maximum number of tokens to generate\n",
    "    \"temperature\": 0.7,             # Temperature for generation\n",
    "    \"top_k\": 50,                    # Top-k sampling\n",
    "    \"top_p\": 0.95,                  # Top-p sampling\n",
    "    \"do_sample\": True,              # Use sampling for generation\n",
    "    \"rollout_batch_size\": 8         # Batch size for rollout generation\n",
    "}\n",
    "\n",
    "print(\"Training arguments and GRPO configuration ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "998853e1",
   "metadata": {},
   "source": [
    "## 6. Initialize GRPO Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e300c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import GroupPPOConfig, GroupPPOTrainer\n",
    "from trl.core import LengthSampler\n",
    "\n",
    "# 1. Define a single, comprehensive configuration object for GroupPPO\n",
    "# This combines parameters that were previously in TrainingArguments and the separate RL config.\n",
    "ppo_config = GroupPPOConfig(\n",
    "    # --- Training loop parameters ---\n",
    "    learning_rate=1e-5,\n",
    "    batch_size=16,          # Combined from per_device_train_batch_size * gradient_accumulation_steps\n",
    "    mini_batch_size=4,      # Corresponds to per_device_train_batch_size\n",
    "    gradient_accumulation_steps=4,\n",
    "    ppo_epochs=4,           # Number of optimization epochs per PPO phase\n",
    "    \n",
    "    # --- RL-specific parameters ---\n",
    "    beta=0.1,               # KL penalty coefficient\n",
    "    lambda_=0.95,           # GAE lambda coefficient (note the underscore)\n",
    "    gamma=0.99,             # Discount factor\n",
    "    cliprange=0.2,          # PPO clip range\n",
    "    cliprange_value=0.2,    # Value function clip range\n",
    "    vf_coef=0.1,            # Value function coefficient in the loss\n",
    "    \n",
    "    # --- Other trainer settings ---\n",
    "    log_with=None,          # Set to \"wandb\" or \"tensorboard\" to enable logging\n",
    "    tracker_project_name=\"bitcoin_grpo\",\n",
    "    seed=42,\n",
    "    optimize_cuda_cache=True,\n",
    "    target_kl=0.1\n",
    ")\n",
    "\n",
    "# 2. Set generation kwargs for rollouts\n",
    "# These are passed to the `trainer.generate` method inside the training loop\n",
    "generation_kwargs = {\n",
    "    \"min_length\": -1,\n",
    "    \"max_new_tokens\": 256,\n",
    "    \"temperature\": 0.7,\n",
    "    \"top_k\": 50,\n",
    "    \"top_p\": 0.95,\n",
    "    \"do_sample\": True,\n",
    "    \"pad_token_id\": tokenizer.pad_token_id, # Assumes tokenizer is already loaded\n",
    "}\n",
    "\n",
    "# 3. Initialize the GRPO trainer\n",
    "# Assumes model, tokenizer, and grpo_dataset are already loaded\n",
    "print(\"Initializing GRPO trainer...\")\n",
    "grpo_trainer = GroupPPOTrainer(\n",
    "    config=ppo_config,\n",
    "    model=model,\n",
    "    ref_model=None,  # It's good practice to have a reference model for KL divergence\n",
    "    tokenizer=tokenizer,\n",
    "    dataset=grpo_dataset,\n",
    "    data_collator=None, # The trainer will use its default collator\n",
    ")\n",
    "\n",
    "print(\"GRPO trainer initialized successfully!\")\n",
    "print(\"\\nNext, you will start the training loop (e.g., for batch in grpo_trainer.dataloader: ...)\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b11e0cc",
   "metadata": {},
   "source": [
    "## 7. Run GRPO Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa226e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a length sampler for response generation\n",
    "length_sampler = LengthSampler(min_value=32, max_value=grpo_config['max_new_tokens'])\n",
    "\n",
    "# Performance tracking variables\n",
    "epochs = 3\n",
    "reward_history = []\n",
    "loss_history = []\n",
    "kl_div_history = []\n",
    "\n",
    "print(\"Starting GRPO training...\")\n",
    "for epoch in range(epochs):\n",
    "    print(f\"\\n===== Epoch {epoch+1}/{epochs} =====\")\n",
    "    \n",
    "    # Training loop for multiple batch iterations\n",
    "    for batch_idx in range(10):  # Process 10 batches per epoch - adjust as needed\n",
    "        print(f\"Processing batch {batch_idx+1}/10\")\n",
    "        \n",
    "        # Sample batch of prompts\n",
    "        batch_indices = random.sample(range(len(grpo_dataset)), grpo_config['chunk_size'])\n",
    "        batch = grpo_dataset.select(batch_indices)\n",
    "        \n",
    "        # Generate model responses\n",
    "        prompts = batch['prompt']\n",
    "        references = batch['chosen']\n",
    "        \n",
    "        # Step 1: Generate responses for reward computation\n",
    "        response_tensors = []\n",
    "        responses = []\n",
    "        \n",
    "        for prompt in prompts:\n",
    "            prompt_tensor = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "            response_length = length_sampler()\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                response_tensor = grpo_trainer.generate(\n",
    "                    prompt_tensor['input_ids'], \n",
    "                    **generation_kwargs\n",
    "                )\n",
    "                \n",
    "            response = tokenizer.decode(\n",
    "                response_tensor[0][prompt_tensor['input_ids'].shape[1]:],\n",
    "                skip_special_tokens=True\n",
    "            )\n",
    "            responses.append(response)\n",
    "            response_tensors.append(response_tensor)\n",
    "        \n",
    "        # Step 2: Compute rewards based on price differences\n",
    "        rewards = compute_rewards(responses, references)\n",
    "        mean_reward = sum(rewards) / len(rewards) if rewards else 0\n",
    "        reward_history.append(mean_reward)\n",
    "        \n",
    "        print(f\"Mean reward: {mean_reward:.4f}\")\n",
    "        \n",
    "        # Step 3: Perform GRPO update\n",
    "        stats = grpo_trainer.step(prompts, responses, rewards)\n",
    "        \n",
    "        # Track metrics\n",
    "        if stats:\n",
    "            loss_history.append(stats['ppo/loss/total'])\n",
    "            kl_div_history.append(stats.get('ppo/kl', 0))\n",
    "            \n",
    "            print(f\"Loss: {stats['ppo/loss/total']:.4f}, KL div: {stats.get('ppo/kl', 0):.4f}\")\n",
    "    \n",
    "    # Save checkpoint at the end of each epoch\n",
    "    checkpoint_path = f\"{output_dir}/checkpoint-epoch-{epoch+1}\"\n",
    "    grpo_trainer.save_pretrained(checkpoint_path)\n",
    "    print(f\"Saved checkpoint to {checkpoint_path}\")\n",
    "\n",
    "print(\"GRPO training completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "788cccfd",
   "metadata": {},
   "source": [
    "## 8. Evaluate GRPO-Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac6e995",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import wilcoxon\n",
    "\n",
    "# Plot training metrics\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(reward_history)\n",
    "plt.title('Mean Reward History')\n",
    "plt.xlabel('Training Step')\n",
    "plt.ylabel('Reward')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(loss_history)\n",
    "plt.title('Loss History')\n",
    "plt.xlabel('Training Step')\n",
    "plt.ylabel('Loss')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.plot(kl_div_history)\n",
    "plt.title('KL Divergence History')\n",
    "plt.xlabel('Training Step')\n",
    "plt.ylabel('KL Divergence')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('grpo_training_metrics.png', dpi=300)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nEvaluating GRPO-trained model...\")\n",
    "\n",
    "# Evaluate GRPO-trained model on test dataset\n",
    "grpo_model = grpo_trainer.model\n",
    "grpo_model.eval()\n",
    "\n",
    "def evaluate_model(model, tokenizer, dataset, num_samples=50):\n",
    "    results = []\n",
    "    total_samples = min(num_samples, len(dataset))\n",
    "    \n",
    "    print(f\"Running evaluation on {total_samples} samples...\")\n",
    "    \n",
    "    for i in range(total_samples):\n",
    "        test_example = dataset[i]\n",
    "        prompt = format_prompt(test_example)\n",
    "        \n",
    "        inputs = tokenizer(prompt, return_tensors='pt', truncation=True, max_length=2048)\n",
    "        inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=256,\n",
    "                do_sample=False,  # Use greedy decoding for consistency\n",
    "                pad_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "        \n",
    "        generated_text = tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)\n",
    "        \n",
    "        # Extract predictions and ground truth\n",
    "        predicted_prices = extract_prices_from_text(generated_text)\n",
    "        actual_output = test_example.get('output', '')\n",
    "        actual_prices = extract_prices_from_text(actual_output)\n",
    "        \n",
    "        # Calculate metrics if predictions are valid\n",
    "        if predicted_prices and actual_prices:\n",
    "            min_len = min(len(predicted_prices), len(actual_prices))\n",
    "            if min_len > 0:\n",
    "                pred_truncated = predicted_prices[:min_len]\n",
    "                actual_truncated = actual_prices[:min_len]\n",
    "                \n",
    "                # Calculate absolute differences\n",
    "                abs_diffs = np.abs(np.array(pred_truncated) - np.array(actual_truncated))\n",
    "                mean_abs_diff = np.mean(abs_diffs)\n",
    "                \n",
    "                # Calculate direction accuracy\n",
    "                actual_direction = np.diff(actual_truncated)\n",
    "                pred_direction = np.diff(pred_truncated)\n",
    "                direction_correct = np.sign(actual_direction) == np.sign(pred_direction)\n",
    "                direction_accuracy = np.mean(direction_correct) * 100 if len(direction_correct) > 0 else 0\n",
    "                \n",
    "                # Calculate reward\n",
    "                reward = calculate_price_difference_reward(pred_truncated, actual_truncated)\n",
    "                \n",
    "                results.append({\n",
    "                    'sample_id': i,\n",
    "                    'predicted': pred_truncated,\n",
    "                    'actual': actual_truncated,\n",
    "                    'mean_abs_diff': mean_abs_diff,\n",
    "                    'direction_accuracy': direction_accuracy,\n",
    "                    'reward': reward\n",
    "                })\n",
    "        \n",
    "        if (i + 1) % 10 == 0:\n",
    "            print(f\"Processed {i + 1}/{total_samples} samples...\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Evaluate GRPO model\n",
    "grpo_results = evaluate_model(grpo_model, tokenizer, test_dataset)\n",
    "\n",
    "# Calculate aggregate metrics\n",
    "if grpo_results:\n",
    "    grpo_abs_diffs = [r['mean_abs_diff'] for r in grpo_results]\n",
    "    grpo_direction_accs = [r['direction_accuracy'] for r in grpo_results]\n",
    "    grpo_rewards = [r['reward'] for r in grpo_results]\n",
    "    \n",
    "    print(\"\\nGRPO Model Evaluation Results:\")\n",
    "    print(f\"Mean Absolute Price Difference: ${np.mean(grpo_abs_diffs):.2f}\")\n",
    "    print(f\"Median Absolute Price Difference: ${np.median(grpo_abs_diffs):.2f}\")\n",
    "    print(f\"Mean Direction Accuracy: {np.mean(grpo_direction_accs):.2f}%\")\n",
    "    print(f\"Mean Reward: {np.mean(grpo_rewards):.4f}\")\n",
    "else:\n",
    "    print(\"No valid evaluation results for GRPO model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5cdd405",
   "metadata": {},
   "source": [
    "## 9. Compare SFT vs GRPO Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06496151",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload the original SFT model for comparison\n",
    "print(\"Loading SFT model for comparison...\")\n",
    "sft_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_id,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "sft_model = PeftModel.from_pretrained(sft_model, adapter_path)\n",
    "sft_model.eval()\n",
    "\n",
    "# Evaluate SFT model (pre-GRPO)\n",
    "sft_results = evaluate_model(sft_model, tokenizer, test_dataset)\n",
    "\n",
    "# Compare SFT and GRPO models\n",
    "if sft_results and grpo_results:\n",
    "    sft_abs_diffs = [r['mean_abs_diff'] for r in sft_results]\n",
    "    sft_direction_accs = [r['direction_accuracy'] for r in sft_results]\n",
    "    sft_rewards = [r['reward'] for r in sft_results]\n",
    "    \n",
    "    # Calculate improvements\n",
    "    abs_diff_improvement = ((np.mean(sft_abs_diffs) - np.mean(grpo_abs_diffs)) / np.mean(sft_abs_diffs)) * 100\n",
    "    direction_improvement = ((np.mean(grpo_direction_accs) - np.mean(sft_direction_accs)) / np.mean(sft_direction_accs)) * 100\n",
    "    reward_improvement = ((np.mean(grpo_rewards) - np.mean(sft_rewards)) / np.mean(sft_rewards)) * 100 if np.mean(sft_rewards) != 0 else 0\n",
    "    \n",
    "    # Create comparison table\n",
    "    comparison_metrics = {\n",
    "        'Metric': [\n",
    "            'Mean Absolute Price Difference ($)',\n",
    "            'Median Absolute Price Difference ($)',\n",
    "            'Mean Direction Accuracy (%)',\n",
    "            'Mean Reward'\n",
    "        ],\n",
    "        'SFT Model': [\n",
    "            f\"{np.mean(sft_abs_diffs):.2f}\",\n",
    "            f\"{np.median(sft_abs_diffs):.2f}\",\n",
    "            f\"{np.mean(sft_direction_accs):.2f}\",\n",
    "            f\"{np.mean(sft_rewards):.4f}\"\n",
    "        ],\n",
    "        'GRPO Model': [\n",
    "            f\"{np.mean(grpo_abs_diffs):.2f}\",\n",
    "            f\"{np.median(grpo_abs_diffs):.2f}\",\n",
    "            f\"{np.mean(grpo_direction_accs):.2f}\",\n",
    "            f\"{np.mean(grpo_rewards):.4f}\"\n",
    "        ],\n",
    "        'Improvement (%)': [\n",
    "            f\"{abs_diff_improvement:.2f}\",\n",
    "            f\"{((np.median(sft_abs_diffs) - np.median(grpo_abs_diffs)) / np.median(sft_abs_diffs) * 100):.2f}\",\n",
    "            f\"{direction_improvement:.2f}\",\n",
    "            f\"{reward_improvement:.2f}\"\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    comparison_df = pd.DataFrame(comparison_metrics)\n",
    "    print(\"\\n=== SFT vs GRPO Model Comparison ===\")\n",
    "    print(comparison_df.to_string(index=False))\n",
    "    \n",
    "    # Statistical significance tests\n",
    "    print(\"\\n=== Statistical Significance Tests ===\")\n",
    "    try:\n",
    "        abs_diff_stat, abs_diff_pval = wilcoxon(sft_abs_diffs, grpo_abs_diffs)\n",
    "        direction_stat, direction_pval = wilcoxon(sft_direction_accs, grpo_direction_accs)\n",
    "        reward_stat, reward_pval = wilcoxon(sft_rewards, grpo_rewards)\n",
    "        \n",
    "        alpha = 0.05\n",
    "        print(f\"Absolute Difference p-value: {abs_diff_pval:.6f} - {'Significant' if abs_diff_pval < alpha else 'Not significant'}\")\n",
    "        print(f\"Direction Accuracy p-value: {direction_pval:.6f} - {'Significant' if direction_pval < alpha else 'Not significant'}\")\n",
    "        print(f\"Reward p-value: {reward_pval:.6f} - {'Significant' if reward_pval < alpha else 'Not significant'}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Statistical test error: {e}\")\n",
    "    \n",
    "    # Visualization\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    # Price difference comparison\n",
    "    plt.subplot(2, 2, 1)\n",
    "    sns.histplot([sft_abs_diffs, grpo_abs_diffs], bins=30, alpha=0.6, \n",
    "                 label=['SFT Model', 'GRPO Model'])\n",
    "    plt.title('Absolute Price Differences Distribution')\n",
    "    plt.xlabel('Absolute Difference ($)')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Direction accuracy comparison\n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.boxplot([sft_direction_accs, grpo_direction_accs], labels=['SFT Model', 'GRPO Model'])\n",
    "    plt.title('Direction Accuracy Comparison')\n",
    "    plt.ylabel('Direction Accuracy (%)')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Reward comparison\n",
    "    plt.subplot(2, 2, 3)\n",
    "    plt.boxplot([sft_rewards, grpo_rewards], labels=['SFT Model', 'GRPO Model'])\n",
    "    plt.title('Reward Comparison')\n",
    "    plt.ylabel('Reward')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Improvement metrics\n",
    "    plt.subplot(2, 2, 4)\n",
    "    metrics = ['Abs Diff', 'Direction Acc', 'Reward']\n",
    "    improvements = [abs_diff_improvement, direction_improvement, reward_improvement]\n",
    "    \n",
    "    colors = ['green' if imp > 0 else 'red' for imp in improvements]\n",
    "    plt.bar(metrics, improvements, color=colors, alpha=0.7)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for i, imp in enumerate(improvements):\n",
    "        plt.text(i, imp, f'{imp:.1f}%', ha='center', va='bottom' if imp > 0 else 'top',\n",
    "                fontweight='bold')\n",
    "    \n",
    "    plt.title('GRPO Improvements over SFT')\n",
    "    plt.ylabel('Improvement (%)')\n",
    "    plt.axhline(y=0, color='black', linestyle='-', alpha=0.3)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('sft_vs_grpo_comparison.png', dpi=300)\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nComparison visualization saved as 'sft_vs_grpo_comparison.png'\")\n",
    "    \n",
    "else:\n",
    "    print(\"Insufficient data for SFT vs GRPO comparison\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a72492cb",
   "metadata": {},
   "source": [
    "## 10. Save GRPO Model and Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a37aad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the final GRPO model\n",
    "final_output_dir = f\"{output_dir}/final\"\n",
    "grpo_trainer.save_pretrained(final_output_dir)\n",
    "print(f\"Saved final GRPO model to {final_output_dir}\")\n",
    "\n",
    "# Save training metrics\n",
    "training_metrics = {\n",
    "    'reward_history': reward_history,\n",
    "    'loss_history': loss_history,\n",
    "    'kl_div_history': kl_div_history\n",
    "}\n",
    "\n",
    "import json\n",
    "\n",
    "with open(f\"{output_dir}/training_metrics.json\", 'w') as f:\n",
    "    json.dump(training_metrics, f, indent=2)\n",
    "\n",
    "# Save evaluation results\n",
    "if sft_results and grpo_results:\n",
    "    eval_results = {\n",
    "        'sft_model': {\n",
    "            'model_id': base_model_id,\n",
    "            'adapter_path': adapter_path,\n",
    "            'results': [{\n",
    "                'sample_id': r['sample_id'],\n",
    "                'predicted': [float(x) for x in r['predicted']],\n",
    "                'actual': [float(x) for x in r['actual']],\n",
    "                'mean_abs_diff': float(r['mean_abs_diff']),\n",
    "                'direction_accuracy': float(r['direction_accuracy']),\n",
    "                'reward': float(r['reward'])\n",
    "            } for r in sft_results],\n",
    "            'metrics': {\n",
    "                'mean_abs_diff': float(np.mean(sft_abs_diffs)),\n",
    "                'median_abs_diff': float(np.median(sft_abs_diffs)),\n",
    "                'mean_direction_accuracy': float(np.mean(sft_direction_accs)),\n",
    "                'mean_reward': float(np.mean(sft_rewards))\n",
    "            }\n",
    "        },\n",
    "        'grpo_model': {\n",
    "            'model_id': base_model_id,\n",
    "            'adapter_path': final_output_dir,\n",
    "            'results': [{\n",
    "                'sample_id': r['sample_id'],\n",
    "                'predicted': [float(x) for x in r['predicted']],\n",
    "                'actual': [float(x) for x in r['actual']],\n",
    "                'mean_abs_diff': float(r['mean_abs_diff']),\n",
    "                'direction_accuracy': float(r['direction_accuracy']),\n",
    "                'reward': float(r['reward'])\n",
    "            } for r in grpo_results],\n",
    "            'metrics': {\n",
    "                'mean_abs_diff': float(np.mean(grpo_abs_diffs)),\n",
    "                'median_abs_diff': float(np.median(grpo_abs_diffs)),\n",
    "                'mean_direction_accuracy': float(np.mean(grpo_direction_accs)),\n",
    "                'mean_reward': float(np.mean(grpo_rewards))\n",
    "            }\n",
    "        },\n",
    "        'improvements': {\n",
    "            'abs_diff_improvement': float(abs_diff_improvement),\n",
    "            'direction_improvement': float(direction_improvement),\n",
    "            'reward_improvement': float(reward_improvement)\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    with open(f\"{output_dir}/evaluation_results.json\", 'w') as f:\n",
    "        json.dump(eval_results, f, indent=2)\n",
    "    \n",
    "    print(f\"Saved evaluation results to {output_dir}/evaluation_results.json\")\n",
    "\n",
    "print(\"\\nüìä SUMMARY: GRPO TRAINING FOR BITCOIN PRICE PREDICTION\")\n",
    "print(\"=\"* 60)\n",
    "print(\"‚úÖ Completed GRPO training with price difference rewards\")\n",
    "print(\"‚úÖ Model saved and ready for inference\")\n",
    "print(\"‚úÖ Performance analysis and comparisons completed\")\n",
    "print(\"\\nTo use this model for inference, load it with:\")\n",
    "print(\"```python\")\n",
    "print(\"from transformers import AutoModelForCausalLM, AutoTokenizer\")\n",
    "print(\"from peft import PeftModel\")\n",
    "print(f\"model = AutoModelForCausalLM.from_pretrained('{base_model_id}', trust_remote_code=True)\")\n",
    "print(f\"model = PeftModel.from_pretrained(model, '{final_output_dir}')\")\n",
    "print(f\"tokenizer = AutoTokenizer.from_pretrained('{base_model_id}', trust_remote_code=True)\")\n",
    "print(\"```\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29cfa4fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample prediction with GRPO model\n",
    "print(\"Sample prediction with GRPO model:\")\n",
    "\n",
    "# Use a test sample\n",
    "test_sample = test_dataset[0]\n",
    "prompt = format_prompt(test_sample)\n",
    "\n",
    "# Generate prediction\n",
    "inputs = tokenizer(prompt, return_tensors='pt', truncation=True, max_length=2048)\n",
    "inputs = {k: v.to(grpo_model.device) for k, v in inputs.items()}\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = grpo_model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=256,\n",
    "        do_sample=False,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "generated_text = tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)\n",
    "\n",
    "# Extract predictions\n",
    "predicted_prices = extract_prices_from_text(generated_text)\n",
    "actual_output = test_sample.get('output', '')\n",
    "actual_prices = extract_prices_from_text(actual_output)\n",
    "\n",
    "print(\"Input prompt:\")\n",
    "print(prompt[:500] + \"...\" if len(prompt) > 500 else prompt)\n",
    "print(\"\\nGRPO model prediction:\")\n",
    "print(generated_text[:500] + \"...\" if len(generated_text) > 500 else generated_text)\n",
    "print(f\"\\nPredicted prices: {predicted_prices}\")\n",
    "print(f\"Actual prices: {actual_prices}\")\n",
    "\n",
    "if predicted_prices and actual_prices:\n",
    "    min_len = min(len(predicted_prices), len(actual_prices))\n",
    "    if min_len > 0:\n",
    "        pred_truncated = predicted_prices[:min_len]\n",
    "        actual_truncated = actual_prices[:min_len]\n",
    "        \n",
    "        abs_diffs = np.abs(np.array(pred_truncated) - np.array(actual_truncated))\n",
    "        mean_abs_diff = np.mean(abs_diffs)\n",
    "        reward = calculate_price_difference_reward(pred_truncated, actual_truncated)\n",
    "        \n",
    "        print(f\"\\nMean absolute difference: ${mean_abs_diff:.2f}\")\n",
    "        print(f\"Reward: {reward:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
