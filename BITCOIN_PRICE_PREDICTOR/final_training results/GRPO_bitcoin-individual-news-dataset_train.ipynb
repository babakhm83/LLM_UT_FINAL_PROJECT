{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f173d13",
   "metadata": {},
   "source": [
    "# GRPO Training for Bitcoin News Analysis\n",
    "\n",
    "This notebook implements Group Relative Policy Optimization (GRPO) for enhanced Bitcoin news analysis and prediction.\n",
    "\n",
    "**Dataset**: `bitcoin-individual-news-dataset`\n",
    "\n",
    "**Training Method**: Group Relative Policy Optimization (GRPO)\n",
    "- Preference learning through relative comparisons\n",
    "- Custom reward system for Bitcoin news analysis quality\n",
    "- Multi-response generation and ranking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f340681",
   "metadata": {},
   "source": [
    "## Install Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e25887",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U unsloth\n",
    "# !pip install trl\n",
    "# !pip install accelerate\n",
    "# !pip install datasets\n",
    "# !pip install transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcae0c92",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde4dbdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "from datasets import load_dataset, Dataset\n",
    "import torch, random, os\n",
    "from typing import Dict, List, Any\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import AdamW\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from peft import PeftModel\n",
    "\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b30cad5f",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f9fb44f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model and training configuration\n",
    "MODEL_CONFIG = {\n",
    "    \"base_model_path\": \"unsloth/Qwen2.5-7B-Instruct-bnb-4bit\",  # Base model\n",
    "    \"adapter_path\": \"tahamajs/my-awesome-model_final_bitcoin-individual-news-dataset\",  # Pre-trained adapter\n",
    "    \"checkpoint\": \"checkpoint-1000\",  # Specific checkpoint\n",
    "    \"max_seq_length\": 4096,\n",
    "    \"dtype\": torch.float16,\n",
    "    \"load_in_4bit\": True,\n",
    "    \"lora_r\": 32,\n",
    "    \"lora_alpha\": 32,\n",
    "    \"lora_dropout\": 0.0,\n",
    "    \"target_modules\": [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"]\n",
    "}\n",
    "\n",
    "# Reward model configuration for Bitcoin news analysis\n",
    "REWARD_MODEL_CONFIG = {\n",
    "    \"model_name\": \"OpenAssistant/reward-model-deberta-v3-large-v2\",  # Strong reward model for news analysis\n",
    "    \"max_length\": 512,\n",
    "    \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "}\n",
    "\n",
    "GRPO_CONFIG = {\n",
    "    \"output_dir\": \"qwen_bitcoin_news_grpo_pretrained_enhanced\",\n",
    "    \"num_train_epochs\": 2,  # Reduced since we start from pre-trained\n",
    "    \"per_device_train_batch_size\": 2,\n",
    "    \"gradient_accumulation_steps\": 4,\n",
    "    \"learning_rate\": 5e-6,  # Lower LR for fine-tuning pre-trained model\n",
    "    \"logging_steps\": 5,\n",
    "    \"save_steps\": 50,  # Save more frequently\n",
    "    \"warmup_ratio\": 0.05,  # Less warmup needed\n",
    "    \"group_size\": 4,  # Number of responses to compare per group\n",
    "    \"temperature\": 0.7,  # For response generation\n",
    "}\n",
    "\n",
    "DATASET_NAME = \"tahamajs/bitcoin-individual-news-dataset\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ef69b47",
   "metadata": {},
   "source": [
    "## Load Model and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36af8d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load base model\n",
    "print(f\"üîÑ Loading base model: {MODEL_CONFIG['base_model_path']}\")\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    MODEL_CONFIG[\"base_model_path\"],\n",
    "    max_seq_length=MODEL_CONFIG[\"max_seq_length\"],\n",
    "    dtype=MODEL_CONFIG[\"dtype\"],\n",
    "    load_in_4bit=MODEL_CONFIG[\"load_in_4bit\"],\n",
    ")\n",
    "\n",
    "# Load pre-trained adapter\n",
    "print(f\"üîÑ Loading pre-trained adapter: {MODEL_CONFIG['adapter_path']}/{MODEL_CONFIG['checkpoint']}\")\n",
    "try:\n",
    "    # Load the adapter weights\n",
    "    adapter_path = f\"{MODEL_CONFIG['adapter_path']}/{MODEL_CONFIG['checkpoint']}\"\n",
    "    model = PeftModel.from_pretrained(model, adapter_path)\n",
    "    print(f\"‚úÖ Successfully loaded adapter from {adapter_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Could not load adapter, using base model: {e}\")\n",
    "\n",
    "# Prepare model for further training\n",
    "FastLanguageModel.for_training(model)\n",
    "\n",
    "# Add additional LoRA layers for GRPO training\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=MODEL_CONFIG[\"lora_r\"],\n",
    "    target_modules=MODEL_CONFIG[\"target_modules\"],\n",
    "    lora_alpha=MODEL_CONFIG[\"lora_alpha\"],\n",
    "    lora_dropout=MODEL_CONFIG[\"lora_dropout\"],\n",
    "    use_rslora=True,\n",
    ")\n",
    "\n",
    "print(f\"üìä Model Configuration:\")\n",
    "print(f\"  Base model: {MODEL_CONFIG['base_model_path']}\")\n",
    "print(f\"  Pre-trained adapter: {MODEL_CONFIG['adapter_path']}/{MODEL_CONFIG['checkpoint']}\")\n",
    "print(f\"  Total parameters: {model.num_parameters():,}\")\n",
    "print(f\"  Trainable parameters: {model.num_parameters(only_trainable=True):,}\")\n",
    "\n",
    "# Load reward model for Bitcoin news analysis\n",
    "print(f\"\\nüîÑ Loading reward model: {REWARD_MODEL_CONFIG['model_name']}\")\n",
    "try:\n",
    "    reward_tokenizer = AutoTokenizer.from_pretrained(REWARD_MODEL_CONFIG[\"model_name\"])\n",
    "    reward_model = AutoModelForSequenceClassification.from_pretrained(REWARD_MODEL_CONFIG[\"model_name\"])\n",
    "    reward_model = reward_model.to(REWARD_MODEL_CONFIG[\"device\"])\n",
    "    reward_model.eval()\n",
    "    print(f\"‚úÖ Reward model loaded successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Could not load reward model, using simple reward calculation: {e}\")\n",
    "    reward_model = None\n",
    "    reward_tokenizer = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d98198",
   "metadata": {},
   "source": [
    "## Special Tokens Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "531ffba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define special tokens for news analysis\n",
    "SPECIAL_TOKENS = [\"<|response|>\", \"<|analysis|>\", \"<|sentiment|>\", \"<|impact|>\", \"<|thinking|>\"]\n",
    "num_added = tokenizer.add_special_tokens({\"additional_special_tokens\": SPECIAL_TOKENS})\n",
    "if num_added > 0:\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "    print(f\"Added {num_added} special tokens\")\n",
    "\n",
    "RESPONSE_TAG = \"<|response|>\"\n",
    "ANALYSIS_TAG = \"<|analysis|>\"\n",
    "SENTIMENT_TAG = \"<|sentiment|>\"\n",
    "IMPACT_TAG = \"<|impact|>\"\n",
    "THINKING_TAG = \"<|thinking|>\"\n",
    "\n",
    "response_token_id = tokenizer.convert_tokens_to_ids(RESPONSE_TAG)\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.truncation_side = \"left\"\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "print(f\"Special tokens: {SPECIAL_TOKENS}\")\n",
    "print(f\"Response token ID: {response_token_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f37b7aae",
   "metadata": {},
   "source": [
    "## Load and Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb3e046",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "raw_dataset = load_dataset(DATASET_NAME)\n",
    "print(f\"Loaded dataset: {DATASET_NAME}\")\n",
    "print(f\"Dataset structure: {raw_dataset}\")\n",
    "\n",
    "train_data = raw_dataset[\"train\"]\n",
    "print(f\"Total training samples: {len(train_data)}\")\n",
    "\n",
    "# Show sample data\n",
    "print(\"\\n=== Sample Data ===\")\n",
    "sample = train_data[0]\n",
    "for key, value in sample.items():\n",
    "    print(f\"{key}: {str(value)[:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b550771a",
   "metadata": {},
   "source": [
    "## Data Processing for GRPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "779cc05c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_chat_grpo(ex):\n",
    "    \"\"\"Format example for GRPO training (input only for generation)\"\"\"\n",
    "    instruction = ex.get(\"instruction\", \"\") or \"\"\n",
    "    user_input = ex.get(\"input\", \"\") or \"\"\n",
    "    \n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": instruction},\n",
    "        {\"role\": \"user\", \"content\": user_input},\n",
    "    ]\n",
    "    return {\n",
    "        \"query\": tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True),\n",
    "        \"ground_truth\": ex.get(\"output\", \"\")\n",
    "    }\n",
    "\n",
    "# Format data for GRPO\n",
    "grpo_formatted = train_data.map(format_chat_grpo, remove_columns=train_data.column_names)\n",
    "print(f\"GRPO data prepared: {len(grpo_formatted)} samples\")\n",
    "\n",
    "# Create batches for GRPO training\n",
    "def create_grpo_batches(dataset, batch_size=2):\n",
    "    batches = []\n",
    "    for i in range(0, len(dataset), batch_size):\n",
    "        batch = dataset[i:i + batch_size]\n",
    "        queries = [item[\"query\"] for item in batch]\n",
    "        ground_truths = [item[\"ground_truth\"] for item in batch]\n",
    "        batches.append((queries, ground_truths))\n",
    "    return batches\n",
    "\n",
    "grpo_batches = create_grpo_batches(grpo_formatted, batch_size=GRPO_CONFIG[\"per_device_train_batch_size\"])\n",
    "print(f\"Created {len(grpo_batches)} GRPO batches\")\n",
    "\n",
    "# Show sample GRPO data\n",
    "print(\"\\n=== Sample GRPO Query ===\")\n",
    "print(grpo_formatted[0][\"query\"][:300] + \"...\")\n",
    "print(f\"\\nGround truth: {grpo_formatted[0]['ground_truth'][:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "059d965d",
   "metadata": {},
   "source": [
    "## GRPO Trainer Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f621cf10",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRPOTrainer:\n",
    "    def __init__(self, model, tokenizer, config):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.config = config\n",
    "        self.device = model.device\n",
    "        \n",
    "    def generate_responses(self, queries, num_responses=4, temperature=0.7, max_length=512):\n",
    "        \"\"\"\n",
    "        Generate multiple responses for each query\n",
    "        \"\"\"\n",
    "        self.model.eval()\n",
    "        responses = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for query in queries:\n",
    "                query_responses = []\n",
    "                inputs = self.tokenizer(query, return_tensors=\"pt\", truncation=True, max_length=2048)\n",
    "                inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "                \n",
    "                for _ in range(num_responses):\n",
    "                    outputs = self.model.generate(\n",
    "                        **inputs,\n",
    "                        max_length=inputs[\"input_ids\"].shape[1] + max_length,\n",
    "                        temperature=temperature,\n",
    "                        do_sample=True,\n",
    "                        pad_token_id=self.tokenizer.eos_token_id,\n",
    "                        eos_token_id=self.tokenizer.eos_token_id,\n",
    "                    )\n",
    "                    \n",
    "                    response = self.tokenizer.decode(\n",
    "                        outputs[0][inputs[\"input_ids\"].shape[1]:], \n",
    "                        skip_special_tokens=True\n",
    "                    )\n",
    "                    query_responses.append(response.strip())\n",
    "                \n",
    "                responses.append(query_responses)\n",
    "        \n",
    "        return responses\n",
    "    \n",
    "    def compute_rewards(self, queries, responses, ground_truths):\n",
    "        \"\"\"\n",
    "        Compute rewards for responses based on Bitcoin news analysis quality\n",
    "        \"\"\"\n",
    "        rewards = []\n",
    "        \n",
    "        for query_responses, gt in zip(responses, ground_truths):\n",
    "            query_rewards = []\n",
    "            \n",
    "            for response in query_responses:\n",
    "                reward = self._calculate_news_analysis_reward(response, gt)\n",
    "                query_rewards.append(reward)\n",
    "            \n",
    "            rewards.append(query_rewards)\n",
    "        \n",
    "        return rewards\n",
    "    \n",
    "    def _calculate_news_analysis_reward(self, response, ground_truth):\n",
    "        \"\"\"\n",
    "        Calculate reward for a news analysis response using both rule-based and model-based scoring\n",
    "        \"\"\"\n",
    "        total_reward = 0.0\n",
    "        \n",
    "        # Rule-based rewards (40% weight)\n",
    "        rule_reward = 0.0\n",
    "        \n",
    "        # Length penalty/reward\n",
    "        if 50 <= len(response) <= 500:\n",
    "            rule_reward += 0.1\n",
    "        \n",
    "        # Contains analysis keywords\n",
    "        analysis_keywords = ['analysis', 'sentiment', 'impact', 'market', 'bitcoin', 'price', 'news']\n",
    "        response_lower = response.lower()\n",
    "        keyword_count = sum(1 for keyword in analysis_keywords if keyword in response_lower)\n",
    "        rule_reward += min(0.3, keyword_count * 0.05)\n",
    "        \n",
    "        # Contains sentiment indicators\n",
    "        sentiment_words = ['positive', 'negative', 'neutral', 'bullish', 'bearish', 'optimistic', 'pessimistic']\n",
    "        sentiment_count = sum(1 for word in sentiment_words if word in response_lower)\n",
    "        if sentiment_count > 0:\n",
    "            rule_reward += 0.2\n",
    "        \n",
    "        # Contains structured tags\n",
    "        if ANALYSIS_TAG in response:\n",
    "            rule_reward += 0.2\n",
    "        if SENTIMENT_TAG in response:\n",
    "            rule_reward += 0.2\n",
    "        if IMPACT_TAG in response:\n",
    "            rule_reward += 0.2\n",
    "        \n",
    "        # Simple text similarity with ground truth\n",
    "        common_words = set(response.lower().split()) & set(ground_truth.lower().split())\n",
    "        if len(common_words) > 0:\n",
    "            rule_reward += min(0.3, len(common_words) * 0.02)\n",
    "        \n",
    "        total_reward += rule_reward * 0.4\n",
    "        \n",
    "        # Model-based reward using reward model (60% weight)\n",
    "        if reward_model is not None and reward_tokenizer is not None:\n",
    "            try:\n",
    "                # Create input for reward model (query + response format)\n",
    "                reward_input = f\"News Analysis: {response}\\nReference: {ground_truth}\"\n",
    "                \n",
    "                # Tokenize and get reward score\n",
    "                inputs = reward_tokenizer(\n",
    "                    reward_input, \n",
    "                    return_tensors=\"pt\", \n",
    "                    truncation=True, \n",
    "                    max_length=REWARD_MODEL_CONFIG[\"max_length\"]\n",
    "                ).to(REWARD_MODEL_CONFIG[\"device\"])\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    outputs = reward_model(**inputs)\n",
    "                    # Get reward score (assuming higher score is better)\n",
    "                    reward_score = torch.sigmoid(outputs.logits).cpu().item()\n",
    "                \n",
    "                total_reward += reward_score * 0.6\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è Reward model error: {e}\")\n",
    "                # Fallback to rule-based only\n",
    "                total_reward = rule_reward\n",
    "        else:\n",
    "            # Use only rule-based reward if model not available\n",
    "            total_reward = rule_reward\n",
    "        \n",
    "        return total_reward\n",
    "    \n",
    "    def compute_grpo_loss(self, queries, responses, rewards):\n",
    "        \"\"\"\n",
    "        Compute GRPO loss based on relative preferences within groups\n",
    "        \"\"\"\n",
    "        self.model.train()\n",
    "        total_loss = 0.0\n",
    "        num_pairs = 0\n",
    "        \n",
    "        for query, query_responses, query_rewards in zip(queries, responses, rewards):\n",
    "            if len(query_responses) < 2:\n",
    "                continue\n",
    "            \n",
    "            # Tokenize query and responses\n",
    "            query_tokens = self.tokenizer(query, return_tensors=\"pt\", truncation=True, max_length=2048)\n",
    "            query_tokens = {k: v.to(self.device) for k, v in query_tokens.items()}\n",
    "            \n",
    "            response_logprobs = []\n",
    "            \n",
    "            for response in query_responses:\n",
    "                # Create full text (query + response)\n",
    "                full_text = query + \" \" + response\n",
    "                tokens = self.tokenizer(full_text, return_tensors=\"pt\", truncation=True, max_length=4096)\n",
    "                tokens = {k: v.to(self.device) for k, v in tokens.items()}\n",
    "                \n",
    "                # Get model outputs\n",
    "                outputs = self.model(**tokens)\n",
    "                logits = outputs.logits\n",
    "                \n",
    "                # Calculate log probabilities for the response part\n",
    "                query_length = query_tokens[\"input_ids\"].shape[1]\n",
    "                response_logits = logits[0, query_length-1:-1]  # Shift for next token prediction\n",
    "                response_tokens = tokens[\"input_ids\"][0, query_length:]\n",
    "                \n",
    "                if response_tokens.numel() > 0:\n",
    "                    log_probs = F.log_softmax(response_logits, dim=-1)\n",
    "                    response_log_prob = log_probs.gather(1, response_tokens.unsqueeze(-1)).squeeze(-1)\n",
    "                    avg_log_prob = response_log_prob.mean()\n",
    "                    response_logprobs.append(avg_log_prob)\n",
    "                else:\n",
    "                    response_logprobs.append(torch.tensor(0.0, device=self.device))\n",
    "            \n",
    "            if len(response_logprobs) >= 2:\n",
    "                # Compute pairwise losses\n",
    "                for i in range(len(response_logprobs)):\n",
    "                    for j in range(i + 1, len(response_logprobs)):\n",
    "                        if query_rewards[i] != query_rewards[j]:  # Only if different rewards\n",
    "                            # Preference: higher reward should have higher log prob\n",
    "                            if query_rewards[i] > query_rewards[j]:\n",
    "                                preferred_logprob = response_logprobs[i]\n",
    "                                dispreferred_logprob = response_logprobs[j]\n",
    "                            else:\n",
    "                                preferred_logprob = response_logprobs[j]\n",
    "                                dispreferred_logprob = response_logprobs[i]\n",
    "                            \n",
    "                            # GRPO loss: negative log sigmoid of difference\n",
    "                            diff = preferred_logprob - dispreferred_logprob\n",
    "                            loss = -F.logsigmoid(diff)\n",
    "                            total_loss += loss\n",
    "                            num_pairs += 1\n",
    "        \n",
    "        return total_loss / max(num_pairs, 1)\n",
    "    \n",
    "    def train_step(self, batch_queries, batch_ground_truths):\n",
    "        \"\"\"\n",
    "        Perform one training step of GRPO\n",
    "        \"\"\"\n",
    "        # Generate responses\n",
    "        responses = self.generate_responses(\n",
    "            batch_queries, \n",
    "            num_responses=self.config[\"group_size\"],\n",
    "            temperature=self.config[\"temperature\"]\n",
    "        )\n",
    "        \n",
    "        # Compute rewards\n",
    "        rewards = self.compute_rewards(batch_queries, responses, batch_ground_truths)\n",
    "        \n",
    "        # Compute and return GRPO loss\n",
    "        loss = self.compute_grpo_loss(batch_queries, responses, rewards)\n",
    "        \n",
    "        return loss, responses, rewards\n",
    "\n",
    "print(\"‚úÖ GRPO Trainer class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e40a177a",
   "metadata": {},
   "source": [
    "## GRPO Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c02c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize GRPO trainer\n",
    "grpo_trainer = GRPOTrainer(model, tokenizer, GRPO_CONFIG)\n",
    "\n",
    "# Setup optimizer for GRPO\n",
    "optimizer = AdamW(model.parameters(), lr=GRPO_CONFIG[\"learning_rate\"])\n",
    "\n",
    "print(\"üöÄ Starting GRPO training...\")\n",
    "print(f\"Training on {len(grpo_batches)} batches for {GRPO_CONFIG['num_train_epochs']} epochs\")\n",
    "\n",
    "# Training loop\n",
    "training_logs = []\n",
    "global_step = 0\n",
    "\n",
    "for epoch in range(GRPO_CONFIG[\"num_train_epochs\"]):\n",
    "    epoch_loss = 0.0\n",
    "    epoch_batches = 0\n",
    "    \n",
    "    progress_bar = tqdm(grpo_batches, desc=f\"GRPO Epoch {epoch+1}/{GRPO_CONFIG['num_train_epochs']}\")\n",
    "    \n",
    "    for batch_queries, batch_ground_truths in progress_bar:\n",
    "        try:\n",
    "            # Perform training step\n",
    "            loss, responses, rewards = grpo_trainer.train_step(batch_queries, batch_ground_truths)\n",
    "            \n",
    "            if loss.requires_grad:\n",
    "                # Backward pass\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                \n",
    "                # Gradient clipping\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "                \n",
    "                optimizer.step()\n",
    "                \n",
    "                epoch_loss += loss.item()\n",
    "                epoch_batches += 1\n",
    "                global_step += 1\n",
    "                \n",
    "                # Update progress bar\n",
    "                progress_bar.set_postfix({\n",
    "                    'loss': f'{loss.item():.4f}',\n",
    "                    'avg_loss': f'{epoch_loss/epoch_batches:.4f}'\n",
    "                })\n",
    "                \n",
    "                # Logging\n",
    "                if global_step % GRPO_CONFIG[\"logging_steps\"] == 0:\n",
    "                    log_entry = {\n",
    "                        \"epoch\": epoch,\n",
    "                        \"step\": global_step,\n",
    "                        \"loss\": loss.item(),\n",
    "                        \"avg_epoch_loss\": epoch_loss / epoch_batches,\n",
    "                        \"timestamp\": datetime.now().isoformat()\n",
    "                    }\n",
    "                    training_logs.append(log_entry)\n",
    "                    \n",
    "                    print(f\"\\nStep {global_step}: Loss = {loss.item():.4f}\")\n",
    "                    print(f\"Sample responses for batch:\")\n",
    "                    for i, (query_responses, query_rewards) in enumerate(zip(responses[:1], rewards[:1])):\n",
    "                        print(f\"  Query {i+1} responses:\")\n",
    "                        for j, (resp, rew) in enumerate(zip(query_responses[:2], query_rewards[:2])):\n",
    "                            print(f\"    Response {j+1} (reward: {rew:.3f}): {resp[:100]}...\")\n",
    "                \n",
    "                # Save checkpoint\n",
    "                if global_step % GRPO_CONFIG[\"save_steps\"] == 0:\n",
    "                    checkpoint_dir = f\"{GRPO_CONFIG['output_dir']}/checkpoint-{global_step}\"\n",
    "                    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "                    model.save_pretrained(f\"{checkpoint_dir}/lora_adapter\")\n",
    "                    tokenizer.save_pretrained(checkpoint_dir)\n",
    "                    print(f\"\\nüíæ Checkpoint saved at step {global_step}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"\\n‚ö†Ô∏è Error in batch: {e}\")\n",
    "            continue\n",
    "    \n",
    "    avg_epoch_loss = epoch_loss / max(epoch_batches, 1)\n",
    "    print(f\"\\n‚úÖ Epoch {epoch+1} completed. Average loss: {avg_epoch_loss:.4f}\")\n",
    "\n",
    "print(\"\\nüéâ GRPO training completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d9ae200",
   "metadata": {},
   "source": [
    "## Save GRPO Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa2c435",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save final GRPO model\n",
    "final_model_dir = f\"{GRPO_CONFIG['output_dir']}/final_model\"\n",
    "os.makedirs(final_model_dir, exist_ok=True)\n",
    "\n",
    "model.save_pretrained(f\"{final_model_dir}/lora_adapter\")\n",
    "tokenizer.save_pretrained(final_model_dir)\n",
    "\n",
    "print(f\"‚úÖ Final GRPO model saved to {final_model_dir}\")\n",
    "\n",
    "# Save GRPO training logs\n",
    "with open(f\"{GRPO_CONFIG['output_dir']}/grpo_training_logs.json\", \"w\") as f:\n",
    "    json.dump(training_logs, f, indent=2)\n",
    "\n",
    "print(f\"GRPO training logs saved to {GRPO_CONFIG['output_dir']}/grpo_training_logs.json\")\n",
    "\n",
    "# Create training summary\n",
    "training_summary = {\n",
    "    \"dataset\": DATASET_NAME,\n",
    "    \"model_config\": MODEL_CONFIG,\n",
    "    \"grpo_config\": GRPO_CONFIG,\n",
    "    \"total_samples\": len(train_data),\n",
    "    \"total_grpo_steps\": global_step,\n",
    "    \"training_completed\": datetime.now().isoformat(),\n",
    "    \"final_model_path\": final_model_dir\n",
    "}\n",
    "\n",
    "with open(f\"{GRPO_CONFIG['output_dir']}/training_summary.json\", \"w\") as f:\n",
    "    json.dump(training_summary, f, indent=2)\n",
    "\n",
    "print(f\"Training summary saved to {GRPO_CONFIG['output_dir']}/training_summary.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b7aea9",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "792353a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the final model\n",
    "print(\"üß™ Testing the final GRPO model...\")\n",
    "\n",
    "def test_model_generation(model, tokenizer, test_query, max_length=512):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        inputs = tokenizer(test_query, return_tensors=\"pt\", truncation=True, max_length=2048)\n",
    "        inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "        \n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_length=inputs[\"input_ids\"].shape[1] + max_length,\n",
    "            temperature=0.7,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "        \n",
    "        response = tokenizer.decode(\n",
    "            outputs[0][inputs[\"input_ids\"].shape[1]:], \n",
    "            skip_special_tokens=True\n",
    "        )\n",
    "        return response.strip()\n",
    "\n",
    "# Create a test query\n",
    "test_sample = grpo_formatted[0]\n",
    "test_query = test_sample[\"query\"]\n",
    "ground_truth = test_sample[\"ground_truth\"]\n",
    "\n",
    "print(\"=== Test Query ===\")\n",
    "print(test_query[:500] + \"...\")\n",
    "\n",
    "print(\"\\n=== Ground Truth ===\")\n",
    "print(ground_truth)\n",
    "\n",
    "print(\"\\n=== Model Response ===\")\n",
    "response = test_model_generation(model, tokenizer, test_query)\n",
    "print(response)\n",
    "\n",
    "print(\"\\n‚úÖ Model evaluation completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4de4349",
   "metadata": {},
   "source": [
    "## Training Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b8da69",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üìä GRPO Training Summary\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Dataset: {DATASET_NAME}\")\n",
    "print(f\"Total samples: {len(train_data):,}\")\n",
    "print(f\"Training method: Group Relative Policy Optimization (GRPO)\")\n",
    "print(\"\\nüéØ Training Configuration:\")\n",
    "print(f\"GRPO epochs: {GRPO_CONFIG['num_train_epochs']}\")\n",
    "print(f\"Total GRPO steps: {global_step}\")\n",
    "print(f\"Batch size: {GRPO_CONFIG['per_device_train_batch_size']}\")\n",
    "print(f\"Learning rate: {GRPO_CONFIG['learning_rate']}\")\n",
    "print(f\"Group size: {GRPO_CONFIG['group_size']}\")\n",
    "print(\"\\nüíæ Model Output:\")\n",
    "print(f\"Final GRPO model: {final_model_dir}\")\n",
    "print(\"\\nüî¨ Key Features:\")\n",
    "print(\"‚úÖ Group Relative Policy Optimization (GRPO) for preference learning\")\n",
    "print(\"‚úÖ Custom reward system for Bitcoin news analysis quality\")\n",
    "print(\"‚úÖ Multi-response generation and ranking\")\n",
    "print(\"‚úÖ Sentiment and impact analysis optimization\")\n",
    "print(\"‚úÖ Special token handling for structured news analysis\")\n",
    "print(\"\\nüéâ GRPO training pipeline completed successfully!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
