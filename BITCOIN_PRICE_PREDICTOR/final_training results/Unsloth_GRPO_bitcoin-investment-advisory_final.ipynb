{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81d891e0",
   "metadata": {},
   "source": [
    "# Unsloth GRPO Training for Bitcoin Investment Advisory\n",
    "\n",
    "This notebook implements Group Relative Policy Optimization (GRPO) using Unsloth for Bitcoin investment advisory.\n",
    "\n",
    "**Dataset**: `bitcoin-investment-advisory-dataset`\n",
    "\n",
    "**Training Method**: Unsloth GRPO\n",
    "- Built-in preference learning optimization\n",
    "- Efficient memory usage with Unsloth\n",
    "- Streamlined training pipeline for investment advisory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9bd0a7c",
   "metadata": {},
   "source": [
    "## Install Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a24972",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
    "# !pip install -U xformers trl peft accelerate bitsandbytes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c75cace",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20420636",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel, is_bfloat16_supported\n",
    "from unsloth.chat_templates import get_chat_template\n",
    "from trl import GRPOTrainer, GRPOConfig\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import TrainingArguments, AutoTokenizer, AutoModelForSequenceClassification\n",
    "from peft import PeftModel\n",
    "import torch, random, os\n",
    "import json\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f1e8dbe",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ebcdb01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model configuration\n",
    "BASE_MODEL_NAME = \"unsloth/Qwen2.5-7B-Instruct-bnb-4bit\"  # Base model\n",
    "ADAPTER_PATH = \"tahamajs/my-awesome-model_final_for_explanation_end_summerized\"  # Pre-trained adapter\n",
    "CHECKPOINT = \"checkpoint-800\"  # Specific checkpoint\n",
    "MAX_SEQ_LENGTH = 2048\n",
    "DTYPE = None  # Auto-detection\n",
    "LOAD_IN_4BIT = True\n",
    "\n",
    "# LoRA configuration\n",
    "LORA_R = 16\n",
    "LORA_ALPHA = 16\n",
    "LORA_DROPOUT = 0.0\n",
    "TARGET_MODULES = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"]\n",
    "\n",
    "# GRPO configuration\n",
    "OUTPUT_DIR = \"./qwen_bitcoin_advisory_grpo_unsloth_pretrained\"\n",
    "LEARNING_RATE = 3e-7  # Lower for pre-trained model\n",
    "NUM_TRAIN_EPOCHS = 1\n",
    "PER_DEVICE_TRAIN_BATCH_SIZE = 1\n",
    "GRADIENT_ACCUMULATION_STEPS = 8\n",
    "MAX_LENGTH = 1024\n",
    "MAX_PROMPT_LENGTH = 512\n",
    "BETA = 0.1\n",
    "\n",
    "# Dataset\n",
    "DATASET_NAME = \"tahamajs/bitcoin-investment-advisory-dataset\"\n",
    "\n",
    "# Reward model for financial advisory\n",
    "REWARD_MODEL_NAME = \"ProsusAI/finbert\"  # Specialized for financial text analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "942b48a5",
   "metadata": {},
   "source": [
    "## Load Model and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c3f2a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load base model\n",
    "print(f\"🔄 Loading base model: {BASE_MODEL_NAME}\")\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=BASE_MODEL_NAME,\n",
    "    max_seq_length=MAX_SEQ_LENGTH,\n",
    "    dtype=DTYPE,\n",
    "    load_in_4bit=LOAD_IN_4BIT,\n",
    ")\n",
    "\n",
    "# Load pre-trained adapter\n",
    "print(f\"🔄 Loading pre-trained adapter: {ADAPTER_PATH}/{CHECKPOINT}\")\n",
    "try:\n",
    "    adapter_path = f\"{ADAPTER_PATH}/{CHECKPOINT}\"\n",
    "    model = PeftModel.from_pretrained(model, adapter_path)\n",
    "    print(f\"✅ Successfully loaded adapter from {adapter_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"⚠️ Could not load adapter, using base model: {e}\")\n",
    "\n",
    "# Apply chat template\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template=\"chatml\",  # Supports Qwen models\n",
    ")\n",
    "\n",
    "# Prepare model for PEFT training\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=LORA_R,\n",
    "    target_modules=TARGET_MODULES,\n",
    "    lora_alpha=LORA_ALPHA,\n",
    "    lora_dropout=LORA_DROPOUT,\n",
    "    bias=\"none\",\n",
    "    use_gradient_checkpointing=\"unsloth\",\n",
    "    random_state=SEED,\n",
    "    use_rslora=False,\n",
    "    loftq_config=None,\n",
    ")\n",
    "\n",
    "# Load reward model for financial advisory\n",
    "print(f\"\\n🔄 Loading reward model: {REWARD_MODEL_NAME}\")\n",
    "try:\n",
    "    reward_tokenizer = AutoTokenizer.from_pretrained(REWARD_MODEL_NAME)\n",
    "    reward_model = AutoModelForSequenceClassification.from_pretrained(REWARD_MODEL_NAME)\n",
    "    reward_model.eval()\n",
    "    print(f\"✅ Reward model loaded successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"⚠️ Could not load reward model, using rule-based rewards: {e}\")\n",
    "    reward_model = None\n",
    "    reward_tokenizer = None\n",
    "\n",
    "print(f\"\\n📊 Model Configuration:\")\n",
    "print(f\"  Base model: {BASE_MODEL_NAME}\")\n",
    "print(f\"  Pre-trained adapter: {ADAPTER_PATH}/{CHECKPOINT}\")\n",
    "print(f\"  Max sequence length: {MAX_SEQ_LENGTH}\")\n",
    "print(f\"  LoRA rank: {LORA_R}\")\n",
    "print(f\"  Load in 4bit: {LOAD_IN_4BIT}\")\n",
    "print(f\"  Reward model: {REWARD_MODEL_NAME if reward_model else 'Rule-based only'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1684106c",
   "metadata": {},
   "source": [
    "## Load and Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b02491",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "dataset = load_dataset(DATASET_NAME, split=\"train\")\n",
    "print(f\"Dataset loaded: {DATASET_NAME}\")\n",
    "print(f\"Total samples: {len(dataset):,}\")\n",
    "\n",
    "# Show sample\n",
    "print(\"\\n=== Sample Data ===\")\n",
    "sample = dataset[0]\n",
    "for key, value in sample.items():\n",
    "    print(f\"{key}: {str(value)[:150]}{'...' if len(str(value)) > 150 else ''}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81425f1d",
   "metadata": {},
   "source": [
    "## Format Dataset for GRPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de854c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatting_prompts_func(examples):\n",
    "    \"\"\"\n",
    "    Format examples for Unsloth GRPO training.\n",
    "    Creates conversational format with system, user, and assistant messages.\n",
    "    \"\"\"\n",
    "    instructions = examples.get(\"instruction\", [\"\"] * len(examples.get(\"input\", [])))\n",
    "    inputs = examples.get(\"input\", [])\n",
    "    outputs = examples.get(\"output\", [])\n",
    "    \n",
    "    conversations = []\n",
    "    for instruction, user_input, output in zip(instructions, inputs, outputs):\n",
    "        conversation = [\n",
    "            {\"role\": \"system\", \"content\": instruction or \"You are a professional Bitcoin investment advisor.\"},\n",
    "            {\"role\": \"user\", \"content\": user_input or \"\"},\n",
    "            {\"role\": \"assistant\", \"content\": output or \"\"},\n",
    "        ]\n",
    "        conversations.append(conversation)\n",
    "    \n",
    "    return {\"conversations\": conversations}\n",
    "\n",
    "# Format dataset\n",
    "formatted_dataset = dataset.map(\n",
    "    formatting_prompts_func,\n",
    "    batched=True,\n",
    "    remove_columns=dataset.column_names\n",
    ")\n",
    "\n",
    "print(f\"Formatted dataset: {len(formatted_dataset):,} samples\")\n",
    "\n",
    "# Show formatted sample\n",
    "print(\"\\n=== Formatted Sample ===\")\n",
    "sample_conv = formatted_dataset[0][\"conversations\"]\n",
    "for msg in sample_conv:\n",
    "    print(f\"**{msg['role'].upper()}**: {msg['content'][:200]}{'...' if len(msg['content']) > 200 else ''}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e7b108",
   "metadata": {},
   "outputs": [],
   "source": [
    "training# Enhanced Reward Function for Bitcoin Investment Advisory\n",
    "def calculate_investment_advisory_reward(response, ground_truth, reward_model=None, reward_tokenizer=None):\n",
    "    \"\"\"\n",
    "    Comprehensive reward function for Bitcoin investment advisory responses.\n",
    "    Combines multiple scoring mechanisms for robust evaluation.\n",
    "    \"\"\"\n",
    "    total_reward = 0.0\n",
    "    \n",
    "    # 1. Rule-based Investment Quality Assessment (40% weight)\n",
    "    investment_reward = 0.0\n",
    "    \n",
    "    # Length appropriateness (investment advice should be detailed but not too verbose)\n",
    "    response_len = len(response)\n",
    "    if 100 <= response_len <= 800:\n",
    "        investment_reward += 0.15\n",
    "    elif 50 <= response_len <= 1200:\n",
    "        investment_reward += 0.10\n",
    "    elif response_len < 50:\n",
    "        investment_reward -= 0.10  # Too short for investment advice\n",
    "    \n",
    "    # Investment-specific keywords and concepts\n",
    "    investment_keywords = {\n",
    "        'strategy': 0.08, 'portfolio': 0.08, 'risk': 0.10, 'diversification': 0.05,\n",
    "        'bitcoin': 0.05, 'cryptocurrency': 0.05, 'investment': 0.08, 'allocation': 0.06,\n",
    "        'market': 0.05, 'analysis': 0.06, 'recommendation': 0.08, 'advice': 0.05,\n",
    "        'hodl': 0.04, 'dca': 0.06, 'volatility': 0.05, 'return': 0.05\n",
    "    }\n",
    "    \n",
    "    response_lower = response.lower()\n",
    "    for keyword, weight in investment_keywords.items():\n",
    "        if keyword in response_lower:\n",
    "            investment_reward += weight\n",
    "    \n",
    "    # Risk management indicators (crucial for investment advice)\n",
    "    risk_indicators = ['risk tolerance', 'risk management', 'stop loss', 'risk assessment', \n",
    "                      'conservative', 'aggressive', 'moderate risk', 'diversify', 'hedge']\n",
    "    risk_score = sum(0.05 for indicator in risk_indicators if indicator in response_lower)\n",
    "    investment_reward += min(0.20, risk_score)\n",
    "    \n",
    "    # Financial metrics and numbers (good investment advice includes specifics)\n",
    "    import re\n",
    "    percentage_matches = re.findall(r'\\d+\\.?\\d*%', response)\n",
    "    dollar_matches = re.findall(r'\\$\\d+', response)\n",
    "    if percentage_matches or dollar_matches:\n",
    "        investment_reward += 0.10\n",
    "    \n",
    "    # Investment timeframe mentions\n",
    "    timeframes = ['short-term', 'long-term', 'monthly', 'yearly', 'quarterly', 'daily', \n",
    "                 'weeks', 'months', 'years']\n",
    "    if any(timeframe in response_lower for timeframe in timeframes):\n",
    "        investment_reward += 0.08\n",
    "    \n",
    "    # Structured advice format\n",
    "    if any(marker in response for marker in ['1.', '2.', '•', '-', 'Step']):\n",
    "        investment_reward += 0.10\n",
    "    \n",
    "    total_reward += investment_reward * 0.4\n",
    "    \n",
    "    # 2. Sentiment and Tone Analysis (20% weight)\n",
    "    sentiment_reward = 0.0\n",
    "    \n",
    "    # Professional tone indicators\n",
    "    professional_phrases = ['recommend', 'suggest', 'consider', 'analysis shows', 'based on',\n",
    "                          'in my opinion', 'professional advice', 'financial guidance']\n",
    "    professional_score = sum(0.03 for phrase in professional_phrases if phrase in response_lower)\n",
    "    sentiment_reward += min(0.15, professional_score)\n",
    "    \n",
    "    # Balanced perspective (good investment advice shows multiple viewpoints)\n",
    "    balance_indicators = ['however', 'on the other hand', 'alternatively', 'but', 'although',\n",
    "                         'pros and cons', 'advantages', 'disadvantages', 'benefits', 'risks']\n",
    "    balance_score = sum(0.02 for indicator in balance_indicators if indicator in response_lower)\n",
    "    sentiment_reward += min(0.10, balance_score)\n",
    "    \n",
    "    total_reward += sentiment_reward * 0.2\n",
    "    \n",
    "    # 3. Content Similarity with Ground Truth (25% weight)\n",
    "    similarity_reward = 0.0\n",
    "    \n",
    "    # Token overlap\n",
    "    response_tokens = set(response.lower().split())\n",
    "    gt_tokens = set(ground_truth.lower().split())\n",
    "    \n",
    "    if len(gt_tokens) > 0:\n",
    "        jaccard_similarity = len(response_tokens & gt_tokens) / len(response_tokens | gt_tokens)\n",
    "        similarity_reward += jaccard_similarity * 0.6\n",
    "        \n",
    "        # Key concept alignment\n",
    "        key_concepts = ['buy', 'sell', 'hold', 'accumulate', 'wait', 'caution', 'bullish', 'bearish']\n",
    "        concept_matches = sum(1 for concept in key_concepts \n",
    "                            if concept in response_lower and concept in ground_truth.lower())\n",
    "        if concept_matches > 0:\n",
    "            similarity_reward += min(0.4, concept_matches * 0.1)\n",
    "    \n",
    "    total_reward += similarity_reward * 0.25\n",
    "    \n",
    "    # 4. AI Reward Model Assessment (15% weight)\n",
    "    if reward_model is not None and reward_tokenizer is not None:\n",
    "        try:\n",
    "            # Format input for financial sentiment analysis\n",
    "            model_input = f\"Investment Advice: {response[:400]}\"  # Limit length for model\n",
    "            \n",
    "            inputs = reward_tokenizer(\n",
    "                model_input, \n",
    "                return_tensors=\"pt\", \n",
    "                truncation=True, \n",
    "                max_length=512,\n",
    "                padding=True\n",
    "            )\n",
    "            \n",
    "            if torch.cuda.is_available():\n",
    "                inputs = {k: v.cuda() for k, v in inputs.items()}\n",
    "                reward_model = reward_model.cuda()\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = reward_model(**inputs)\n",
    "                # For FinBERT, we want positive financial sentiment\n",
    "                logits = outputs.logits\n",
    "                probabilities = torch.softmax(logits, dim=-1)\n",
    "                \n",
    "                # FinBERT typically has [negative, neutral, positive] classes\n",
    "                if probabilities.shape[-1] == 3:\n",
    "                    positive_score = probabilities[0, 2].item()  # Positive class\n",
    "                    neutral_score = probabilities[0, 1].item()   # Neutral class\n",
    "                    # Weight positive sentiment higher for investment advice\n",
    "                    ai_reward = positive_score * 0.8 + neutral_score * 0.2\n",
    "                else:\n",
    "                    ai_reward = torch.sigmoid(logits).mean().item()\n",
    "                \n",
    "                total_reward += ai_reward * 0.15\n",
    "                \n",
    "        except Exception as e:\n",
    "            # Fallback: use rule-based scoring only\n",
    "            print(f\"⚠️ Reward model error: {e}\")\n",
    "            pass\n",
    "    \n",
    "    # 5. Penalty for Poor Investment Advice (-0.5 to 0 weight)\n",
    "    penalties = 0.0\n",
    "    \n",
    "    # Penalty for overly speculative language\n",
    "    speculative_words = ['guaranteed', 'definitely will', 'certain profit', 'no risk', 'sure thing']\n",
    "    speculation_penalty = sum(0.1 for word in speculative_words if word in response_lower)\n",
    "    penalties -= min(0.3, speculation_penalty)\n",
    "    \n",
    "    # Penalty for financial advice disclaimers absence (good practice to include them)\n",
    "    disclaimer_phrases = ['not financial advice', 'dyor', 'do your own research', 'consult', 'disclaimer']\n",
    "    if not any(phrase in response_lower for phrase in disclaimer_phrases) and len(response) > 200:\n",
    "        penalties -= 0.05\n",
    "    \n",
    "    total_reward += penalties\n",
    "    \n",
    "    # Ensure reward is in reasonable range [0, 1]\n",
    "    total_reward = max(0.0, min(1.0, total_reward))\n",
    "    \n",
    "    return total_reward\n",
    "\n",
    "print(\"✅ Enhanced investment advisory reward function created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e615716",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom GRPO Trainer with Enhanced Reward Integration\n",
    "class CustomGRPOTrainer(GRPOTrainer):\n",
    "    \"\"\"\n",
    "    Custom GRPO trainer that integrates our enhanced reward function\n",
    "    for Bitcoin investment advisory training.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, reward_model=None, reward_tokenizer=None, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.reward_model = reward_model\n",
    "        self.reward_tokenizer = reward_tokenizer\n",
    "        \n",
    "    def compute_rewards(self, queries, responses, ground_truths=None):\n",
    "        \"\"\"\n",
    "        Compute rewards using our enhanced investment advisory reward function.\n",
    "        \"\"\"\n",
    "        rewards = []\n",
    "        \n",
    "        for i, (query_responses) in enumerate(responses):\n",
    "            query_rewards = []\n",
    "            ground_truth = ground_truths[i] if ground_truths and i < len(ground_truths) else \"\"\n",
    "            \n",
    "            for response in query_responses:\n",
    "                # Use our enhanced reward function\n",
    "                reward = calculate_investment_advisory_reward(\n",
    "                    response=response,\n",
    "                    ground_truth=ground_truth,\n",
    "                    reward_model=self.reward_model,\n",
    "                    reward_tokenizer=self.reward_tokenizer\n",
    "                )\n",
    "                query_rewards.append(reward)\n",
    "            \n",
    "            rewards.append(query_rewards)\n",
    "        \n",
    "        return rewards\n",
    "    \n",
    "    def generate_completions(self, queries, num_completions=4):\n",
    "        \"\"\"\n",
    "        Generate multiple completions for each query using the model.\n",
    "        \"\"\"\n",
    "        all_completions = []\n",
    "        \n",
    "        for query in queries:\n",
    "            completions = []\n",
    "            \n",
    "            # Tokenize the query\n",
    "            inputs = self.tokenizer(\n",
    "                query, \n",
    "                return_tensors=\"pt\", \n",
    "                truncation=True, \n",
    "                max_length=self.max_prompt_length\n",
    "            ).to(self.model.device)\n",
    "            \n",
    "            for _ in range(num_completions):\n",
    "                with torch.no_grad():\n",
    "                    outputs = self.model.generate(\n",
    "                        **inputs,\n",
    "                        max_new_tokens=min(512, self.max_length - inputs['input_ids'].shape[1]),\n",
    "                        temperature=0.8,\n",
    "                        do_sample=True,\n",
    "                        pad_token_id=self.tokenizer.eos_token_id,\n",
    "                        eos_token_id=self.tokenizer.eos_token_id,\n",
    "                        repetition_penalty=1.1,\n",
    "                    )\n",
    "                \n",
    "                # Decode the generated response\n",
    "                generated_ids = outputs[0][inputs['input_ids'].shape[1]:]\n",
    "                response = self.tokenizer.decode(generated_ids, skip_special_tokens=True).strip()\n",
    "                completions.append(response)\n",
    "            \n",
    "            all_completions.append(completions)\n",
    "        \n",
    "        return all_completions\n",
    "\n",
    "print(\"✅ Custom GRPO trainer with enhanced rewards created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b84258a9",
   "metadata": {},
   "source": [
    "## Setup GRPO Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "091d1cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    num_train_epochs=NUM_TRAIN_EPOCHS,\n",
    "    per_device_train_batch_size=PER_DEVICE_TRAIN_BATCH_SIZE,\n",
    "    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    logging_steps=10,\n",
    "    save_steps=100,\n",
    "    save_strategy=\"steps\",\n",
    "    evaluation_strategy=\"no\",\n",
    "    warmup_ratio=0.1,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    optim=\"adamw_8bit\",\n",
    "    weight_decay=0.01,\n",
    "    max_grad_norm=1.0,\n",
    "    fp16=not is_bfloat16_supported(),\n",
    "    bf16=is_bfloat16_supported(),\n",
    "    remove_unused_columns=False,\n",
    "    dataloader_num_workers=2,\n",
    "    seed=SEED,\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "print(f\"🎯 Training Configuration:\")\n",
    "print(f\"Output directory: {OUTPUT_DIR}\")\n",
    "print(f\"Learning rate: {LEARNING_RATE}\")\n",
    "print(f\"Epochs: {NUM_TRAIN_EPOCHS}\")\n",
    "print(f\"Batch size: {PER_DEVICE_TRAIN_BATCH_SIZE}\")\n",
    "print(f\"Gradient accumulation: {GRADIENT_ACCUMULATION_STEPS}\")\n",
    "print(f\"Max length: {MAX_LENGTH}\")\n",
    "print(f\"Max prompt length: {MAX_PROMPT_LENGTH}\")\n",
    "print(f\"Beta (KL penalty): {BETA}\")\n",
    "print(f\"Using {'bfloat16' if is_bfloat16_supported() else 'float16'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baae0b00",
   "metadata": {},
   "source": [
    "## Initialize GRPO Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f1fd79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize GRPO trainer\n",
    "grpo_trainer = GRPOTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=formatted_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=MAX_LENGTH,\n",
    "    max_prompt_length=MAX_PROMPT_LENGTH,\n",
    "    beta=BETA,\n",
    "    formatting_func=formatting_prompts_func,\n",
    ")\n",
    "\n",
    "print(\"✅ Unsloth GRPO Trainer initialized successfully!\")\n",
    "print(f\"Model class: {model.__class__.__name__}\")\n",
    "print(f\"Trainer class: {grpo_trainer.__class__.__name__}\")\n",
    "print(f\"Training samples: {len(formatted_dataset):,}\")\n",
    "print(f\"Effective batch size: {PER_DEVICE_TRAIN_BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43996a0c",
   "metadata": {},
   "source": [
    "## Start GRPO Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8cc8a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training\n",
    "print(\"🚀 Starting Unsloth GRPO Training for Investment Advisory...\")\n",
    "print(f\"Training {NUM_TRAIN_EPOCHS} epoch(s) on {len(formatted_dataset):,} samples\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Record start time\n",
    "start_time = datetime.now()\n",
    "print(f\"Training started at: {start_time.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "# Train the model\n",
    "trainer_stats = grpo_trainer.train()\n",
    "\n",
    "# Record end time\n",
    "end_time = datetime.now()\n",
    "training_duration = end_time - start_time\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"🎉 GRPO Training Completed!\")\n",
    "print(f\"Training finished at: {end_time.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"Total training time: {training_duration}\")\n",
    "print(f\"Final training loss: {trainer_stats.training_loss:.4f}\")\n",
    "print(f\"Training steps: {trainer_stats.global_step}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efcd9431",
   "metadata": {},
   "source": [
    "## Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58455130",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the final model\n",
    "print(\"💾 Saving trained model...\")\n",
    "\n",
    "# Save model and tokenizer\n",
    "model.save_pretrained(f\"{OUTPUT_DIR}/final_model\")\n",
    "tokenizer.save_pretrained(f\"{OUTPUT_DIR}/final_model\")\n",
    "\n",
    "print(f\"✅ Model saved to: {OUTPUT_DIR}/final_model\")\n",
    "\n",
    "# Save training summary\n",
    "training_summary = {\n",
    "    \"model_name\": MODEL_NAME,\n",
    "    \"dataset\": DATASET_NAME,\n",
    "    \"training_method\": \"Unsloth GRPO for Investment Advisory\",\n",
    "    \"total_samples\": len(formatted_dataset),\n",
    "    \"training_config\": {\n",
    "        \"epochs\": NUM_TRAIN_EPOCHS,\n",
    "        \"learning_rate\": LEARNING_RATE,\n",
    "        \"batch_size\": PER_DEVICE_TRAIN_BATCH_SIZE,\n",
    "        \"gradient_accumulation_steps\": GRADIENT_ACCUMULATION_STEPS,\n",
    "        \"max_length\": MAX_LENGTH,\n",
    "        \"max_prompt_length\": MAX_PROMPT_LENGTH,\n",
    "        \"beta\": BETA,\n",
    "    },\n",
    "    \"training_results\": {\n",
    "        \"final_loss\": trainer_stats.training_loss,\n",
    "        \"total_steps\": trainer_stats.global_step,\n",
    "        \"training_duration\": str(training_duration),\n",
    "    },\n",
    "    \"timestamps\": {\n",
    "        \"start_time\": start_time.isoformat(),\n",
    "        \"end_time\": end_time.isoformat(),\n",
    "    },\n",
    "    \"model_path\": f\"{OUTPUT_DIR}/final_model\",\n",
    "}\n",
    "\n",
    "# Save summary\n",
    "with open(f\"{OUTPUT_DIR}/training_summary.json\", \"w\") as f:\n",
    "    json.dump(training_summary, f, indent=2)\n",
    "\n",
    "print(f\"Training summary saved to: {OUTPUT_DIR}/training_summary.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa57d6eb",
   "metadata": {},
   "source": [
    "## Test Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa11d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the trained model\n",
    "print(\"🧪 Testing the trained GRPO model...\")\n",
    "\n",
    "# Prepare model for inference\n",
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "# Test sample for investment advisory\n",
    "test_messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a professional Bitcoin investment advisor. Provide strategic investment advice based on market analysis.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Given the current market conditions with Bitcoin at $45,000 and recent institutional adoption trends, what would be your investment recommendation for a moderate risk tolerance portfolio?\"}\n",
    "]\n",
    "\n",
    "# Format with chat template\n",
    "test_prompt = tokenizer.apply_chat_template(\n",
    "    test_messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "\n",
    "print(\"Test prompt:\")\n",
    "print(test_prompt)\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "\n",
    "# Generate response\n",
    "inputs = tokenizer(test_prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=512,\n",
    "        temperature=0.7,\n",
    "        do_sample=True,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "\n",
    "# Decode response\n",
    "response = tokenizer.decode(\n",
    "    outputs[0][len(inputs.input_ids[0]):],\n",
    "    skip_special_tokens=True\n",
    ")\n",
    "\n",
    "print(\"Model Response:\")\n",
    "print(response)\n",
    "print(\"\\n✅ Model testing completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98bdc753",
   "metadata": {},
   "source": [
    "## Training Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b822c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"📊 Unsloth GRPO Training Summary - Bitcoin Investment Advisory\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"🤖 Model: {MODEL_NAME}\")\n",
    "print(f\"📚 Dataset: {DATASET_NAME}\")\n",
    "print(f\"📈 Training method: Unsloth GRPO for Investment Advisory\")\n",
    "print(f\"📝 Total samples: {len(formatted_dataset):,}\")\n",
    "print()\n",
    "print(\"🎯 Training Configuration:\")\n",
    "print(f\"  • Epochs: {NUM_TRAIN_EPOCHS}\")\n",
    "print(f\"  • Learning rate: {LEARNING_RATE}\")\n",
    "print(f\"  • Batch size: {PER_DEVICE_TRAIN_BATCH_SIZE}\")\n",
    "print(f\"  • Gradient accumulation: {GRADIENT_ACCUMULATION_STEPS}\")\n",
    "print(f\"  • Effective batch size: {PER_DEVICE_TRAIN_BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS}\")\n",
    "print(f\"  • Max length: {MAX_LENGTH}\")\n",
    "print(f\"  • Max prompt length: {MAX_PROMPT_LENGTH}\")\n",
    "print(f\"  • Beta (KL penalty): {BETA}\")\n",
    "print(f\"  • LoRA rank: {LORA_R}\")\n",
    "print()\n",
    "print(\"📊 Training Results:\")\n",
    "print(f\"  • Final loss: {trainer_stats.training_loss:.4f}\")\n",
    "print(f\"  • Training steps: {trainer_stats.global_step:,}\")\n",
    "print(f\"  • Training duration: {training_duration}\")\n",
    "print()\n",
    "print(\"💾 Outputs:\")\n",
    "print(f\"  • Model saved to: {OUTPUT_DIR}/final_model\")\n",
    "print(f\"  • Summary saved to: {OUTPUT_DIR}/training_summary.json\")\n",
    "print()\n",
    "print(\"🔬 Key Features for Investment Advisory:\")\n",
    "print(\"  ✅ Unsloth-optimized GRPO training\")\n",
    "print(\"  ✅ Bitcoin investment strategy generation\")\n",
    "print(\"  ✅ Risk assessment and management\")\n",
    "print(\"  ✅ Portfolio optimization advice\")\n",
    "print(\"  ✅ Market analysis and insights\")\n",
    "print(\"  ✅ Preference learning for advice quality\")\n",
    "print(\"  ✅ Memory-efficient training\")\n",
    "print()\n",
    "print(\"🎉 Unsloth GRPO training completed successfully!\")\n",
    "print(\"💰 Model ready for Bitcoin investment advisory tasks!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
