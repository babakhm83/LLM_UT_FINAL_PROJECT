{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81d891e0",
   "metadata": {},
   "source": [
    "# Unsloth GRPO Training for Bitcoin Investment Advisory\n",
    "\n",
    "This notebook implements Group Relative Policy Optimization (GRPO) using Unsloth for Bitcoin investment advisory.\n",
    "\n",
    "**Dataset**: `bitcoin-investment-advisory-dataset`\n",
    "\n",
    "**Training Method**: Unsloth GRPO\n",
    "- Built-in preference learning optimization\n",
    "- Efficient memory usage with Unsloth\n",
    "- Streamlined training pipeline for investment advisory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9bd0a7c",
   "metadata": {},
   "source": [
    "## Install Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a24972",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
    "# !pip install -U xformers trl peft accelerate bitsandbytes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c75cace",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20420636",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel, is_bfloat16_supported\n",
    "from unsloth.chat_templates import get_chat_template\n",
    "from trl import GRPOTrainer, GRPOConfig\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import TrainingArguments, AutoTokenizer, AutoModelForSequenceClassification\n",
    "from peft import PeftModel\n",
    "import torch, random, os\n",
    "import json\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f1e8dbe",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ebcdb01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model configuration\n",
    "BASE_MODEL_NAME = \"unsloth/Qwen2.5-7B-Instruct-bnb-4bit\"  # Base model\n",
    "ADAPTER_PATH = \"tahamajs/my-awesome-model_final_for_explanation_end_summerized\"  # Pre-trained adapter\n",
    "CHECKPOINT = \"checkpoint-800\"  # Specific checkpoint\n",
    "MAX_SEQ_LENGTH = 2048\n",
    "DTYPE = None  # Auto-detection\n",
    "LOAD_IN_4BIT = True\n",
    "\n",
    "# LoRA configuration\n",
    "LORA_R = 16\n",
    "LORA_ALPHA = 16\n",
    "LORA_DROPOUT = 0.0\n",
    "TARGET_MODULES = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"]\n",
    "\n",
    "# GRPO configuration\n",
    "OUTPUT_DIR = \"./qwen_bitcoin_advisory_grpo_unsloth_pretrained\"\n",
    "LEARNING_RATE = 3e-7  # Lower for pre-trained model\n",
    "NUM_TRAIN_EPOCHS = 1\n",
    "PER_DEVICE_TRAIN_BATCH_SIZE = 1\n",
    "GRADIENT_ACCUMULATION_STEPS = 8\n",
    "MAX_LENGTH = 1024\n",
    "MAX_PROMPT_LENGTH = 512\n",
    "BETA = 0.1\n",
    "\n",
    "# Dataset\n",
    "DATASET_NAME = \"tahamajs/bitcoin-investment-advisory-dataset\"\n",
    "\n",
    "# Reward model for financial advisory\n",
    "REWARD_MODEL_NAME = \"ProsusAI/finbert\"  # Specialized for financial text analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "942b48a5",
   "metadata": {},
   "source": [
    "## Load Model and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c3f2a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load base model\n",
    "print(f\"ðŸ”„ Loading base model: {BASE_MODEL_NAME}\")\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=BASE_MODEL_NAME,\n",
    "    max_seq_length=MAX_SEQ_LENGTH,\n",
    "    dtype=DTYPE,\n",
    "    load_in_4bit=LOAD_IN_4BIT,\n",
    ")\n",
    "\n",
    "# Load pre-trained adapter\n",
    "print(f\"ðŸ”„ Loading pre-trained adapter: {ADAPTER_PATH}/{CHECKPOINT}\")\n",
    "try:\n",
    "    adapter_path = f\"{ADAPTER_PATH}/{CHECKPOINT}\"\n",
    "    model = PeftModel.from_pretrained(model, adapter_path)\n",
    "    print(f\"âœ… Successfully loaded adapter from {adapter_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸ Could not load adapter, using base model: {e}\")\n",
    "\n",
    "# Apply chat template\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template=\"chatml\",  # Supports Qwen models\n",
    ")\n",
    "\n",
    "# Prepare model for PEFT training\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=LORA_R,\n",
    "    target_modules=TARGET_MODULES,\n",
    "    lora_alpha=LORA_ALPHA,\n",
    "    lora_dropout=LORA_DROPOUT,\n",
    "    bias=\"none\",\n",
    "    use_gradient_checkpointing=\"unsloth\",\n",
    "    random_state=SEED,\n",
    "    use_rslora=False,\n",
    "    loftq_config=None,\n",
    ")\n",
    "\n",
    "# Load reward model for financial advisory\n",
    "print(f\"\\nðŸ”„ Loading reward model: {REWARD_MODEL_NAME}\")\n",
    "try:\n",
    "    reward_tokenizer = AutoTokenizer.from_pretrained(REWARD_MODEL_NAME)\n",
    "    reward_model = AutoModelForSequenceClassification.from_pretrained(REWARD_MODEL_NAME)\n",
    "    reward_model.eval()\n",
    "    print(f\"âœ… Reward model loaded successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸ Could not load reward model, using rule-based rewards: {e}\")\n",
    "    reward_model = None\n",
    "    reward_tokenizer = None\n",
    "\n",
    "print(f\"\\nðŸ“Š Model Configuration:\")\n",
    "print(f\"  Base model: {BASE_MODEL_NAME}\")\n",
    "print(f\"  Pre-trained adapter: {ADAPTER_PATH}/{CHECKPOINT}\")\n",
    "print(f\"  Max sequence length: {MAX_SEQ_LENGTH}\")\n",
    "print(f\"  LoRA rank: {LORA_R}\")\n",
    "print(f\"  Load in 4bit: {LOAD_IN_4BIT}\")\n",
    "print(f\"  Reward model: {REWARD_MODEL_NAME if reward_model else 'Rule-based only'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1684106c",
   "metadata": {},
   "source": [
    "## Load and Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b02491",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "dataset = load_dataset(DATASET_NAME, split=\"train\")\n",
    "print(f\"Dataset loaded: {DATASET_NAME}\")\n",
    "print(f\"Total samples: {len(dataset):,}\")\n",
    "\n",
    "# Show sample\n",
    "print(\"\\n=== Sample Data ===\")\n",
    "sample = dataset[0]\n",
    "for key, value in sample.items():\n",
    "    print(f\"{key}: {str(value)[:150]}{'...' if len(str(value)) > 150 else ''}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81425f1d",
   "metadata": {},
   "source": [
    "## Format Dataset for GRPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de854c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatting_prompts_func(examples):\n",
    "    \"\"\"\n",
    "    Format examples for Unsloth GRPO training.\n",
    "    Creates conversational format with system, user, and assistant messages.\n",
    "    \"\"\"\n",
    "    instructions = examples.get(\"instruction\", [\"\"] * len(examples.get(\"input\", [])))\n",
    "    inputs = examples.get(\"input\", [])\n",
    "    outputs = examples.get(\"output\", [])\n",
    "    \n",
    "    conversations = []\n",
    "    for instruction, user_input, output in zip(instructions, inputs, outputs):\n",
    "        conversation = [\n",
    "            {\"role\": \"system\", \"content\": instruction or \"You are a professional Bitcoin investment advisor.\"},\n",
    "            {\"role\": \"user\", \"content\": user_input or \"\"},\n",
    "            {\"role\": \"assistant\", \"content\": output or \"\"},\n",
    "        ]\n",
    "        conversations.append(conversation)\n",
    "    \n",
    "    return {\"conversations\": conversations}\n",
    "\n",
    "# Format dataset\n",
    "formatted_dataset = dataset.map(\n",
    "    formatting_prompts_func,\n",
    "    batched=True,\n",
    "    remove_columns=dataset.column_names\n",
    ")\n",
    "\n",
    "print(f\"Formatted dataset: {len(formatted_dataset):,} samples\")\n",
    "\n",
    "# Show formatted sample\n",
    "print(\"\\n=== Formatted Sample ===\")\n",
    "sample_conv = formatted_dataset[0][\"conversations\"]\n",
    "for msg in sample_conv:\n",
    "    print(f\"**{msg['role'].upper()}**: {msg['content'][:200]}{'...' if len(msg['content']) > 200 else ''}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e7b108",
   "metadata": {},
   "outputs": [],
   "source": [
    "training# Enhanced Reward Function for Bitcoin Investment Advisory\n",
    "def calculate_investment_advisory_reward(response, ground_truth, reward_model=None, reward_tokenizer=None):\n",
    "    \"\"\"\n",
    "    Comprehensive reward function for Bitcoin investment advisory responses.\n",
    "    Combines multiple scoring mechanisms for robust evaluation.\n",
    "    \"\"\"\n",
    "    total_reward = 0.0\n",
    "    \n",
    "    # 1. Rule-based Investment Quality Assessment (40% weight)\n",
    "    investment_reward = 0.0\n",
    "    \n",
    "    # Length appropriateness (investment advice should be detailed but not too verbose)\n",
    "    response_len = len(response)\n",
    "    if 100 <= response_len <= 800:\n",
    "        investment_reward += 0.15\n",
    "    elif 50 <= response_len <= 1200:\n",
    "        investment_reward += 0.10\n",
    "    elif response_len < 50:\n",
    "        investment_reward -= 0.10  # Too short for investment advice\n",
    "    \n",
    "    # Investment-specific keywords and concepts\n",
    "    investment_keywords = {\n",
    "        'strategy': 0.08, 'portfolio': 0.08, 'risk': 0.10, 'diversification': 0.05,\n",
    "        'bitcoin': 0.05, 'cryptocurrency': 0.05, 'investment': 0.08, 'allocation': 0.06,\n",
    "        'market': 0.05, 'analysis': 0.06, 'recommendation': 0.08, 'advice': 0.05,\n",
    "        'hodl': 0.04, 'dca': 0.06, 'volatility': 0.05, 'return': 0.05\n",
    "    }\n",
    "    \n",
    "    response_lower = response.lower()\n",
    "    for keyword, weight in investment_keywords.items():\n",
    "        if keyword in response_lower:\n",
    "            investment_reward += weight\n",
    "    \n",
    "    # Risk management indicators (crucial for investment advice)\n",
    "    risk_indicators = ['risk tolerance', 'risk management', 'stop loss', 'risk assessment', \n",
    "                      'conservative', 'aggressive', 'moderate risk', 'diversify', 'hedge']\n",
    "    risk_score = sum(0.05 for indicator in risk_indicators if indicator in response_lower)\n",
    "    investment_reward += min(0.20, risk_score)\n",
    "    \n",
    "    # Financial metrics and numbers (good investment advice includes specifics)\n",
    "    import re\n",
    "    percentage_matches = re.findall(r'\\d+\\.?\\d*%', response)\n",
    "    dollar_matches = re.findall(r'\\$\\d+', response)\n",
    "    if percentage_matches or dollar_matches:\n",
    "        investment_reward += 0.10\n",
    "    \n",
    "    # Investment timeframe mentions\n",
    "    timeframes = ['short-term', 'long-term', 'monthly', 'yearly', 'quarterly', 'daily', \n",
    "                 'weeks', 'months', 'years']\n",
    "    if any(timeframe in response_lower for timeframe in timeframes):\n",
    "        investment_reward += 0.08\n",
    "    \n",
    "    # Structured advice format\n",
    "    if any(marker in response for marker in ['1.', '2.', 'â€¢', '-', 'Step']):\n",
    "        investment_reward += 0.10\n",
    "    \n",
    "    total_reward += investment_reward * 0.4\n",
    "    \n",
    "    # 2. Sentiment and Tone Analysis (20% weight)\n",
    "    sentiment_reward = 0.0\n",
    "    \n",
    "    # Professional tone indicators\n",
    "    professional_phrases = ['recommend', 'suggest', 'consider', 'analysis shows', 'based on',\n",
    "                          'in my opinion', 'professional advice', 'financial guidance']\n",
    "    professional_score = sum(0.03 for phrase in professional_phrases if phrase in response_lower)\n",
    "    sentiment_reward += min(0.15, professional_score)\n",
    "    \n",
    "    # Balanced perspective (good investment advice shows multiple viewpoints)\n",
    "    balance_indicators = ['however', 'on the other hand', 'alternatively', 'but', 'although',\n",
    "                         'pros and cons', 'advantages', 'disadvantages', 'benefits', 'risks']\n",
    "    balance_score = sum(0.02 for indicator in balance_indicators if indicator in response_lower)\n",
    "    sentiment_reward += min(0.10, balance_score)\n",
    "    \n",
    "    total_reward += sentiment_reward * 0.2\n",
    "    \n",
    "    # 3. Content Similarity with Ground Truth (25% weight)\n",
    "    similarity_reward = 0.0\n",
    "    \n",
    "    # Token overlap\n",
    "    response_tokens = set(response.lower().split())\n",
    "    gt_tokens = set(ground_truth.lower().split())\n",
    "    \n",
    "    if len(gt_tokens) > 0:\n",
    "        jaccard_similarity = len(response_tokens & gt_tokens) / len(response_tokens | gt_tokens)\n",
    "        similarity_reward += jaccard_similarity * 0.6\n",
    "        \n",
    "        # Key concept alignment\n",
    "        key_concepts = ['buy', 'sell', 'hold', 'accumulate', 'wait', 'caution', 'bullish', 'bearish']\n",
    "        concept_matches = sum(1 for concept in key_concepts \n",
    "                            if concept in response_lower and concept in ground_truth.lower())\n",
    "        if concept_matches > 0:\n",
    "            similarity_reward += min(0.4, concept_matches * 0.1)\n",
    "    \n",
    "    total_reward += similarity_reward * 0.25\n",
    "    \n",
    "    # 4. AI Reward Model Assessment (15% weight)\n",
    "    if reward_model is not None and reward_tokenizer is not None:\n",
    "        try:\n",
    "            # Format input for financial sentiment analysis\n",
    "            model_input = f\"Investment Advice: {response[:400]}\"  # Limit length for model\n",
    "            \n",
    "            inputs = reward_tokenizer(\n",
    "                model_input, \n",
    "                return_tensors=\"pt\", \n",
    "                truncation=True, \n",
    "                max_length=512,\n",
    "                padding=True\n",
    "            )\n",
    "            \n",
    "            if torch.cuda.is_available():\n",
    "                inputs = {k: v.cuda() for k, v in inputs.items()}\n",
    "                reward_model = reward_model.cuda()\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = reward_model(**inputs)\n",
    "                # For FinBERT, we want positive financial sentiment\n",
    "                logits = outputs.logits\n",
    "                probabilities = torch.softmax(logits, dim=-1)\n",
    "                \n",
    "                # FinBERT typically has [negative, neutral, positive] classes\n",
    "                if probabilities.shape[-1] == 3:\n",
    "                    positive_score = probabilities[0, 2].item()  # Positive class\n",
    "                    neutral_score = probabilities[0, 1].item()   # Neutral class\n",
    "                    # Weight positive sentiment higher for investment advice\n",
    "                    ai_reward = positive_score * 0.8 + neutral_score * 0.2\n",
    "                else:\n",
    "                    ai_reward = torch.sigmoid(logits).mean().item()\n",
    "                \n",
    "                total_reward += ai_reward * 0.15\n",
    "                \n",
    "        except Exception as e:\n",
    "            # Fallback: use rule-based scoring only\n",
    "            print(f\"âš ï¸ Reward model error: {e}\")\n",
    "            pass\n",
    "    \n",
    "    # 5. Penalty for Poor Investment Advice (-0.5 to 0 weight)\n",
    "    penalties = 0.0\n",
    "    \n",
    "    # Penalty for overly speculative language\n",
    "    speculative_words = ['guaranteed', 'definitely will', 'certain profit', 'no risk', 'sure thing']\n",
    "    speculation_penalty = sum(0.1 for word in speculative_words if word in response_lower)\n",
    "    penalties -= min(0.3, speculation_penalty)\n",
    "    \n",
    "    # Penalty for financial advice disclaimers absence (good practice to include them)\n",
    "    disclaimer_phrases = ['not financial advice', 'dyor', 'do your own research', 'consult', 'disclaimer']\n",
    "    if not any(phrase in response_lower for phrase in disclaimer_phrases) and len(response) > 200:\n",
    "        penalties -= 0.05\n",
    "    \n",
    "    total_reward += penalties\n",
    "    \n",
    "    # Ensure reward is in reasonable range [0, 1]\n",
    "    total_reward = max(0.0, min(1.0, total_reward))\n",
    "    \n",
    "    return total_reward\n",
    "\n",
    "print(\"âœ… Enhanced investment advisory reward function created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e615716",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom GRPO Trainer with Enhanced Reward Integration\n",
    "class CustomGRPOTrainer(GRPOTrainer):\n",
    "    \"\"\"\n",
    "    Custom GRPO trainer that integrates our enhanced reward function\n",
    "    for Bitcoin investment advisory training.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, reward_model=None, reward_tokenizer=None, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.reward_model = reward_model\n",
    "        self.reward_tokenizer = reward_tokenizer\n",
    "        \n",
    "    def compute_rewards(self, queries, responses, ground_truths=None):\n",
    "        \"\"\"\n",
    "        Compute rewards using our enhanced investment advisory reward function.\n",
    "        \"\"\"\n",
    "        rewards = []\n",
    "        \n",
    "        for i, (query_responses) in enumerate(responses):\n",
    "            query_rewards = []\n",
    "            ground_truth = ground_truths[i] if ground_truths and i < len(ground_truths) else \"\"\n",
    "            \n",
    "            for response in query_responses:\n",
    "                # Use our enhanced reward function\n",
    "                reward = calculate_investment_advisory_reward(\n",
    "                    response=response,\n",
    "                    ground_truth=ground_truth,\n",
    "                    reward_model=self.reward_model,\n",
    "                    reward_tokenizer=self.reward_tokenizer\n",
    "                )\n",
    "                query_rewards.append(reward)\n",
    "            \n",
    "            rewards.append(query_rewards)\n",
    "        \n",
    "        return rewards\n",
    "    \n",
    "    def generate_completions(self, queries, num_completions=4):\n",
    "        \"\"\"\n",
    "        Generate multiple completions for each query using the model.\n",
    "        \"\"\"\n",
    "        all_completions = []\n",
    "        \n",
    "        for query in queries:\n",
    "            completions = []\n",
    "            \n",
    "            # Tokenize the query\n",
    "            inputs = self.tokenizer(\n",
    "                query, \n",
    "                return_tensors=\"pt\", \n",
    "                truncation=True, \n",
    "                max_length=self.max_prompt_length\n",
    "            ).to(self.model.device)\n",
    "            \n",
    "            for _ in range(num_completions):\n",
    "                with torch.no_grad():\n",
    "                    outputs = self.model.generate(\n",
    "                        **inputs,\n",
    "                        max_new_tokens=min(512, self.max_length - inputs['input_ids'].shape[1]),\n",
    "                        temperature=0.8,\n",
    "                        do_sample=True,\n",
    "                        pad_token_id=self.tokenizer.eos_token_id,\n",
    "                        eos_token_id=self.tokenizer.eos_token_id,\n",
    "                        repetition_penalty=1.1,\n",
    "                    )\n",
    "                \n",
    "                # Decode the generated response\n",
    "                generated_ids = outputs[0][inputs['input_ids'].shape[1]:]\n",
    "                response = self.tokenizer.decode(generated_ids, skip_special_tokens=True).strip()\n",
    "                completions.append(response)\n",
    "            \n",
    "            all_completions.append(completions)\n",
    "        \n",
    "        return all_completions\n",
    "\n",
    "print(\"âœ… Custom GRPO trainer with enhanced rewards created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b84258a9",
   "metadata": {},
   "source": [
    "## Setup GRPO Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "091d1cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    num_train_epochs=NUM_TRAIN_EPOCHS,\n",
    "    per_device_train_batch_size=PER_DEVICE_TRAIN_BATCH_SIZE,\n",
    "    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    logging_steps=10,\n",
    "    save_steps=100,\n",
    "    save_strategy=\"steps\",\n",
    "    evaluation_strategy=\"no\",\n",
    "    warmup_ratio=0.1,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    optim=\"adamw_8bit\",\n",
    "    weight_decay=0.01,\n",
    "    max_grad_norm=1.0,\n",
    "    fp16=not is_bfloat16_supported(),\n",
    "    bf16=is_bfloat16_supported(),\n",
    "    remove_unused_columns=False,\n",
    "    dataloader_num_workers=2,\n",
    "    seed=SEED,\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "print(f\"ðŸŽ¯ Training Configuration:\")\n",
    "print(f\"Output directory: {OUTPUT_DIR}\")\n",
    "print(f\"Learning rate: {LEARNING_RATE}\")\n",
    "print(f\"Epochs: {NUM_TRAIN_EPOCHS}\")\n",
    "print(f\"Batch size: {PER_DEVICE_TRAIN_BATCH_SIZE}\")\n",
    "print(f\"Gradient accumulation: {GRADIENT_ACCUMULATION_STEPS}\")\n",
    "print(f\"Max length: {MAX_LENGTH}\")\n",
    "print(f\"Max prompt length: {MAX_PROMPT_LENGTH}\")\n",
    "print(f\"Beta (KL penalty): {BETA}\")\n",
    "print(f\"Using {'bfloat16' if is_bfloat16_supported() else 'float16'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baae0b00",
   "metadata": {},
   "source": [
    "## Initialize GRPO Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f1fd79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize GRPO trainer\n",
    "grpo_trainer = GRPOTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=formatted_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=MAX_LENGTH,\n",
    "    max_prompt_length=MAX_PROMPT_LENGTH,\n",
    "    beta=BETA,\n",
    "    formatting_func=formatting_prompts_func,\n",
    ")\n",
    "\n",
    "print(\"âœ… Unsloth GRPO Trainer initialized successfully!\")\n",
    "print(f\"Model class: {model.__class__.__name__}\")\n",
    "print(f\"Trainer class: {grpo_trainer.__class__.__name__}\")\n",
    "print(f\"Training samples: {len(formatted_dataset):,}\")\n",
    "print(f\"Effective batch size: {PER_DEVICE_TRAIN_BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43996a0c",
   "metadata": {},
   "source": [
    "## Start GRPO Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8cc8a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training\n",
    "print(\"ðŸš€ Starting Unsloth GRPO Training for Investment Advisory...\")\n",
    "print(f\"Training {NUM_TRAIN_EPOCHS} epoch(s) on {len(formatted_dataset):,} samples\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Record start time\n",
    "start_time = datetime.now()\n",
    "print(f\"Training started at: {start_time.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "# Train the model\n",
    "trainer_stats = grpo_trainer.train()\n",
    "\n",
    "# Record end time\n",
    "end_time = datetime.now()\n",
    "training_duration = end_time - start_time\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ðŸŽ‰ GRPO Training Completed!\")\n",
    "print(f\"Training finished at: {end_time.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"Total training time: {training_duration}\")\n",
    "print(f\"Final training loss: {trainer_stats.training_loss:.4f}\")\n",
    "print(f\"Training steps: {trainer_stats.global_step}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efcd9431",
   "metadata": {},
   "source": [
    "## Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58455130",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the final model\n",
    "print(\"ðŸ’¾ Saving trained model...\")\n",
    "\n",
    "# Save model and tokenizer\n",
    "model.save_pretrained(f\"{OUTPUT_DIR}/final_model\")\n",
    "tokenizer.save_pretrained(f\"{OUTPUT_DIR}/final_model\")\n",
    "\n",
    "print(f\"âœ… Model saved to: {OUTPUT_DIR}/final_model\")\n",
    "\n",
    "# Save training summary\n",
    "training_summary = {\n",
    "    \"model_name\": MODEL_NAME,\n",
    "    \"dataset\": DATASET_NAME,\n",
    "    \"training_method\": \"Unsloth GRPO for Investment Advisory\",\n",
    "    \"total_samples\": len(formatted_dataset),\n",
    "    \"training_config\": {\n",
    "        \"epochs\": NUM_TRAIN_EPOCHS,\n",
    "        \"learning_rate\": LEARNING_RATE,\n",
    "        \"batch_size\": PER_DEVICE_TRAIN_BATCH_SIZE,\n",
    "        \"gradient_accumulation_steps\": GRADIENT_ACCUMULATION_STEPS,\n",
    "        \"max_length\": MAX_LENGTH,\n",
    "        \"max_prompt_length\": MAX_PROMPT_LENGTH,\n",
    "        \"beta\": BETA,\n",
    "    },\n",
    "    \"training_results\": {\n",
    "        \"final_loss\": trainer_stats.training_loss,\n",
    "        \"total_steps\": trainer_stats.global_step,\n",
    "        \"training_duration\": str(training_duration),\n",
    "    },\n",
    "    \"timestamps\": {\n",
    "        \"start_time\": start_time.isoformat(),\n",
    "        \"end_time\": end_time.isoformat(),\n",
    "    },\n",
    "    \"model_path\": f\"{OUTPUT_DIR}/final_model\",\n",
    "}\n",
    "\n",
    "# Save summary\n",
    "with open(f\"{OUTPUT_DIR}/training_summary.json\", \"w\") as f:\n",
    "    json.dump(training_summary, f, indent=2)\n",
    "\n",
    "print(f\"Training summary saved to: {OUTPUT_DIR}/training_summary.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa57d6eb",
   "metadata": {},
   "source": [
    "## Test Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa11d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the trained model\n",
    "print(\"ðŸ§ª Testing the trained GRPO model...\")\n",
    "\n",
    "# Prepare model for inference\n",
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "# Test sample for investment advisory\n",
    "test_messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a professional Bitcoin investment advisor. Provide strategic investment advice based on market analysis.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Given the current market conditions with Bitcoin at $45,000 and recent institutional adoption trends, what would be your investment recommendation for a moderate risk tolerance portfolio?\"}\n",
    "]\n",
    "\n",
    "# Format with chat template\n",
    "test_prompt = tokenizer.apply_chat_template(\n",
    "    test_messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "\n",
    "print(\"Test prompt:\")\n",
    "print(test_prompt)\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "\n",
    "# Generate response\n",
    "inputs = tokenizer(test_prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=512,\n",
    "        temperature=0.7,\n",
    "        do_sample=True,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "\n",
    "# Decode response\n",
    "response = tokenizer.decode(\n",
    "    outputs[0][len(inputs.input_ids[0]):],\n",
    "    skip_special_tokens=True\n",
    ")\n",
    "\n",
    "print(\"Model Response:\")\n",
    "print(response)\n",
    "print(\"\\nâœ… Model testing completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98bdc753",
   "metadata": {},
   "source": [
    "## Training Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b822c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ðŸ“Š Unsloth GRPO Training Summary - Bitcoin Investment Advisory\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"ðŸ¤– Model: {MODEL_NAME}\")\n",
    "print(f\"ðŸ“š Dataset: {DATASET_NAME}\")\n",
    "print(f\"ðŸ“ˆ Training method: Unsloth GRPO for Investment Advisory\")\n",
    "print(f\"ðŸ“ Total samples: {len(formatted_dataset):,}\")\n",
    "print()\n",
    "print(\"ðŸŽ¯ Training Configuration:\")\n",
    "print(f\"  â€¢ Epochs: {NUM_TRAIN_EPOCHS}\")\n",
    "print(f\"  â€¢ Learning rate: {LEARNING_RATE}\")\n",
    "print(f\"  â€¢ Batch size: {PER_DEVICE_TRAIN_BATCH_SIZE}\")\n",
    "print(f\"  â€¢ Gradient accumulation: {GRADIENT_ACCUMULATION_STEPS}\")\n",
    "print(f\"  â€¢ Effective batch size: {PER_DEVICE_TRAIN_BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS}\")\n",
    "print(f\"  â€¢ Max length: {MAX_LENGTH}\")\n",
    "print(f\"  â€¢ Max prompt length: {MAX_PROMPT_LENGTH}\")\n",
    "print(f\"  â€¢ Beta (KL penalty): {BETA}\")\n",
    "print(f\"  â€¢ LoRA rank: {LORA_R}\")\n",
    "print()\n",
    "print(\"ðŸ“Š Training Results:\")\n",
    "print(f\"  â€¢ Final loss: {trainer_stats.training_loss:.4f}\")\n",
    "print(f\"  â€¢ Training steps: {trainer_stats.global_step:,}\")\n",
    "print(f\"  â€¢ Training duration: {training_duration}\")\n",
    "print()\n",
    "print(\"ðŸ’¾ Outputs:\")\n",
    "print(f\"  â€¢ Model saved to: {OUTPUT_DIR}/final_model\")\n",
    "print(f\"  â€¢ Summary saved to: {OUTPUT_DIR}/training_summary.json\")\n",
    "print()\n",
    "print(\"ðŸ”¬ Key Features for Investment Advisory:\")\n",
    "print(\"  âœ… Unsloth-optimized GRPO training\")\n",
    "print(\"  âœ… Bitcoin investment strategy generation\")\n",
    "print(\"  âœ… Risk assessment and management\")\n",
    "print(\"  âœ… Portfolio optimization advice\")\n",
    "print(\"  âœ… Market analysis and insights\")\n",
    "print(\"  âœ… Preference learning for advice quality\")\n",
    "print(\"  âœ… Memory-efficient training\")\n",
    "print()\n",
    "print(\"ðŸŽ‰ Unsloth GRPO training completed successfully!\")\n",
    "print(\"ðŸ’° Model ready for Bitcoin investment advisory tasks!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
