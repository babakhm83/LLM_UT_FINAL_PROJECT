{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce224773",
   "metadata": {},
   "source": [
    "# Enhanced Bitcoin Price Prediction Training: SFT + GRPO\n",
    "\n",
    "This notebook combines Supervised Fine-Tuning (SFT) with Group Relative Policy Optimization (GRPO) for enhanced Bitcoin price prediction performance.\n",
    "\n",
    "**Dataset**: `bitcoin-enhanced-prediction-dataset-with-local-comprehensive-news`\n",
    "\n",
    "**Training Pipeline**:\n",
    "1. Phase 1: Supervised Fine-Tuning (SFT)\n",
    "2. Phase 2: Group Relative Policy Optimization (GRPO)\n",
    "3. Model evaluation and comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87cb4474",
   "metadata": {},
   "source": [
    "## Install Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de00799",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U unsloth\n",
    "# !pip install trl\n",
    "# !pip install accelerate\n",
    "# !pip install datasets\n",
    "# !pip install transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf58963",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d9a2a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from datasets import load_dataset, Dataset\n",
    "import torch, random, os\n",
    "from typing import Dict, List, Any\n",
    "from transformers.data.data_collator import DefaultDataCollator\n",
    "import json\n",
    "import numpy as np\n",
    "from trl import SFTTrainer, DataCollatorForCompletionOnlyLM\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb26275",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92557cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model and training configuration\n",
    "MODEL_CONFIG = {\n",
    "    \"model_path\": \"./Qwen3-8B\",\n",
    "    \"max_seq_length\": 4096,\n",
    "    \"dtype\": torch.float16,\n",
    "    \"load_in_4bit\": True,\n",
    "    \"lora_r\": 32,\n",
    "    \"lora_alpha\": 32,\n",
    "    \"lora_dropout\": 0.0,\n",
    "    \"target_modules\": [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"]\n",
    "}\n",
    "\n",
    "SFT_CONFIG = {\n",
    "    \"output_dir\": \"qwen_bitcoin_sft_enhanced\",\n",
    "    \"num_train_epochs\": 3,\n",
    "    \"per_device_train_batch_size\": 4,\n",
    "    \"gradient_accumulation_steps\": 2,\n",
    "    \"learning_rate\": 2e-4,\n",
    "    \"logging_steps\": 10,\n",
    "    \"save_steps\": 200,\n",
    "    \"warmup_ratio\": 0.05,\n",
    "}\n",
    "\n",
    "GRPO_CONFIG = {\n",
    "    \"output_dir\": \"qwen_bitcoin_grpo_enhanced\",\n",
    "    \"num_train_epochs\": 2,\n",
    "    \"per_device_train_batch_size\": 2,\n",
    "    \"gradient_accumulation_steps\": 4,\n",
    "    \"learning_rate\": 1e-5,\n",
    "    \"logging_steps\": 5,\n",
    "    \"save_steps\": 100,\n",
    "    \"warmup_ratio\": 0.1,\n",
    "    \"group_size\": 4,  # Number of responses to compare per group\n",
    "    \"temperature\": 0.7,  # For response generation\n",
    "}\n",
    "\n",
    "DATASET_NAME = \"tahamajs/bitcoin-enhanced-prediction-dataset-with-local-comprehensive-news\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "265e039f",
   "metadata": {},
   "source": [
    "## Load Model and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d192e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    MODEL_CONFIG[\"model_path\"],\n",
    "    max_seq_length=MODEL_CONFIG[\"max_seq_length\"],\n",
    "    dtype=MODEL_CONFIG[\"dtype\"],\n",
    "    load_in_4bit=MODEL_CONFIG[\"load_in_4bit\"],\n",
    ")\n",
    "\n",
    "# Prepare model for training\n",
    "FastLanguageModel.for_training(model)\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=MODEL_CONFIG[\"lora_r\"],\n",
    "    target_modules=MODEL_CONFIG[\"target_modules\"],\n",
    "    lora_alpha=MODEL_CONFIG[\"lora_alpha\"],\n",
    "    lora_dropout=MODEL_CONFIG[\"lora_dropout\"],\n",
    "    use_rslora=True,\n",
    ")\n",
    "\n",
    "print(f\"Model loaded: {MODEL_CONFIG['model_path']}\")\n",
    "print(f\"Total parameters: {model.num_parameters():,}\")\n",
    "print(f\"Trainable parameters: {model.num_parameters(only_trainable=True):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf99d03",
   "metadata": {},
   "source": [
    "## Special Tokens Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e6a9508",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define special tokens\n",
    "SPECIAL_TOKENS = [\"<|response|>\", \"<|analysis|>\", \"<|forecast|>\", \"<|thinking|>\"]\n",
    "num_added = tokenizer.add_special_tokens({\"additional_special_tokens\": SPECIAL_TOKENS})\n",
    "if num_added > 0:\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "    print(f\"Added {num_added} special tokens\")\n",
    "\n",
    "RESPONSE_TAG = \"<|response|>\"\n",
    "ANALYSIS_TAG = \"<|analysis|>\"\n",
    "FORECAST_TAG = \"<|forecast|>\"\n",
    "THINKING_TAG = \"<|thinking|>\"\n",
    "\n",
    "response_token_id = tokenizer.convert_tokens_to_ids(RESPONSE_TAG)\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.truncation_side = \"left\"\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "print(f\"Special tokens: {SPECIAL_TOKENS}\")\n",
    "print(f\"Response token ID: {response_token_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d79af2",
   "metadata": {},
   "source": [
    "## Data Processing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada983ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_output_to_commas(output: str) -> str:\n",
    "    \"\"\"Normalize output to comma-separated format\"\"\"\n",
    "    txt = str(output).strip()\n",
    "    if txt.startswith(\"[\") and txt.endswith(\"]\"):\n",
    "        try:\n",
    "            arr = json.loads(txt)\n",
    "            return \",\".join(str(x).strip() for x in arr)\n",
    "        except Exception:\n",
    "            pass\n",
    "    return \",\".join([t.strip() for t in txt.split(\",\")])\n",
    "\n",
    "def make_brief_analysis(thinking: str, limit_chars: int = 200) -> str:\n",
    "    \"\"\"Create brief analysis from thinking\"\"\"\n",
    "    t = (thinking or \"\").strip()\n",
    "    if not t:\n",
    "        return \"brief outlook based on the provided data\"\n",
    "    return t[:limit_chars].replace(\"\\n\", \" \")\n",
    "\n",
    "def format_chat_sft(ex):\n",
    "    \"\"\"Format example for SFT training\"\"\"\n",
    "    instruction = ex.get(\"instruction\", \"\") or \"\"\n",
    "    user_input = ex.get(\"input\", \"\") or \"\"\n",
    "    output = ex.get(\"output\", \"\") or \"\"\n",
    "    \n",
    "    assistant_payload = (\n",
    "        f\"{RESPONSE_TAG}\\n\"\n",
    "        f\"{FORECAST_TAG}\\n{output}\"\n",
    "    )\n",
    "    \n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": instruction},\n",
    "        {\"role\": \"user\", \"content\": user_input},\n",
    "        {\"role\": \"assistant\", \"content\": assistant_payload},\n",
    "    ]\n",
    "    return {\"text\": tokenizer.apply_chat_template(messages, tokenize=False)}\n",
    "\n",
    "def format_chat_grpo(ex):\n",
    "    \"\"\"Format example for GRPO training (input only for generation)\"\"\"\n",
    "    instruction = ex.get(\"instruction\", \"\") or \"\"\n",
    "    user_input = ex.get(\"input\", \"\") or \"\"\n",
    "    \n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": instruction},\n",
    "        {\"role\": \"user\", \"content\": user_input},\n",
    "    ]\n",
    "    return {\n",
    "        \"query\": tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True),\n",
    "        \"ground_truth\": ex.get(\"output\", \"\")\n",
    "    }\n",
    "\n",
    "print(\"Data processing functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0556216f",
   "metadata": {},
   "source": [
    "## Load and Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91248bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "raw_dataset = load_dataset(DATASET_NAME)\n",
    "print(f\"Loaded dataset: {DATASET_NAME}\")\n",
    "print(f\"Dataset structure: {raw_dataset}\")\n",
    "\n",
    "# Split data for SFT and GRPO\n",
    "train_data = raw_dataset[\"train\"]\n",
    "print(f\"Total training samples: {len(train_data)}\")\n",
    "\n",
    "# Use 80% for SFT, 20% for GRPO\n",
    "sft_size = int(0.8 * len(train_data))\n",
    "sft_data = train_data.select(range(sft_size))\n",
    "grpo_data = train_data.select(range(sft_size, len(train_data)))\n",
    "\n",
    "print(f\"SFT training samples: {len(sft_data)}\")\n",
    "print(f\"GRPO training samples: {len(grpo_data)}\")\n",
    "\n",
    "# Show sample data\n",
    "print(\"\\n=== Sample Data ===\")\n",
    "sample = train_data[0]\n",
    "for key, value in sample.items():\n",
    "    print(f\"{key}: {str(value)[:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a5c18b",
   "metadata": {},
   "source": [
    "## Data Collator for SFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0afafa5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.tokenization_utils_base import PreTrainedTokenizerBase\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch\n",
    "from typing import Dict, List, Any\n",
    "\n",
    "def _find_subsequence(haystack: torch.Tensor, needle: torch.Tensor) -> int:\n",
    "    if needle.numel() == 0 or haystack.numel() < needle.numel():\n",
    "        return -1\n",
    "    for i in range(haystack.numel() - needle.numel() + 1):\n",
    "        if torch.equal(haystack[i:i+needle.numel()], needle):\n",
    "            return i\n",
    "    return -1\n",
    "\n",
    "class DataCollatorMaskResponse:\n",
    "    def __init__(self, tokenizer: PreTrainedTokenizerBase, response_token_id: int):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.response_token_id = response_token_id\n",
    "        \n",
    "        assistant_start_str = tokenizer.apply_chat_template(\n",
    "            [{\"role\":\"assistant\",\"content\":\"\"}],\n",
    "            tokenize=False, add_generation_prompt=True\n",
    "        )\n",
    "        \n",
    "        self.assistant_start_ids = torch.tensor(\n",
    "            tokenizer(assistant_start_str, add_special_tokens=False)[\"input_ids\"],\n",
    "            dtype=torch.long\n",
    "        )\n",
    "    \n",
    "    def __call__(self, features: List[Dict[str, Any]]) -> Dict[str, torch.Tensor]:\n",
    "        input_ids_list = [torch.tensor(f[\"input_ids\"], dtype=torch.long) for f in features]\n",
    "        attention_mask_list = [torch.tensor(f[\"attention_mask\"], dtype=torch.long) for f in features]\n",
    "        \n",
    "        input_ids = pad_sequence(input_ids_list, batch_first=True,\n",
    "                                 padding_value=self.tokenizer.pad_token_id)\n",
    "        attention_mask = pad_sequence(attention_mask_list, batch_first=True, padding_value=0)\n",
    "        \n",
    "        labels = input_ids.clone()\n",
    "        \n",
    "        for i in range(labels.size(0)):\n",
    "            row = input_ids[i]\n",
    "            \n",
    "            pos = (row == self.response_token_id).nonzero(as_tuple=True)\n",
    "            start_idx = -1\n",
    "            if len(pos[0]) > 0:\n",
    "                start_idx = int(pos[0][0].item())\n",
    "            \n",
    "            if start_idx < 0 and self.assistant_start_ids.numel() > 0:\n",
    "                j = _find_subsequence(row, self.assistant_start_ids)\n",
    "                if j >= 0:\n",
    "                    start_idx = j + self.assistant_start_ids.numel() - 1\n",
    "            \n",
    "            if start_idx >= 0 and start_idx + 1 < row.numel():\n",
    "                labels[i, : start_idx + 1] = -100\n",
    "            else:\n",
    "                keep = min(64, row.numel())\n",
    "                labels[i, : row.numel() - keep] = -100\n",
    "        \n",
    "        return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"labels\": labels,\n",
    "        }\n",
    "\n",
    "collator = DataCollatorMaskResponse(tokenizer, response_token_id)\n",
    "print(\"Data collator for SFT training ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5614aa00",
   "metadata": {},
   "source": [
    "# Phase 2: Group Relative Policy Optimization (GRPO)\n",
    "\n",
    "Now we'll implement GRPO to further improve the model by optimizing relative preferences between multiple responses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d59fa45",
   "metadata": {},
   "source": [
    "## GRPO Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50bfed9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "class GRPOTrainer:\n",
    "    def __init__(self, model, tokenizer, config):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.config = config\n",
    "        self.device = model.device\n",
    "        \n",
    "    def generate_responses(self, queries, num_responses=4, temperature=0.7, max_length=512):\n",
    "        \"\"\"\n",
    "        Generate multiple responses for each query\n",
    "        \"\"\"\n",
    "        self.model.eval()\n",
    "        responses = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for query in queries:\n",
    "                query_responses = []\n",
    "                inputs = self.tokenizer(query, return_tensors=\"pt\", truncation=True, max_length=2048)\n",
    "                inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "                \n",
    "                for _ in range(num_responses):\n",
    "                    outputs = self.model.generate(\n",
    "                        **inputs,\n",
    "                        max_length=inputs[\"input_ids\"].shape[1] + max_length,\n",
    "                        temperature=temperature,\n",
    "                        do_sample=True,\n",
    "                        pad_token_id=self.tokenizer.eos_token_id,\n",
    "                        eos_token_id=self.tokenizer.eos_token_id,\n",
    "                    )\n",
    "                    \n",
    "                    response = self.tokenizer.decode(\n",
    "                        outputs[0][inputs[\"input_ids\"].shape[1]:], \n",
    "                        skip_special_tokens=True\n",
    "                    )\n",
    "                    query_responses.append(response.strip())\n",
    "                \n",
    "                responses.append(query_responses)\n",
    "        \n",
    "        return responses\n",
    "    \n",
    "    def compute_rewards(self, queries, responses, ground_truths):\n",
    "        \"\"\"\n",
    "        Compute rewards for responses based on similarity to ground truth\n",
    "        \"\"\"\n",
    "        rewards = []\n",
    "        \n",
    "        for query_responses, gt in zip(responses, ground_truths):\n",
    "            query_rewards = []\n",
    "            \n",
    "            for response in query_responses:\n",
    "                # Simple reward based on response quality (can be enhanced)\n",
    "                reward = self._calculate_response_reward(response, gt)\n",
    "                query_rewards.append(reward)\n",
    "            \n",
    "            rewards.append(query_rewards)\n",
    "        \n",
    "        return rewards\n",
    "    \n",
    "    def _calculate_response_reward(self, response, ground_truth):\n",
    "        \"\"\"\n",
    "        Calculate reward for a response based on various criteria\n",
    "        \"\"\"\n",
    "        reward = 0.0\n",
    "        \n",
    "        # Length penalty/reward\n",
    "        if 50 <= len(response) <= 500:\n",
    "            reward += 0.1\n",
    "        \n",
    "        # Format check (contains numbers/predictions)\n",
    "        if any(char.isdigit() for char in response):\n",
    "            reward += 0.2\n",
    "        \n",
    "        # Contains forecast tag\n",
    "        if FORECAST_TAG in response:\n",
    "            reward += 0.3\n",
    "        \n",
    "        # Simple text similarity (can be enhanced with better metrics)\n",
    "        common_words = set(response.lower().split()) & set(ground_truth.lower().split())\n",
    "        if len(common_words) > 0:\n",
    "            reward += min(0.4, len(common_words) * 0.02)\n",
    "        \n",
    "        return reward\n",
    "    \n",
    "    def compute_grpo_loss(self, queries, responses, rewards):\n",
    "        \"\"\"\n",
    "        Compute GRPO loss based on relative preferences within groups\n",
    "        \"\"\"\n",
    "        self.model.train()\n",
    "        total_loss = 0.0\n",
    "        num_groups = 0\n",
    "        \n",
    "        for query, query_responses, query_rewards in zip(queries, responses, rewards):\n",
    "            if len(query_responses) < 2:\n",
    "                continue\n",
    "            \n",
    "            # Tokenize query and responses\n",
    "            query_tokens = self.tokenizer(query, return_tensors=\"pt\", truncation=True, max_length=2048)\n",
    "            query_tokens = {k: v.to(self.device) for k, v in query_tokens.items()}\n",
    "            \n",
    "            response_logprobs = []\n",
    "            \n",
    "            for response in query_responses:\n",
    "                # Create full text (query + response)\n",
    "                full_text = query + \" \" + response\n",
    "                tokens = self.tokenizer(full_text, return_tensors=\"pt\", truncation=True, max_length=4096)\n",
    "                tokens = {k: v.to(self.device) for k, v in tokens.items()}\n",
    "                \n",
    "                # Get model outputs\n",
    "                outputs = self.model(**tokens)\n",
    "                logits = outputs.logits\n",
    "                \n",
    "                # Calculate log probabilities for the response part\n",
    "                query_length = query_tokens[\"input_ids\"].shape[1]\n",
    "                response_logits = logits[0, query_length-1:-1]  # Shift for next token prediction\n",
    "                response_tokens = tokens[\"input_ids\"][0, query_length:]\n",
    "                \n",
    "                if response_tokens.numel() > 0:\n",
    "                    log_probs = F.log_softmax(response_logits, dim=-1)\n",
    "                    response_log_prob = log_probs.gather(1, response_tokens.unsqueeze(-1)).squeeze(-1)\n",
    "                    avg_log_prob = response_log_prob.mean()\n",
    "                    response_logprobs.append(avg_log_prob)\n",
    "                else:\n",
    "                    response_logprobs.append(torch.tensor(0.0, device=self.device))\n",
    "            \n",
    "            if len(response_logprobs) >= 2:\n",
    "                # Convert rewards to preferences\n",
    "                reward_tensor = torch.tensor(query_rewards, device=self.device)\n",
    "                \n",
    "                # Compute pairwise losses\n",
    "                for i in range(len(response_logprobs)):\n",
    "                    for j in range(i + 1, len(response_logprobs)):\n",
    "                        if query_rewards[i] != query_rewards[j]:  # Only if different rewards\n",
    "                            # Preference: higher reward should have higher log prob\n",
    "                            if query_rewards[i] > query_rewards[j]:\n",
    "                                preferred_logprob = response_logprobs[i]\n",
    "                                dispreferred_logprob = response_logprobs[j]\n",
    "                            else:\n",
    "                                preferred_logprob = response_logprobs[j]\n",
    "                                dispreferred_logprob = response_logprobs[i]\n",
    "                            \n",
    "                            # GRPO loss: negative log sigmoid of difference\n",
    "                            diff = preferred_logprob - dispreferred_logprob\n",
    "                            loss = -F.logsigmoid(diff)\n",
    "                            total_loss += loss\n",
    "                            num_groups += 1\n",
    "        \n",
    "        return total_loss / max(num_groups, 1)\n",
    "    \n",
    "    def train_step(self, batch_queries, batch_ground_truths):\n",
    "        \"\"\"\n",
    "        Perform one training step of GRPO\n",
    "        \"\"\"\n",
    "        # Generate responses\n",
    "        responses = self.generate_responses(\n",
    "            batch_queries, \n",
    "            num_responses=self.config[\"group_size\"],\n",
    "            temperature=self.config[\"temperature\"]\n",
    "        )\n",
    "        \n",
    "        # Compute rewards\n",
    "        rewards = self.compute_rewards(batch_queries, responses, batch_ground_truths)\n",
    "        \n",
    "        # Compute and return GRPO loss\n",
    "        loss = self.compute_grpo_loss(batch_queries, responses, rewards)\n",
    "        \n",
    "        return loss, responses, rewards\n",
    "\n",
    "print(\"‚úÖ GRPO Trainer class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb7b661",
   "metadata": {},
   "source": [
    "## Prepare GRPO Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae141ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format data for GRPO\n",
    "grpo_formatted = grpo_data.map(format_chat_grpo, remove_columns=grpo_data.column_names)\n",
    "\n",
    "print(f\"GRPO data prepared: {len(grpo_formatted)} samples\")\n",
    "\n",
    "# Create batches for GRPO training\n",
    "def create_grpo_batches(dataset, batch_size=2):\n",
    "    batches = []\n",
    "    for i in range(0, len(dataset), batch_size):\n",
    "        batch = dataset[i:i + batch_size]\n",
    "        queries = [item[\"query\"] for item in batch]\n",
    "        ground_truths = [item[\"ground_truth\"] for item in batch]\n",
    "        batches.append((queries, ground_truths))\n",
    "    return batches\n",
    "\n",
    "grpo_batches = create_grpo_batches(grpo_formatted, batch_size=GRPO_CONFIG[\"per_device_train_batch_size\"])\n",
    "print(f\"Created {len(grpo_batches)} GRPO batches\")\n",
    "\n",
    "# Show sample GRPO data\n",
    "print(\"\\n=== Sample GRPO Query ===\")\n",
    "print(grpo_formatted[0][\"query\"][:300] + \"...\")\n",
    "print(f\"\\nGround truth: {grpo_formatted[0]['ground_truth'][:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5a5372c",
   "metadata": {},
   "source": [
    "## GRPO Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0509320",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize GRPO trainer\n",
    "grpo_trainer = GRPOTrainer(model, tokenizer, GRPO_CONFIG)\n",
    "\n",
    "# Setup optimizer for GRPO\n",
    "optimizer = AdamW(model.parameters(), lr=GRPO_CONFIG[\"learning_rate\"])\n",
    "\n",
    "print(\"üöÄ Starting GRPO training...\")\n",
    "print(f\"Training on {len(grpo_batches)} batches for {GRPO_CONFIG['num_train_epochs']} epochs\")\n",
    "\n",
    "# Training loop\n",
    "training_logs = []\n",
    "global_step = 0\n",
    "\n",
    "for epoch in range(GRPO_CONFIG[\"num_train_epochs\"]):\n",
    "    epoch_loss = 0.0\n",
    "    epoch_batches = 0\n",
    "    \n",
    "    progress_bar = tqdm(grpo_batches, desc=f\"GRPO Epoch {epoch+1}/{GRPO_CONFIG['num_train_epochs']}\")\n",
    "    \n",
    "    for batch_queries, batch_ground_truths in progress_bar:\n",
    "        try:\n",
    "            # Perform training step\n",
    "            loss, responses, rewards = grpo_trainer.train_step(batch_queries, batch_ground_truths)\n",
    "            \n",
    "            if loss.requires_grad:\n",
    "                # Backward pass\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                \n",
    "                # Gradient clipping\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "                \n",
    "                optimizer.step()\n",
    "                \n",
    "                epoch_loss += loss.item()\n",
    "                epoch_batches += 1\n",
    "                global_step += 1\n",
    "                \n",
    "                # Update progress bar\n",
    "                progress_bar.set_postfix({\n",
    "                    'loss': f'{loss.item():.4f}',\n",
    "                    'avg_loss': f'{epoch_loss/epoch_batches:.4f}'\n",
    "                })\n",
    "                \n",
    "                # Logging\n",
    "                if global_step % GRPO_CONFIG[\"logging_steps\"] == 0:\n",
    "                    log_entry = {\n",
    "                        \"epoch\": epoch,\n",
    "                        \"step\": global_step,\n",
    "                        \"loss\": loss.item(),\n",
    "                        \"avg_epoch_loss\": epoch_loss / epoch_batches,\n",
    "                        \"timestamp\": datetime.now().isoformat()\n",
    "                    }\n",
    "                    training_logs.append(log_entry)\n",
    "                    \n",
    "                    print(f\"\\nStep {global_step}: Loss = {loss.item():.4f}\")\n",
    "                    print(f\"Sample responses for batch:\")\n",
    "                    for i, (query_responses, query_rewards) in enumerate(zip(responses[:1], rewards[:1])):\n",
    "                        print(f\"  Query {i+1} responses:\")\n",
    "                        for j, (resp, rew) in enumerate(zip(query_responses[:2], query_rewards[:2])):\n",
    "                            print(f\"    Response {j+1} (reward: {rew:.3f}): {resp[:100]}...\")\n",
    "                \n",
    "                # Save checkpoint\n",
    "                if global_step % GRPO_CONFIG[\"save_steps\"] == 0:\n",
    "                    checkpoint_dir = f\"{GRPO_CONFIG['output_dir']}/checkpoint-{global_step}\"\n",
    "                    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "                    model.save_pretrained(f\"{checkpoint_dir}/lora_adapter\")\n",
    "                    tokenizer.save_pretrained(checkpoint_dir)\n",
    "                    print(f\"\\nüíæ Checkpoint saved at step {global_step}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"\\n‚ö†Ô∏è Error in batch: {e}\")\n",
    "            continue\n",
    "    \n",
    "    avg_epoch_loss = epoch_loss / max(epoch_batches, 1)\n",
    "    print(f\"\\n‚úÖ Epoch {epoch+1} completed. Average loss: {avg_epoch_loss:.4f}\")\n",
    "\n",
    "print(\"\\nüéâ GRPO training completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "941a1c7f",
   "metadata": {},
   "source": [
    "## Save GRPO Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b38df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save final GRPO model\n",
    "final_model_dir = f\"{GRPO_CONFIG['output_dir']}/final_model\"\n",
    "os.makedirs(final_model_dir, exist_ok=True)\n",
    "\n",
    "model.save_pretrained(f\"{final_model_dir}/lora_adapter\")\n",
    "tokenizer.save_pretrained(final_model_dir)\n",
    "\n",
    "print(f\"‚úÖ Final GRPO model saved to {final_model_dir}\")\n",
    "\n",
    "# Save GRPO training logs\n",
    "with open(f\"{GRPO_CONFIG['output_dir']}/grpo_training_logs.json\", \"w\") as f:\n",
    "    json.dump(training_logs, f, indent=2)\n",
    "\n",
    "print(f\"GRPO training logs saved to {GRPO_CONFIG['output_dir']}/grpo_training_logs.json\")\n",
    "\n",
    "# Create training summary\n",
    "training_summary = {\n",
    "    \"dataset\": DATASET_NAME,\n",
    "    \"model_config\": MODEL_CONFIG,\n",
    "    \"sft_config\": SFT_CONFIG,\n",
    "    \"grpo_config\": GRPO_CONFIG,\n",
    "    \"sft_samples\": len(sft_data),\n",
    "    \"grpo_samples\": len(grpo_data),\n",
    "    \"total_grpo_steps\": global_step,\n",
    "    \"training_completed\": datetime.now().isoformat(),\n",
    "    \"final_model_path\": final_model_dir\n",
    "}\n",
    "\n",
    "with open(f\"{GRPO_CONFIG['output_dir']}/training_summary.json\", \"w\") as f:\n",
    "    json.dump(training_summary, f, indent=2)\n",
    "\n",
    "print(f\"Training summary saved to {GRPO_CONFIG['output_dir']}/training_summary.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2683922",
   "metadata": {},
   "source": [
    "## Model Evaluation and Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cae665e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the final model\n",
    "print(\"üß™ Testing the final SFT+GRPO model...\")\n",
    "\n",
    "def test_model_generation(model, tokenizer, test_query, max_length=512):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        inputs = tokenizer(test_query, return_tensors=\"pt\", truncation=True, max_length=2048)\n",
    "        inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "        \n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_length=inputs[\"input_ids\"].shape[1] + max_length,\n",
    "            temperature=0.7,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "        \n",
    "        response = tokenizer.decode(\n",
    "            outputs[0][inputs[\"input_ids\"].shape[1]:], \n",
    "            skip_special_tokens=True\n",
    "        )\n",
    "        return response.strip()\n",
    "\n",
    "# Create a test query\n",
    "test_sample = grpo_formatted[0]\n",
    "test_query = test_sample[\"query\"]\n",
    "ground_truth = test_sample[\"ground_truth\"]\n",
    "\n",
    "print(\"=== Test Query ===\")\n",
    "print(test_query[:500] + \"...\")\n",
    "\n",
    "print(\"\\n=== Ground Truth ===\")\n",
    "print(ground_truth)\n",
    "\n",
    "print(\"\\n=== Model Response ===\")\n",
    "response = test_model_generation(model, tokenizer, test_query)\n",
    "print(response)\n",
    "\n",
    "print(\"\\n‚úÖ Model evaluation completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcf6020a",
   "metadata": {},
   "source": [
    "## Training Summary and Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "392f3840",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üìä Training Summary\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Dataset: {DATASET_NAME}\")\n",
    "print(f\"Total samples: {len(train_data):,}\")\n",
    "print(f\"SFT samples: {len(sft_data):,}\")\n",
    "print(f\"GRPO samples: {len(grpo_data):,}\")\n",
    "print(\"\\nüéØ Training Configuration:\")\n",
    "print(f\"SFT epochs: {SFT_CONFIG['num_train_epochs']}\")\n",
    "print(f\"GRPO epochs: {GRPO_CONFIG['num_train_epochs']}\")\n",
    "print(f\"Total GRPO steps: {global_step}\")\n",
    "print(\"\\nüíæ Model Outputs:\")\n",
    "print(f\"SFT model: {SFT_CONFIG['output_dir']}\")\n",
    "print(f\"Final SFT+GRPO model: {final_model_dir}\")\n",
    "print(\"\\nüî¨ Key Features:\")\n",
    "print(\"‚úÖ Supervised Fine-Tuning (SFT) for base capability\")\n",
    "print(\"‚úÖ Group Relative Policy Optimization (GRPO) for preference learning\")\n",
    "print(\"‚úÖ Custom reward system for Bitcoin prediction quality\")\n",
    "print(\"‚úÖ Response masking for efficient training\")\n",
    "print(\"‚úÖ Special token handling for structured outputs\")\n",
    "print(\"\\nüéâ Training pipeline completed successfully!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
