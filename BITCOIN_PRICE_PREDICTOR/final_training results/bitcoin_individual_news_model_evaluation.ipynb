{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a084b248",
   "metadata": {},
   "source": [
    "# Bitcoin Individual News Model Evaluation\n",
    "This notebook evaluates the Bitcoin prediction model trained on the individual news dataset with longer training parameters and provides comprehensive analysis with base model comparison."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e733fdd",
   "metadata": {},
   "source": [
    "## Install Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c87280c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers datasets torch peft accelerate matplotlib seaborn scipy pandas numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c77481",
   "metadata": {},
   "source": [
    "## Load Individual News Model and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5092026",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "import json\n",
    "import re\n",
    "from scipy.stats import wilcoxon\n",
    "from collections import defaultdict\n",
    "\n",
    "# Load the base model for individual news training\n",
    "base_model_id = './Qwen3_8B'\n",
    "\n",
    "# Load the base model and tokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_id,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    base_model_id,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(\"Individual News Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9557cab7",
   "metadata": {},
   "source": [
    "## Load LoRA Adapter from Individual News Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9512f59d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the LoRA adapter from the individual news training (checkpoint 200)\n",
    "adapter_path = './qwen_bitcoin_chat_fast_more_longer/checkpoint-200'  # Using checkpoint 200\n",
    "\n",
    "# Load the model with LoRA adapter\n",
    "model = PeftModel.from_pretrained(model, adapter_path)\n",
    "model.eval()\n",
    "\n",
    "print(\"Individual News Model with adapter (checkpoint-200) loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90469356",
   "metadata": {},
   "source": [
    "## Load Test Data from Individual News Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f28433f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the individual news dataset\n",
    "test_dataset = load_dataset('tahamajs/bitcoin-investment-advisory-dataset', split='test')\n",
    "print(f\"Loaded {len(test_dataset)} test samples\")\n",
    "print(\"Sample test data:\")\n",
    "print(test_dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d01a90",
   "metadata": {},
   "source": [
    "## Utility Functions for Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f900806e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_prices_from_text(text):\n",
    "    \"\"\"Extract price predictions from model output\"\"\"\n",
    "    # Look for patterns like numbers separated by commas\n",
    "    price_pattern = r'(\\d+(?:\\.\\d+)?(?:,\\s*\\d+(?:\\.\\d+)?)*)'  \n",
    "    matches = re.findall(price_pattern, text)\n",
    "    \n",
    "    if matches:\n",
    "        # Take the first match and split by comma\n",
    "        prices_str = matches[0]\n",
    "        try:\n",
    "            prices = [float(p.strip()) for p in prices_str.split(',')]\n",
    "            return prices\n",
    "        except:\n",
    "            return []\n",
    "    return []\n",
    "\n",
    "def calculate_metrics(predictions, ground_truth):\n",
    "    \"\"\"Calculate evaluation metrics\"\"\"\n",
    "    if len(predictions) != len(ground_truth):\n",
    "        return None\n",
    "    \n",
    "    predictions = np.array(predictions)\n",
    "    ground_truth = np.array(ground_truth)\n",
    "    \n",
    "    mse = np.mean((predictions - ground_truth) ** 2)\n",
    "    mae = np.mean(np.abs(predictions - ground_truth))\n",
    "    mape = np.mean(np.abs((ground_truth - predictions) / ground_truth)) * 100\n",
    "    \n",
    "    return {\n",
    "        'MSE': mse,\n",
    "        'MAE': mae,\n",
    "        'MAPE': mape,\n",
    "        'RMSE': np.sqrt(mse)\n",
    "    }\n",
    "\n",
    "def analyze_text_quality(generated_text, expected_text):\n",
    "    \"\"\"Analyze text quality metrics beyond numerical predictions\"\"\"\n",
    "    # Text length analysis\n",
    "    gen_length = len(generated_text.split())\n",
    "    exp_length = len(expected_text.split())\n",
    "    length_ratio = gen_length / exp_length if exp_length > 0 else 0\n",
    "    \n",
    "    # Word overlap analysis\n",
    "    gen_words = set(generated_text.lower().split())\n",
    "    exp_words = set(expected_text.lower().split())\n",
    "    \n",
    "    intersection = gen_words.intersection(exp_words)\n",
    "    union = gen_words.union(exp_words)\n",
    "    \n",
    "    jaccard_similarity = len(intersection) / len(union) if len(union) > 0 else 0\n",
    "    \n",
    "    # Check for key financial terms\n",
    "    financial_terms = ['price', 'market', 'trend', 'analysis', 'prediction', 'forecast', \n",
    "                      'bitcoin', 'btc', 'increase', 'decrease', 'bullish', 'bearish',\n",
    "                      'investment', 'trading', 'volatility', 'support', 'resistance']\n",
    "    \n",
    "    gen_financial_terms = sum(1 for term in financial_terms if term in generated_text.lower())\n",
    "    exp_financial_terms = sum(1 for term in financial_terms if term in expected_text.lower())\n",
    "    \n",
    "    return {\n",
    "        'length_ratio': length_ratio,\n",
    "        'jaccard_similarity': jaccard_similarity,\n",
    "        'generated_length': gen_length,\n",
    "        'expected_length': exp_length,\n",
    "        'generated_financial_terms': gen_financial_terms,\n",
    "        'expected_financial_terms': exp_financial_terms,\n",
    "        'financial_term_coverage': gen_financial_terms / max(exp_financial_terms, 1)\n",
    "    }\n",
    "\n",
    "def format_input(example):\n",
    "    \"\"\"Format input for the individual news model\"\"\"\n",
    "    instruction = example.get('instruction', '')\n",
    "    user_input = example.get('input', '')\n",
    "    messages = [\n",
    "        {'role': 'system', 'content': instruction},\n",
    "        {'role': 'user', 'content': user_input}\n",
    "    ]\n",
    "    return tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "print(\"Utility functions loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4db2fe6",
   "metadata": {},
   "source": [
    "## Comprehensive Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01604cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run comprehensive evaluation\n",
    "results = []\n",
    "text_quality_results = []\n",
    "total_samples = min(100, len(test_dataset))  # Test on 100 samples or all if less\n",
    "\n",
    "print(f\"Running comprehensive evaluation on {total_samples} samples...\")\n",
    "\n",
    "for i in range(total_samples):\n",
    "    test_example = test_dataset[i]\n",
    "    test_text = format_input(test_example)\n",
    "    \n",
    "    inputs = tokenizer(test_text, return_tensors='pt', truncation=True, max_length=2048)\n",
    "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=256,\n",
    "            do_sample=False,  # Use greedy decoding for consistency\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    generated_text = tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)\n",
    "    \n",
    "    # Extract predictions and ground truth\n",
    "    predicted_prices = extract_prices_from_text(generated_text)\n",
    "    actual_output = test_example.get('output', '')\n",
    "    actual_prices = extract_prices_from_text(actual_output)\n",
    "    \n",
    "    # Analyze text quality\n",
    "    text_quality = analyze_text_quality(generated_text, actual_output)\n",
    "    text_quality_results.append(text_quality)\n",
    "    \n",
    "    if predicted_prices and actual_prices:\n",
    "        # Truncate to minimum length for fair comparison\n",
    "        min_len = min(len(predicted_prices), len(actual_prices))\n",
    "        if min_len > 0:\n",
    "            pred_truncated = predicted_prices[:min_len]\n",
    "            actual_truncated = actual_prices[:min_len]\n",
    "            \n",
    "            metrics = calculate_metrics(pred_truncated, actual_truncated)\n",
    "            if metrics:\n",
    "                results.append({\n",
    "                    'sample_id': i,\n",
    "                    'predicted': pred_truncated,\n",
    "                    'actual': actual_truncated,\n",
    "                    'metrics': metrics,\n",
    "                    'text_quality': text_quality,\n",
    "                    'generated_text': generated_text,\n",
    "                    'expected_text': actual_output\n",
    "                })\n",
    "    \n",
    "    if (i + 1) % 20 == 0:\n",
    "        print(f\"Processed {i + 1}/{total_samples} samples...\")\n",
    "\n",
    "print(f\"\\nEvaluation completed! Analyzed {len(results)} valid samples out of {total_samples}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb79ee5",
   "metadata": {},
   "source": [
    "## Detailed Results Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82cc64b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "if results:\n",
    "    # Calculate numerical performance metrics\n",
    "    all_mse = [r['metrics']['MSE'] for r in results]\n",
    "    all_mae = [r['metrics']['MAE'] for r in results]\n",
    "    all_mape = [r['metrics']['MAPE'] for r in results]\n",
    "    all_rmse = [r['metrics']['RMSE'] for r in results]\n",
    "    \n",
    "    # Calculate text quality metrics\n",
    "    all_length_ratios = [r['text_quality']['length_ratio'] for r in results]\n",
    "    all_jaccard_sim = [r['text_quality']['jaccard_similarity'] for r in results]\n",
    "    all_financial_coverage = [r['text_quality']['financial_term_coverage'] for r in results]\n",
    "    \n",
    "    print(\"=\" * 80)\n",
    "    print(\"🎯 INDIVIDUAL NEWS MODEL - COMPREHENSIVE ANALYSIS\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    print(f\"\\n📊 NUMERICAL PERFORMANCE METRICS:\")\n",
    "    print(f\"Mean Squared Error (MSE): {np.mean(all_mse):.4f} ± {np.std(all_mse):.4f}\")\n",
    "    print(f\"Mean Absolute Error (MAE): {np.mean(all_mae):.4f} ± {np.std(all_mae):.4f}\")\n",
    "    print(f\"Root Mean Squared Error (RMSE): {np.mean(all_rmse):.4f} ± {np.std(all_rmse):.4f}\")\n",
    "    print(f\"Mean Absolute Percentage Error (MAPE): {np.mean(all_mape):.2f}% ± {np.std(all_mape):.2f}%\")\n",
    "    \n",
    "    print(f\"\\n📝 TEXT QUALITY METRICS:\")\n",
    "    print(f\"Average Response Length Ratio: {np.mean(all_length_ratios):.2f} ± {np.std(all_length_ratios):.2f}\")\n",
    "    print(f\"Average Jaccard Similarity: {np.mean(all_jaccard_sim):.3f} ± {np.std(all_jaccard_sim):.3f}\")\n",
    "    print(f\"Financial Term Coverage: {np.mean(all_financial_coverage):.2f} ± {np.std(all_financial_coverage):.2f}\")\n",
    "    \n",
    "    print(f\"\\n📋 DETAILED TEXT ANALYSIS:\")\n",
    "    avg_gen_length = np.mean([r['text_quality']['generated_length'] for r in results])\n",
    "    avg_exp_length = np.mean([r['text_quality']['expected_length'] for r in results])\n",
    "    avg_gen_fin_terms = np.mean([r['text_quality']['generated_financial_terms'] for r in results])\n",
    "    avg_exp_fin_terms = np.mean([r['text_quality']['expected_financial_terms'] for r in results])\n",
    "    \n",
    "    print(f\"Average Generated Response Length: {avg_gen_length:.1f} words\")\n",
    "    print(f\"Average Expected Response Length: {avg_exp_length:.1f} words\")\n",
    "    print(f\"Average Financial Terms in Generated: {avg_gen_fin_terms:.1f}\")\n",
    "    print(f\"Average Financial Terms in Expected: {avg_exp_fin_terms:.1f}\")\n",
    "    \n",
    "    # Median metrics for robustness\n",
    "    print(f\"\\n📊 MEDIAN PERFORMANCE METRICS (Robust):\")\n",
    "    print(f\"Median MSE: {np.median(all_mse):.4f}\")\n",
    "    print(f\"Median MAE: {np.median(all_mae):.4f}\")\n",
    "    print(f\"Median RMSE: {np.median(all_rmse):.4f}\")\n",
    "    print(f\"Median MAPE: {np.median(all_mape):.2f}%\")\n",
    "    print(f\"Median Jaccard Similarity: {np.median(all_jaccard_sim):.3f}\")\n",
    "    \n",
    "    # Sample outputs for qualitative analysis\n",
    "    print(f\"\\n🔍 SAMPLE OUTPUTS FOR QUALITATIVE ANALYSIS:\")\n",
    "    for i, result in enumerate(results[:3]):\n",
    "        print(f\"\\n--- Sample {i+1} ---\")\n",
    "        print(f\"Predicted Prices: {result['predicted']}\")\n",
    "        print(f\"Actual Prices:    {result['actual']}\")\n",
    "        print(f\"MAE: {result['metrics']['MAE']:.4f}, MAPE: {result['metrics']['MAPE']:.2f}%\")\n",
    "        print(f\"Text Similarity: {result['text_quality']['jaccard_similarity']:.3f}\")\n",
    "        print(f\"Generated Text: {result['generated_text'][:200]}...\")\n",
    "        print(f\"Expected Text:  {result['expected_text'][:200]}...\")\n",
    "        print(\"-\" * 60)\n",
    "\n",
    "else:\n",
    "    print(\"❌ No valid results found for analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14009ab4",
   "metadata": {},
   "source": [
    "## Load Base Qwen Model for Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36debb52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load base Qwen model for comparison\n",
    "print(\"Loading base Qwen model for comparison...\")\n",
    "\n",
    "base_qwen_model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"Qwen/Qwen2.5-8B-Instruct\",\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "base_qwen_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"Qwen/Qwen2.5-8B-Instruct\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "if base_qwen_tokenizer.pad_token is None:\n",
    "    base_qwen_tokenizer.pad_token = base_qwen_tokenizer.eos_token\n",
    "\n",
    "print(\"Base Qwen model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa504e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate base model\n",
    "def format_input_for_base_model(example):\n",
    "    \"\"\"Format input for base Qwen model with bitcoin prediction task\"\"\"\n",
    "    instruction = example.get('instruction', '')\n",
    "    user_input = example.get('input', '')\n",
    "    \n",
    "    bitcoin_instruction = \"\"\"You are a Bitcoin investment advisor. Based on the provided market data and news, predict the next 10 days of Bitcoin prices. Provide your predictions as comma-separated numbers.\"\"\"\n",
    "    \n",
    "    messages = [\n",
    "        {'role': 'system', 'content': bitcoin_instruction},\n",
    "        {'role': 'user', 'content': f\"{instruction}\\n\\n{user_input}\\n\\nPlease provide 10 Bitcoin price predictions for the next 10 days, separated by commas.\"}\n",
    "    ]\n",
    "    return base_qwen_tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "# Run evaluation on base model\n",
    "base_results = []\n",
    "base_text_quality_results = []\n",
    "comparison_samples = min(50, len(results))  # Use subset for comparison\n",
    "\n",
    "print(f\"Evaluating base Qwen model on {comparison_samples} samples...\")\n",
    "\n",
    "for i in range(comparison_samples):\n",
    "    test_example = test_dataset[i]\n",
    "    test_text = format_input_for_base_model(test_example)\n",
    "    \n",
    "    inputs = base_qwen_tokenizer(test_text, return_tensors='pt', truncation=True, max_length=2048)\n",
    "    inputs = {k: v.to(base_qwen_model.device) for k, v in inputs.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = base_qwen_model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=256,\n",
    "            do_sample=False,\n",
    "            pad_token_id=base_qwen_tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    generated_text = base_qwen_tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)\n",
    "    \n",
    "    predicted_prices = extract_prices_from_text(generated_text)\n",
    "    actual_output = test_example.get('output', '')\n",
    "    actual_prices = extract_prices_from_text(actual_output)\n",
    "    \n",
    "    # Analyze text quality for base model\n",
    "    text_quality = analyze_text_quality(generated_text, actual_output)\n",
    "    base_text_quality_results.append(text_quality)\n",
    "    \n",
    "    if predicted_prices and actual_prices:\n",
    "        min_len = min(len(predicted_prices), len(actual_prices))\n",
    "        if min_len > 0:\n",
    "            pred_truncated = predicted_prices[:min_len]\n",
    "            actual_truncated = actual_prices[:min_len]\n",
    "            \n",
    "            metrics = calculate_metrics(pred_truncated, actual_truncated)\n",
    "            if metrics:\n",
    "                base_results.append({\n",
    "                    'sample_id': i,\n",
    "                    'predicted': pred_truncated,\n",
    "                    'actual': actual_truncated,\n",
    "                    'metrics': metrics,\n",
    "                    'text_quality': text_quality,\n",
    "                    'generated_text': generated_text,\n",
    "                    'expected_text': actual_output\n",
    "                })\n",
    "    \n",
    "    if (i + 1) % 10 == 0:\n",
    "        print(f\"Processed {i + 1}/{comparison_samples} samples...\")\n",
    "\n",
    "print(f\"\\nBase model evaluation completed! Analyzed {len(base_results)} valid samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f431b7",
   "metadata": {},
   "source": [
    "## Comprehensive Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "768b9509",
   "metadata": {},
   "outputs": [],
   "source": [
    "if base_results and results:\n",
    "    # Extract metrics from results for proper comparison\n",
    "    # Individual News Model metrics\n",
    "    ind_mse = [r['metrics']['MSE'] for r in results]\n",
    "    ind_mae = [r['metrics']['MAE'] for r in results] \n",
    "    ind_rmse = [r['metrics']['RMSE'] for r in results]\n",
    "    ind_mape = [r['metrics']['MAPE'] for r in results]\n",
    "    ind_jaccard = [r['text_quality']['jaccard_similarity'] for r in results]\n",
    "    ind_length_ratio = [r['text_quality']['length_ratio'] for r in results]\n",
    "    ind_financial_coverage = [r['text_quality']['financial_term_coverage'] for r in results]\n",
    "    \n",
    "    # Base Model metrics\n",
    "    base_mse = [r['metrics']['MSE'] for r in base_results]\n",
    "    base_mae = [r['metrics']['MAE'] for r in base_results]\n",
    "    base_rmse = [r['metrics']['RMSE'] for r in base_results]\n",
    "    base_mape = [r['metrics']['MAPE'] for r in base_results]\n",
    "    base_jaccard = [r['text_quality']['jaccard_similarity'] for r in base_results]\n",
    "    base_length_ratio = [r['text_quality']['length_ratio'] for r in base_results]\n",
    "    base_financial_coverage = [r['text_quality']['financial_term_coverage'] for r in base_results]\n",
    "    \n",
    "    # Statistical significance testing\n",
    "    from scipy.stats import wilcoxon\n",
    "    \n",
    "    # Test for significant differences (using matched samples)\n",
    "    min_samples = min(len(results), len(base_results))\n",
    "    if min_samples >= 5:  # Minimum for statistical testing\n",
    "        mse_stat, mse_pvalue = wilcoxon(ind_mse[:min_samples], base_mse[:min_samples])\n",
    "        mae_stat, mae_pvalue = wilcoxon(ind_mae[:min_samples], base_mae[:min_samples])\n",
    "        jaccard_stat, jaccard_pvalue = wilcoxon(ind_jaccard[:min_samples], base_jaccard[:min_samples])\n",
    "        \n",
    "        print(f\"\\n🔬 STATISTICAL SIGNIFICANCE TESTS:\")\n",
    "        print(f\"MSE difference p-value: {mse_pvalue:.4f} ({'Significant' if mse_pvalue < 0.05 else 'Not significant'})\")\n",
    "        print(f\"MAE difference p-value: {mae_pvalue:.4f} ({'Significant' if mae_pvalue < 0.05 else 'Not significant'})\")\n",
    "        print(f\"Jaccard similarity p-value: {jaccard_pvalue:.4f} ({'Significant' if jaccard_pvalue < 0.05 else 'Not significant'})\")\n",
    "    \n",
    "    # Save comprehensive results\n",
    "    individual_news_comprehensive_results = {\n",
    "        'individual_news_model_results': {\n",
    "            'model_id': 'tahamajs/my-awesome-model_final_bitcoin-individual-news-dataset',\n",
    "            'adapter_path': './qwen_bitcoin_chat_fast_more_longer/checkpoint-200',\n",
    "            'dataset': 'bitcoin-investment-advisory-dataset',\n",
    "            'training_config': 'longer_training_4_epochs_checkpoint_200',\n",
    "            'samples_analyzed': len(results),\n",
    "            'numerical_metrics': {\n",
    "                'mean_mse': float(np.mean(ind_mse)),\n",
    "                'mean_mae': float(np.mean(ind_mae)),\n",
    "                'mean_rmse': float(np.mean(ind_rmse)),\n",
    "                'mean_mape': float(np.mean(ind_mape))\n",
    "            },\n",
    "            'text_quality_metrics': {\n",
    "                'mean_jaccard_similarity': float(np.mean(ind_jaccard)),\n",
    "                'mean_length_ratio': float(np.mean(ind_length_ratio)),\n",
    "                'mean_financial_coverage': float(np.mean(ind_financial_coverage))\n",
    "            }\n",
    "        },\n",
    "        'base_model_results': {\n",
    "            'model_id': 'Qwen/Qwen2.5-8B-Instruct',\n",
    "            'samples_analyzed': len(base_results),\n",
    "            'numerical_metrics': {\n",
    "                'mean_mse': float(np.mean(base_mse)),\n",
    "                'mean_mae': float(np.mean(base_mae)),\n",
    "                'mean_rmse': float(np.mean(base_rmse)),\n",
    "                'mean_mape': float(np.mean(base_mape))\n",
    "            },\n",
    "            'text_quality_metrics': {\n",
    "                'mean_jaccard_similarity': float(np.mean(base_jaccard)),\n",
    "                'mean_length_ratio': float(np.mean(base_length_ratio)),\n",
    "                'mean_financial_coverage': float(np.mean(base_financial_coverage))\n",
    "            }\n",
    "        },\n",
    "        'detailed_individual_results': results,\n",
    "        'detailed_base_results': base_results\n",
    "    }\n",
    "    \n",
    "    with open('individual_news_model_comprehensive_analysis.json', 'w') as f:\n",
    "        json.dump(individual_news_comprehensive_results, f, indent=2)\n",
    "    \n",
    "    print(f\"\\n💾 Comprehensive analysis results saved to 'individual_news_model_comprehensive_analysis.json'\")\n",
    "    \n",
    "    # Research paper summary\n",
    "    print(f\"\\n📋 RESEARCH PAPER SUMMARY - INDIVIDUAL NEWS MODEL:\")\n",
    "    print(\"=\" * 70)\n",
    "    mse_improvement = (np.mean(base_mse) - np.mean(ind_mse)) / np.mean(base_mse) * 100\n",
    "    mae_improvement = (np.mean(base_mae) - np.mean(ind_mae)) / np.mean(base_mae) * 100\n",
    "    jaccard_improvement = (np.mean(ind_jaccard) - np.mean(base_jaccard)) / np.mean(base_jaccard) * 100\n",
    "    \n",
    "    print(f\"Individual News Model Performance:\")\n",
    "    print(f\"• MSE improved by {mse_improvement:.2f}%\")\n",
    "    print(f\"• MAE improved by {mae_improvement:.2f}%\")\n",
    "    print(f\"• Text similarity improved by {jaccard_improvement:.2f}%\")\n",
    "    print(f\"• Mean Jaccard Similarity: {np.mean(ind_jaccard):.3f}\")\n",
    "    print(f\"• Financial term coverage: {np.mean(ind_financial_coverage):.2f}\")\n",
    "    print(f\"• Model: tahamajs/my-awesome-model_final_bitcoin-individual-news-dataset\")\n",
    "    print(f\"• Checkpoint: 200 (4 epochs with longer parameters)\")\n",
    "\n",
    "else:\n",
    "    print(\"❌ No valid results found for model comparison\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72851737",
   "metadata": {},
   "source": [
    "## Visualization of Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cdabd6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if base_results and results:\n",
    "    # Create comprehensive visualization\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    fig.suptitle('Individual News Model vs Base Qwen - Comprehensive Analysis', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # MSE comparison\n",
    "    axes[0,0].boxplot([base_mse, ind_mse], labels=['Base Qwen', 'Individual News'])\n",
    "    axes[0,0].set_title('Mean Squared Error', fontweight='bold')\n",
    "    axes[0,0].set_ylabel('MSE')\n",
    "    axes[0,0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # MAE comparison\n",
    "    axes[0,1].boxplot([base_mae, ind_mae], labels=['Base Qwen', 'Individual News'])\n",
    "    axes[0,1].set_title('Mean Absolute Error', fontweight='bold')\n",
    "    axes[0,1].set_ylabel('MAE')\n",
    "    axes[0,1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # MAPE comparison\n",
    "    axes[0,2].boxplot([base_mape, ind_mape], labels=['Base Qwen', 'Individual News'])\n",
    "    axes[0,2].set_title('Mean Absolute Percentage Error', fontweight='bold')\n",
    "    axes[0,2].set_ylabel('MAPE (%)')\n",
    "    axes[0,2].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Jaccard similarity comparison\n",
    "    axes[1,0].boxplot([base_jaccard, ind_jaccard], labels=['Base Qwen', 'Individual News'])\n",
    "    axes[1,0].set_title('Text Similarity (Jaccard)', fontweight='bold')\n",
    "    axes[1,0].set_ylabel('Jaccard Similarity')\n",
    "    axes[1,0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Length ratio comparison\n",
    "    axes[1,1].boxplot([base_length_ratio, ind_length_ratio], labels=['Base Qwen', 'Individual News'])\n",
    "    axes[1,1].set_title('Response Length Ratio', fontweight='bold')\n",
    "    axes[1,1].set_ylabel('Length Ratio')\n",
    "    axes[1,1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Improvement bar chart\n",
    "    metrics = ['MSE', 'MAE', 'MAPE', 'Text Sim', 'Length', 'Fin Terms']\n",
    "    improvements = [\n",
    "        (np.mean(base_mse) - np.mean(ind_mse)) / np.mean(base_mse) * 100,\n",
    "        (np.mean(base_mae) - np.mean(ind_mae)) / np.mean(base_mae) * 100,\n",
    "        (np.mean(base_mape) - np.mean(ind_mape)) / np.mean(base_mape) * 100,\n",
    "        (np.mean(ind_jaccard) - np.mean(base_jaccard)) / np.mean(base_jaccard) * 100,\n",
    "        (np.mean(ind_length_ratio) - np.mean(base_length_ratio)) / np.mean(base_length_ratio) * 100,\n",
    "        (np.mean(ind_financial_coverage) - np.mean(base_financial_coverage)) / np.mean(base_financial_coverage) * 100\n",
    "    ]\n",
    "    \n",
    "    colors = ['green' if imp > 0 else 'red' for imp in improvements]\n",
    "    bars = axes[1,2].bar(metrics, improvements, color=colors, alpha=0.7)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar, imp in zip(bars, improvements):\n",
    "        height = bar.get_height()\n",
    "        axes[1,2].text(bar.get_x() + bar.get_width()/2., height,\n",
    "                       f'{imp:.1f}%', ha='center', va='bottom' if height > 0 else 'top',\n",
    "                       fontweight='bold', fontsize=8)\n",
    "    \n",
    "    axes[1,2].set_title('Performance Improvements', fontweight='bold')\n",
    "    axes[1,2].set_ylabel('Improvement (%)')\n",
    "    axes[1,2].axhline(y=0, color='black', linestyle='-', alpha=0.3)\n",
    "    axes[1,2].grid(True, alpha=0.3)\n",
    "    axes[1,2].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('individual_news_model_comprehensive_analysis.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"📊 Comprehensive analysis visualization saved as 'individual_news_model_comprehensive_analysis.png'\")\n",
    "\n",
    "else:\n",
    "    print(\"❌ Cannot create visualizations - insufficient data\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
