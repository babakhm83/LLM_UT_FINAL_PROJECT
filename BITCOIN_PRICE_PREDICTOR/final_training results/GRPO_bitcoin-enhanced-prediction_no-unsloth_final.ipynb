{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01d83efe",
   "metadata": {},
   "source": [
    "# GRPO Training for Bitcoin Enhanced Prediction (No Unsloth)\n",
    "\n",
    "This notebook performs GRPO training using Hugging Face Transformers, TRL, PEFT, and bitsandbytes — without Unsloth. It loads a pre-trained LoRA adapter and optimizes it with a structured-output reward.\n",
    "\n",
    "- Base model: Qwen 2.5 Instruct (4-bit quantized)\n",
    "- Adapter: tahamajs/my-awesome-model_final_bitcoin_enhanced_prediction_dataset_with_local_comprehensive_news (checkpoint-1152)\n",
    "- Dataset: tahamajs/bitcoin-enhanced-prediction-dataset-with-local-comprehensive-news\n",
    "- Reward: Structured JSON action/confidence/SL/TP/forecast closeness + analysis quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef81794c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional installs (uncomment if needed)\n",
    "# !pip install -U transformers accelerate trl peft bitsandbytes datasets safetensors\n",
    "# !pip install -U einops sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb313c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, random, re\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from peft import PeftModel, LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from trl import GRPOTrainer, GRPOConfig\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb951d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config\n",
    "BASE_MODEL = \"Qwen/Qwen2.5-7B-Instruct\"\n",
    "LOAD_IN_4BIT = True\n",
    "ADAPTER_PATH = \"tahamajs/my-awesome-model_final_bitcoin_enhanced_prediction_dataset_with_local_comprehensive_news\"\n",
    "CHECKPOINT = \"checkpoint-1152\"\n",
    "MAX_SEQ_LEN = 2048\n",
    "MAX_LENGTH = 1024\n",
    "MAX_PROMPT_LENGTH = 512\n",
    "OUTPUT_DIR = \"./qwen_bitcoin_enhanced_grpo_no_unsloth\"\n",
    "LEARNING_RATE = 3e-7\n",
    "EPOCHS = 1\n",
    "BETA = 0.1\n",
    "PER_DEVICE_TRAIN_BATCH_SIZE = 1\n",
    "GRAD_ACCUM_STEPS = 8\n",
    "\n",
    "DATASET_NAME = \"tahamajs/bitcoin-enhanced-prediction-dataset-with-local-comprehensive-news\"\n",
    "REWARD_MODEL_NAME = \"microsoft/DialoGPT-medium\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a517ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load base model in 4bit\n",
    "quant_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=LOAD_IN_4BIT,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16 if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else torch.float16,\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL, use_fast=True)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL,\n",
    "    quantization_config=quant_config if LOAD_IN_4BIT else None,\n",
    "    torch_dtype=torch.bfloat16 if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else torch.float16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "# Attach (and continue training) LoRA\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "lora_cfg = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.0,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "model = get_peft_model(model, lora_cfg)\n",
    "\n",
    "# Load pre-trained adapter\n",
    "try:\n",
    "    adapter_path = f\"{ADAPTER_PATH}/{CHECKPOINT}\"\n",
    "    model = PeftModel.from_pretrained(model, adapter_path)\n",
    "    print(f\"Loaded adapter from {adapter_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"Warning: couldn't load adapter: {e}\")\n",
    "\n",
    "# Load reward model (optional)\n",
    "try:\n",
    "    reward_tokenizer = AutoTokenizer.from_pretrained(REWARD_MODEL_NAME)\n",
    "    reward_model = AutoModelForCausalLM.from_pretrained(REWARD_MODEL_NAME, device_map=\"auto\")\n",
    "    reward_model.eval()\n",
    "    print(\"Reward model loaded\")\n",
    "except Exception as e:\n",
    "    print(f\"Reward model unavailable, fallback to rule-based only: {e}\")\n",
    "    reward_model, reward_tokenizer = None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9556c023",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "raw_ds = load_dataset(DATASET_NAME, split=\"train\")\n",
    "print(f\"Loaded dataset {DATASET_NAME} with {len(raw_ds):,} samples\")\n",
    "\n",
    "# Convert to TRL chat format: list of dicts with 'conversations'\n",
    "def to_conversations(examples):\n",
    "    instructions = examples.get(\"instruction\", [\"\"] * len(examples.get(\"input\", [])))\n",
    "    inputs = examples.get(\"input\", [])\n",
    "    outputs = examples.get(\"output\", [])\n",
    "    chats = []\n",
    "    for inst, inp, out in zip(instructions, inputs, outputs):\n",
    "        chats.append([\n",
    "            {\"role\": \"system\", \"content\": inst or \"You are a helpful Bitcoin market analyst.\"},\n",
    "            {\"role\": \"user\", \"content\": inp or \"\"},\n",
    "            {\"role\": \"assistant\", \"content\": out or \"\"},\n",
    "        ])\n",
    "    return {\"conversations\": chats}\n",
    "\n",
    "train_ds = raw_ds.map(to_conversations, batched=True, remove_columns=raw_ds.column_names)\n",
    "print(\"Example conv:\")\n",
    "for m in train_ds[0][\"conversations\"]:\n",
    "    print(m[\"role\"], m[\"content\"][:120])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2524baa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Structured-output helpers\n",
    "\n",
    "def parse_trading_output(text):\n",
    "    if not text:\n",
    "        return None\n",
    "    try:\n",
    "        return json.loads(text.strip())\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        pattern = r'\\{[^{}]*\"action\"[^{}]*\\}'\n",
    "        matches = re.findall(pattern, text, re.IGNORECASE | re.DOTALL)\n",
    "        if matches:\n",
    "            for m in matches:\n",
    "                try:\n",
    "                    return json.loads(m.strip())\n",
    "                except:\n",
    "                    continue\n",
    "        result = {}\n",
    "        m = re.search(r'\"action\"\\s*:\\s*\"([^\"]+)\"', text, re.IGNORECASE)\n",
    "        if m: result['action'] = m.group(1).upper()\n",
    "        m = re.search(r'\"confidence\"\\s*:\\s*(\\d+(?:\\.\\d+)?)', text, re.IGNORECASE)\n",
    "        if m: result['confidence'] = float(m.group(1))\n",
    "        m = re.search(r'\"stop_loss\"\\s*:\\s*(\\d+(?:\\.\\d+)?)', text, re.IGNORECASE)\n",
    "        if m: result['stop_loss'] = float(m.group(1))\n",
    "        m = re.search(r'\"take_profit\"\\s*:\\s*(\\d+(?:\\.\\d+)?)', text, re.IGNORECASE)\n",
    "        if m: result['take_profit'] = float(m.group(1))\n",
    "        m = re.search(r'\"forecast_10d\"\\s*:\\s*\\[([^\\]]+)\\]', text, re.IGNORECASE)\n",
    "        if m:\n",
    "            try:\n",
    "                vals = [float(x.strip()) for x in m.group(1).split(',')]\n",
    "                result['forecast_10d'] = vals\n",
    "            except:\n",
    "                pass\n",
    "        return result if result else None\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "\n",
    "def calculate_forecast_similarity(resp_forecast, gt_forecast):\n",
    "    if not resp_forecast or not gt_forecast: return 0.0\n",
    "    import numpy as np\n",
    "    try:\n",
    "        r = np.array([float(x) for x in resp_forecast if isinstance(x, (int, float, str))])\n",
    "        g = np.array([float(x) for x in gt_forecast if isinstance(x, (int, float, str))])\n",
    "        if len(r) == 0 or len(g) == 0: return 0.0\n",
    "        n = min(len(r), len(g))\n",
    "        r, g = r[:n], g[:n]\n",
    "        if n < 2: return 0.0\n",
    "        score = 0.0\n",
    "        # corr (40%)\n",
    "        try:\n",
    "            corr = np.corrcoef(r, g)[0,1]\n",
    "            if not np.isnan(corr): score += abs(corr) * 0.4\n",
    "        except: pass\n",
    "        # directional (30%)\n",
    "        rd, gd = np.diff(r) > 0, np.diff(g) > 0\n",
    "        if len(rd) > 0: score += (np.mean(rd == gd) * 0.3)\n",
    "        # magnitude (30%)\n",
    "        try:\n",
    "            rn = (r - r.mean()) / (r.std() + 1e-8)\n",
    "            gn = (g - g.mean()) / (g.std() + 1e-8)\n",
    "            mse = ((rn - gn) ** 2).mean()\n",
    "            mag = max(0, 1 - (mse / 4))\n",
    "            score += mag * 0.3\n",
    "        except: pass\n",
    "        return float(min(1.0, max(0.0, score)))\n",
    "    except:\n",
    "        return 0.0\n",
    "\n",
    "\n",
    "def calculate_comprehensive_prediction_reward(response, ground_truth, reward_model=None, reward_tokenizer=None):\n",
    "    total_reward = 0.0\n",
    "    # 1) Prediction quality (20%)\n",
    "    pred = 0.0\n",
    "    L = len(response)\n",
    "    if 150 <= L <= 1000: pred += 0.15\n",
    "    elif 100 <= L <= 1200: pred += 0.10\n",
    "    elif L < 100: pred -= 0.05\n",
    "    kw = {\n",
    "        'prediction': 0.08,'forecast':0.08,'analysis':0.06,'market':0.06,\n",
    "        'bitcoin':0.05,'btc':0.04,'price':0.07,'trend':0.06,\n",
    "        'technical':0.05,'fundamental':0.05,'news':0.05,'data':0.04,\n",
    "        'indicators':0.05,'volume':0.04,'resistance':0.04,'support':0.04,\n",
    "        'momentum':0.04,'correlation':0.03,'pattern':0.04,'signal':0.04\n",
    "    }\n",
    "    rl = response.lower()\n",
    "    for k,w in kw.items():\n",
    "        if k in rl: pred += w\n",
    "    total_reward += pred * 0.20\n",
    "\n",
    "    # 2) Technical analysis (15%)\n",
    "    tech = 0.0\n",
    "    terms = ['rsi','macd','moving average','bollinger bands','fibonacci','support level','resistance level','breakout','consolidation','overbought','oversold','bullish divergence','bearish divergence']\n",
    "    tech += min(0.15, sum(0.03 for t in terms if t in rl))\n",
    "    patt = ['head and shoulders','double top','double bottom','triangle','flag','pennant','cup and handle','ascending','descending']\n",
    "    tech += min(0.08, sum(0.02 for t in patt if t in rl))\n",
    "    total_reward += tech * 0.15\n",
    "\n",
    "    # 3) News integration (15%)\n",
    "    news = 0.0\n",
    "    nts = ['news impact','market sentiment','adoption news','regulatory','institutional','whale activity','exchange','etf','mainstream','correlation with','influenced by','driven by news']\n",
    "    news += min(0.12, sum(0.03 for t in nts if t in rl))\n",
    "    if any(t in rl for t in ['multiple factors','combined with','along with','considering','taking into account','综合考虑','holistic','comprehensive']):\n",
    "        news += 0.08\n",
    "    total_reward += news * 0.15\n",
    "\n",
    "    # 4) Specificity (10%)\n",
    "    spec = 0.0\n",
    "    price_targets = re.findall(r'\\$\\d+[,\\d]*', response)\n",
    "    pct_targets = re.findall(r'\\d+\\.?\\d*%', response)\n",
    "    if price_targets or pct_targets: spec += 0.10\n",
    "    tfs = ['within.*day','next.*week','coming.*month','short.*term','long.*term','by.*end','Q[1-4]','next.*year']\n",
    "    if sum(1 for tf in tfs if re.search(tf, rl)) > 0: spec += 0.05\n",
    "    total_reward += spec * 0.10\n",
    "\n",
    "    # 5) Structured output (25%)\n",
    "    structured = 0.0\n",
    "    rj = parse_trading_output(response)\n",
    "    gj = parse_trading_output(ground_truth) if ground_truth else None\n",
    "    if rj:\n",
    "        structured += 0.05\n",
    "        req = ['action','confidence','stop_loss','take_profit','forecast_10d']\n",
    "        present = sum(1 for f in req if f in rj)\n",
    "        structured += (present/len(req)) * 0.10\n",
    "        if gj:\n",
    "            if rj.get('action','').upper() == gj.get('action','').upper():\n",
    "                structured += 0.08\n",
    "            rc, gc = rj.get('confidence',0), gj.get('confidence',0)\n",
    "            if isinstance(rc,(int,float)) and isinstance(gc,(int,float)):\n",
    "                structured += max(0, 1-abs(rc-gc)/100) * 0.06\n",
    "            for fld in ['stop_loss','take_profit']:\n",
    "                rp, gp = rj.get(fld,0), gj.get(fld,0)\n",
    "                if isinstance(rp,(int,float)) and isinstance(gp,(int,float)) and gp>0:\n",
    "                    structured += max(0, 1-abs((rp-gp)/gp)) * 0.04\n",
    "            rf, gf = rj.get('forecast_10d',[]), gj.get('forecast_10d',[])\n",
    "            if isinstance(rf,list) and isinstance(gf,list) and len(gf)>0:\n",
    "                structured += calculate_forecast_similarity(rf, gf) * 0.08\n",
    "        if 'confidence' in rj:\n",
    "            c = rj.get('confidence',0)\n",
    "            if isinstance(c,(int,float)) and 0 <= c <= 100: structured += 0.02\n",
    "        for fld in ['stop_loss','take_profit']:\n",
    "            p = rj.get(fld,0)\n",
    "            if isinstance(p,(int,float)) and p>0: structured += 0.01\n",
    "    else:\n",
    "        if ground_truth:\n",
    "            rt = set(response.lower().split())\n",
    "            gt = set(ground_truth.lower().split())\n",
    "            if len(gt)>0:\n",
    "                structured += (len(rt & gt)/len(rt | gt)) * 0.15\n",
    "    total_reward += structured * 0.25\n",
    "\n",
    "    # 6) AI reward (10%)\n",
    "    if reward_model is not None and reward_tokenizer is not None:\n",
    "        try:\n",
    "            text = f\"Bitcoin Prediction Analysis: {response[:400]}\"\n",
    "            inputs = reward_tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512, padding=True)\n",
    "            inputs = {k: v.to(next(reward_model.parameters()).device) for k,v in inputs.items()}\n",
    "            with torch.no_grad():\n",
    "                out = reward_model(**inputs)\n",
    "                logits = getattr(out, 'logits', None)\n",
    "                ai_r = torch.sigmoid(logits.mean()).item() if logits is not None else 0.5\n",
    "                total_reward += ai_r * 0.10\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    # 7) Bonuses\n",
    "    bonus = 0.0\n",
    "    if any(m in response for m in ['1.','2.','3.','•','-','Analysis:','Prediction:','Factors:']):\n",
    "        bonus += 0.06\n",
    "    if any(t in rl for t in ['not financial advice','high risk','volatile','dyor','past performance']):\n",
    "        bonus += 0.04\n",
    "    total_reward += bonus\n",
    "\n",
    "    # 8) Penalties\n",
    "    pen = 0.0\n",
    "    pen -= min(0.15, sum(0.03 for t in ['guaranteed','definitely will','certain','100%','no doubt'] if t in rl))\n",
    "    hedges = sum(1 for t in ['maybe','might','could be','possibly','perhaps'] if t in rl)\n",
    "    if hedges > 2: pen -= 0.05\n",
    "    total_reward += pen\n",
    "\n",
    "    return float(max(0.0, min(1.0, total_reward)))\n",
    "\n",
    "print(\"Reward function ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b5bd8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Trainer integrating the reward\n",
    "class CustomGRPOTrainer(GRPOTrainer):\n",
    "    def __init__(self, reward_model=None, reward_tokenizer=None, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.reward_model = reward_model\n",
    "        self.reward_tokenizer = reward_tokenizer\n",
    "\n",
    "    def compute_reward(self, model_output, reference_output=None):\n",
    "        try:\n",
    "            # model_output may be text already depending on TRL internals; be robust\n",
    "            if isinstance(model_output, str):\n",
    "                resp_text = model_output\n",
    "            elif hasattr(model_output, 'sequences'):\n",
    "                resp_text = self.tokenizer.decode(model_output.sequences[0], skip_special_tokens=True)\n",
    "            else:\n",
    "                resp_text = str(model_output)\n",
    "            ref_text = reference_output if isinstance(reference_output, str) else (str(reference_output) if reference_output is not None else None)\n",
    "            r = calculate_comprehensive_prediction_reward(resp_text, ref_text, self.reward_model, self.reward_tokenizer)\n",
    "            return torch.tensor([r], dtype=torch.float32)\n",
    "        except Exception as e:\n",
    "            print(\"Reward error:\", e)\n",
    "            return torch.tensor([0.5], dtype=torch.float32)\n",
    "\n",
    "print(\"CustomGRPOTrainer ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f639a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training setup\n",
    "args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    num_train_epochs=EPOCHS,\n",
    "    per_device_train_batch_size=PER_DEVICE_TRAIN_BATCH_SIZE,\n",
    "    gradient_accumulation_steps=GRAD_ACCUM_STEPS,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    logging_steps=10,\n",
    "    save_steps=100,\n",
    "    save_strategy=\"steps\",\n",
    "    evaluation_strategy=\"no\",\n",
    "    warmup_ratio=0.1,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    optim=\"adamw_8bit\",\n",
    "    weight_decay=0.01,\n",
    "    max_grad_norm=1.0,\n",
    "    fp16=(torch.cuda.is_available() and not torch.cuda.is_bf16_supported()),\n",
    "    bf16=(torch.cuda.is_available() and torch.cuda.is_bf16_supported()),\n",
    "    remove_unused_columns=False,\n",
    "    dataloader_num_workers=2,\n",
    "    seed=SEED,\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "# Formatting function for TRL GRPO\n",
    "\n",
    "def formatting_func(example):\n",
    "    return example[\"conversations\"]\n",
    "\n",
    "trainer = CustomGRPOTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_ds,\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=MAX_LENGTH,\n",
    "    max_prompt_length=MAX_PROMPT_LENGTH,\n",
    "    beta=BETA,\n",
    "    formatting_func=formatting_func,\n",
    "    reward_model=reward_model,\n",
    "    reward_tokenizer=reward_tokenizer,\n",
    ")\n",
    "\n",
    "print(\"Trainer initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a48ee9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replaced by GRPOConfig above\n",
    "print(\"Using TRL GRPOConfig for GRPO training.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21aaa37a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    "print(\"🚀 Starting GRPO training (no Unsloth)...\")\n",
    "start_time = datetime.now()\n",
    "train_stats = trainer.train()\n",
    "end_time = datetime.now()\n",
    "duration = end_time - start_time\n",
    "print(\"🎉 Training complete.\")\n",
    "print(\"Final loss:\", getattr(train_stats, 'training_loss', None))\n",
    "print(\"Steps:\", getattr(train_stats, 'global_step', None))\n",
    "print(\"Duration:\", duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dffdda8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model and summary\n",
    "save_dir = f\"{OUTPUT_DIR}/final_model\"\n",
    "model.save_pretrained(save_dir)\n",
    "tokenizer.save_pretrained(save_dir)\n",
    "print(\"Saved model to\", save_dir)\n",
    "\n",
    "summary = {\n",
    "    \"model\": BASE_MODEL,\n",
    "    \"adapter\": f\"{ADAPTER_PATH}/{CHECKPOINT}\",\n",
    "    \"dataset\": DATASET_NAME,\n",
    "    \"samples\": len(train_ds),\n",
    "    \"epochs\": EPOCHS,\n",
    "    \"lr\": LEARNING_RATE,\n",
    "    \"beta\": BETA,\n",
    "    \"max_length\": MAX_LENGTH,\n",
    "    \"max_prompt_length\": MAX_PROMPT_LENGTH,\n",
    "    \"time\": {\n",
    "        \"start\": start_time.isoformat(),\n",
    "        \"end\": end_time.isoformat(),\n",
    "        \"duration\": str(duration),\n",
    "    },\n",
    "}\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "with open(os.path.join(OUTPUT_DIR, \"training_summary.json\"), \"w\") as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "print(\"Summary saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd8ca1b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick inference test (structured output)\n",
    "model.eval()\n",
    "\n",
    "def chat(messages):\n",
    "    # Simple prompt builder (no Unsloth template); Qwen supports chat format tokens but we'll use plain concat\n",
    "    prompt = \"\\n\".join([f\"{m['role'].upper()}: {m['content']}\" for m in messages]) + \"\\nASSISTANT:\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    with torch.no_grad():\n",
    "        out = model.generate(**inputs, max_new_tokens=256, temperature=0.7, do_sample=True, pad_token_id=tokenizer.eos_token_id, eos_token_id=tokenizer.eos_token_id)\n",
    "    text = tokenizer.decode(out[0][inputs.input_ids.shape[1]:], skip_special_tokens=True)\n",
    "    return text\n",
    "\n",
    "messages = [\n",
    "    {\"role\":\"system\",\"content\":\"You are an expert Bitcoin market analyst. Output ONLY valid JSON with keys action, confidence, stop_loss, take_profit, forecast_10d (10 numbers).\"},\n",
    "    {\"role\":\"user\",\"content\":\"Give your Bitcoin trading decision for the next 10 days with stop loss, take profit and a 10-day price forecast.\"}\n",
    "]\n",
    "resp = chat(messages)\n",
    "print(\"Model raw response:\\n\", resp)\n",
    "print(\"Parsed:\", parse_trading_output(resp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dfe8814",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRPO reward function wrapper\n",
    "# Tries to handle different shapes passed by TRL\n",
    "\n",
    "def grpo_reward_func(texts=None, prompts=None, samples=None, **kwargs):\n",
    "    rewards = []\n",
    "    if texts is not None and isinstance(texts, list) and (len(texts) == 0 or isinstance(texts[0], str)):\n",
    "        # Only model responses provided\n",
    "        for resp in (texts or []):\n",
    "            rewards.append(calculate_comprehensive_prediction_reward(resp or \"\", ground_truth=\"\", reward_model=reward_model, reward_tokenizer=reward_tokenizer))\n",
    "        return rewards\n",
    "    \n",
    "    if samples is not None and isinstance(samples, list):\n",
    "        for s in samples:\n",
    "            # Try to extract response and ground truth\n",
    "            resp = s.get('response') or s.get('text') or s.get('output') or \"\"\n",
    "            gt = s.get('ground_truth') or \"\"\n",
    "            # If conversations exist, last assistant is GT\n",
    "            conv = s.get('conversations')\n",
    "            if not gt and isinstance(conv, list) and len(conv) > 0 and isinstance(conv[-1], dict) and conv[-1].get('role') == 'assistant':\n",
    "                gt = conv[-1].get('content', '')\n",
    "            rewards.append(calculate_comprehensive_prediction_reward(resp, gt, reward_model=reward_model, reward_tokenizer=reward_tokenizer))\n",
    "        return rewards\n",
    "    \n",
    "    # Fallback\n",
    "    return [0.5] * (len(texts) if isinstance(texts, list) else 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5358bcfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attach reward function if trainer supports it\n",
    "try:\n",
    "    trainer.reward_func = grpo_reward_func\n",
    "    print(\"Custom reward function attached to trainer.\")\n",
    "except Exception as e:\n",
    "    print(\"Could not attach reward function explicitly:\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03e6a501",
   "metadata": {},
   "source": [
    "## How to use\n",
    "\n",
    "1) Optionally install dependencies in Cell 2.\n",
    "2) Run imports and config cells (Cells 3–5).\n",
    "3) Run dataset load (Cell 6).\n",
    "4) Run reward function cells (Cells 7–8).\n",
    "5) Initialize trainer (Cell 9), then attach reward (Cell 14).\n",
    "6) Train (Cell 10), save (Cell 11), and test inference (Cell 12).\n",
    "\n",
    "Note: This notebook does not use Unsloth anywhere."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a01ab9ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use TRL GRPOConfig instead of HF TrainingArguments\n",
    "train_config = GRPOConfig(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    num_train_epochs=EPOCHS,\n",
    "    per_device_train_batch_size=PER_DEVICE_TRAIN_BATCH_SIZE,\n",
    "    gradient_accumulation_steps=GRAD_ACCUM_STEPS,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    logging_steps=10,\n",
    "    save_steps=100,\n",
    "    save_strategy=\"steps\",\n",
    "    evaluation_strategy=\"no\",\n",
    "    warmup_ratio=0.1,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    optim=\"adamw_8bit\",\n",
    "    weight_decay=0.01,\n",
    "    max_grad_norm=1.0,\n",
    "    fp16=(torch.cuda.is_available() and not torch.cuda.is_bf16_supported()),\n",
    "    bf16=(torch.cuda.is_available() and torch.cuda.is_bf16_supported()),\n",
    "    dataloader_num_workers=2,\n",
    "    seed=SEED,\n",
    "    report_to=[]\n",
    ")\n",
    "\n",
    "# Recreate trainer with GRPOConfig\n",
    "trainer = CustomGRPOTrainer(\n",
    "    model=model,\n",
    "    args=train_config,\n",
    "    train_dataset=train_ds,\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=MAX_LENGTH,\n",
    "    max_prompt_length=MAX_PROMPT_LENGTH,\n",
    "    beta=BETA,\n",
    "    formatting_func=formatting_func,\n",
    "    reward_model=reward_model,\n",
    "    reward_tokenizer=reward_tokenizer,\n",
    ")\n",
    "print(\"Trainer re-initialized with TRL GRPOConfig\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
