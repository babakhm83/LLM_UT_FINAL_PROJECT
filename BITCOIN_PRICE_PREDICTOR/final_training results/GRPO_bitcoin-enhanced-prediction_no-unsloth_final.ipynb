{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01d83efe",
   "metadata": {},
   "source": [
    "# GRPO Training for Bitcoin Enhanced Prediction (No Unsloth)\n",
    "\n",
    "This notebook performs GRPO training using Hugging Face Transformers, TRL, PEFT, and bitsandbytes â€” without Unsloth. It loads a pre-trained LoRA adapter and optimizes it with a structured-output reward.\n",
    "\n",
    "- Base model: Qwen 2.5 Instruct (4-bit quantized)\n",
    "- Adapter: tahamajs/my-awesome-model_final_bitcoin_enhanced_prediction_dataset_with_local_comprehensive_news (checkpoint-1152)\n",
    "- Dataset: tahamajs/bitcoin-enhanced-prediction-dataset-with-local-comprehensive-news\n",
    "- Reward: Structured JSON action/confidence/SL/TP/forecast closeness + analysis quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef81794c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional installs (uncomment if needed)\n",
    "# !pip install -U transformers accelerate trl peft bitsandbytes datasets safetensors\n",
    "# !pip install -U einops sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb313c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, random, re\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from peft import PeftModel, LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from trl import GRPOTrainer, GRPOConfig\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb951d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config\n",
    "BASE_MODEL = \"Qwen/Qwen2.5-7B-Instruct\"\n",
    "LOAD_IN_4BIT = True\n",
    "ADAPTER_PATH = \"tahamajs/my-awesome-model_final_bitcoin_enhanced_prediction_dataset_with_local_comprehensive_news\"\n",
    "CHECKPOINT = \"checkpoint-1152\"\n",
    "MAX_SEQ_LEN = 2048\n",
    "MAX_LENGTH = 1024\n",
    "MAX_PROMPT_LENGTH = 512\n",
    "OUTPUT_DIR = \"./qwen_bitcoin_enhanced_grpo_no_unsloth\"\n",
    "LEARNING_RATE = 3e-7\n",
    "EPOCHS = 1\n",
    "BETA = 0.1\n",
    "PER_DEVICE_TRAIN_BATCH_SIZE = 1\n",
    "GRAD_ACCUM_STEPS = 8\n",
    "\n",
    "DATASET_NAME = \"tahamajs/bitcoin-enhanced-prediction-dataset-with-local-comprehensive-news\"\n",
    "REWARD_MODEL_NAME = \"microsoft/DialoGPT-medium\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a517ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load base model in 4bit\n",
    "quant_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=LOAD_IN_4BIT,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16 if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else torch.float16,\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL, use_fast=True)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL,\n",
    "    quantization_config=quant_config if LOAD_IN_4BIT else None,\n",
    "    torch_dtype=torch.bfloat16 if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else torch.float16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "# Attach (and continue training) LoRA\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "lora_cfg = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.0,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "model = get_peft_model(model, lora_cfg)\n",
    "\n",
    "# Load pre-trained adapter\n",
    "try:\n",
    "    adapter_path = f\"{ADAPTER_PATH}/{CHECKPOINT}\"\n",
    "    model = PeftModel.from_pretrained(model, adapter_path)\n",
    "    print(f\"Loaded adapter from {adapter_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"Warning: couldn't load adapter: {e}\")\n",
    "\n",
    "# Load reward model (optional)\n",
    "try:\n",
    "    reward_tokenizer = AutoTokenizer.from_pretrained(REWARD_MODEL_NAME)\n",
    "    reward_model = AutoModelForCausalLM.from_pretrained(REWARD_MODEL_NAME, device_map=\"auto\")\n",
    "    reward_model.eval()\n",
    "    print(\"Reward model loaded\")\n",
    "except Exception as e:\n",
    "    print(f\"Reward model unavailable, fallback to rule-based only: {e}\")\n",
    "    reward_model, reward_tokenizer = None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9556c023",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "raw_ds = load_dataset(DATASET_NAME, split=\"train\")\n",
    "print(f\"Loaded dataset {DATASET_NAME} with {len(raw_ds):,} samples\")\n",
    "\n",
    "# Convert to TRL chat format: list of dicts with 'conversations'\n",
    "def to_conversations(examples):\n",
    "    instructions = examples.get(\"instruction\", [\"\"] * len(examples.get(\"input\", [])))\n",
    "    inputs = examples.get(\"input\", [])\n",
    "    outputs = examples.get(\"output\", [])\n",
    "    chats = []\n",
    "    for inst, inp, out in zip(instructions, inputs, outputs):\n",
    "        chats.append([\n",
    "            {\"role\": \"system\", \"content\": inst or \"You are a helpful Bitcoin market analyst.\"},\n",
    "            {\"role\": \"user\", \"content\": inp or \"\"},\n",
    "            {\"role\": \"assistant\", \"content\": out or \"\"},\n",
    "        ])\n",
    "    return {\"conversations\": chats}\n",
    "\n",
    "train_ds = raw_ds.map(to_conversations, batched=True, remove_columns=raw_ds.column_names)\n",
    "print(\"Example conv:\")\n",
    "for m in train_ds[0][\"conversations\"]:\n",
    "    print(m[\"role\"], m[\"content\"][:120])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2524baa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Structured-output helpers\n",
    "\n",
    "def parse_trading_output(text):\n",
    "    if not text:\n",
    "        return None\n",
    "    try:\n",
    "        return json.loads(text.strip())\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        pattern = r'\\{[^{}]*\"action\"[^{}]*\\}'\n",
    "        matches = re.findall(pattern, text, re.IGNORECASE | re.DOTALL)\n",
    "        if matches:\n",
    "            for m in matches:\n",
    "                try:\n",
    "                    return json.loads(m.strip())\n",
    "                except:\n",
    "                    continue\n",
    "        result = {}\n",
    "        m = re.search(r'\"action\"\\s*:\\s*\"([^\"]+)\"', text, re.IGNORECASE)\n",
    "        if m: result['action'] = m.group(1).upper()\n",
    "        m = re.search(r'\"confidence\"\\s*:\\s*(\\d+(?:\\.\\d+)?)', text, re.IGNORECASE)\n",
    "        if m: result['confidence'] = float(m.group(1))\n",
    "        m = re.search(r'\"stop_loss\"\\s*:\\s*(\\d+(?:\\.\\d+)?)', text, re.IGNORECASE)\n",
    "        if m: result['stop_loss'] = float(m.group(1))\n",
    "        m = re.search(r'\"take_profit\"\\s*:\\s*(\\d+(?:\\.\\d+)?)', text, re.IGNORECASE)\n",
    "        if m: result['take_profit'] = float(m.group(1))\n",
    "        m = re.search(r'\"forecast_10d\"\\s*:\\s*\\[([^\\]]+)\\]', text, re.IGNORECASE)\n",
    "        if m:\n",
    "            try:\n",
    "                vals = [float(x.strip()) for x in m.group(1).split(',')]\n",
    "                result['forecast_10d'] = vals\n",
    "            except:\n",
    "                pass\n",
    "        return result if result else None\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "\n",
    "def calculate_forecast_similarity(resp_forecast, gt_forecast):\n",
    "    if not resp_forecast or not gt_forecast: return 0.0\n",
    "    import numpy as np\n",
    "    try:\n",
    "        r = np.array([float(x) for x in resp_forecast if isinstance(x, (int, float, str))])\n",
    "        g = np.array([float(x) for x in gt_forecast if isinstance(x, (int, float, str))])\n",
    "        if len(r) == 0 or len(g) == 0: return 0.0\n",
    "        n = min(len(r), len(g))\n",
    "        r, g = r[:n], g[:n]\n",
    "        if n < 2: return 0.0\n",
    "        score = 0.0\n",
    "        # corr (40%)\n",
    "        try:\n",
    "            corr = np.corrcoef(r, g)[0,1]\n",
    "            if not np.isnan(corr): score += abs(corr) * 0.4\n",
    "        except: pass\n",
    "        # directional (30%)\n",
    "        rd, gd = np.diff(r) > 0, np.diff(g) > 0\n",
    "        if len(rd) > 0: score += (np.mean(rd == gd) * 0.3)\n",
    "        # magnitude (30%)\n",
    "        try:\n",
    "            rn = (r - r.mean()) / (r.std() + 1e-8)\n",
    "            gn = (g - g.mean()) / (g.std() + 1e-8)\n",
    "            mse = ((rn - gn) ** 2).mean()\n",
    "            mag = max(0, 1 - (mse / 4))\n",
    "            score += mag * 0.3\n",
    "        except: pass\n",
    "        return float(min(1.0, max(0.0, score)))\n",
    "    except:\n",
    "        return 0.0\n",
    "\n",
    "\n",
    "def calculate_comprehensive_prediction_reward(response, ground_truth, reward_model=None, reward_tokenizer=None):\n",
    "    total_reward = 0.0\n",
    "    # 1) Prediction quality (20%)\n",
    "    pred = 0.0\n",
    "    L = len(response)\n",
    "    if 150 <= L <= 1000: pred += 0.15\n",
    "    elif 100 <= L <= 1200: pred += 0.10\n",
    "    elif L < 100: pred -= 0.05\n",
    "    kw = {\n",
    "        'prediction': 0.08,'forecast':0.08,'analysis':0.06,'market':0.06,\n",
    "        'bitcoin':0.05,'btc':0.04,'price':0.07,'trend':0.06,\n",
    "        'technical':0.05,'fundamental':0.05,'news':0.05,'data':0.04,\n",
    "        'indicators':0.05,'volume':0.04,'resistance':0.04,'support':0.04,\n",
    "        'momentum':0.04,'correlation':0.03,'pattern':0.04,'signal':0.04\n",
    "    }\n",
    "    rl = response.lower()\n",
    "    for k,w in kw.items():\n",
    "        if k in rl: pred += w\n",
    "    total_reward += pred * 0.20\n",
    "\n",
    "    # 2) Technical analysis (15%)\n",
    "    tech = 0.0\n",
    "    terms = ['rsi','macd','moving average','bollinger bands','fibonacci','support level','resistance level','breakout','consolidation','overbought','oversold','bullish divergence','bearish divergence']\n",
    "    tech += min(0.15, sum(0.03 for t in terms if t in rl))\n",
    "    patt = ['head and shoulders','double top','double bottom','triangle','flag','pennant','cup and handle','ascending','descending']\n",
    "    tech += min(0.08, sum(0.02 for t in patt if t in rl))\n",
    "    total_reward += tech * 0.15\n",
    "\n",
    "    # 3) News integration (15%)\n",
    "    news = 0.0\n",
    "    nts = ['news impact','market sentiment','adoption news','regulatory','institutional','whale activity','exchange','etf','mainstream','correlation with','influenced by','driven by news']\n",
    "    news += min(0.12, sum(0.03 for t in nts if t in rl))\n",
    "    if any(t in rl for t in ['multiple factors','combined with','along with','considering','taking into account','ç»¼åˆè€ƒè™‘','holistic','comprehensive']):\n",
    "        news += 0.08\n",
    "    total_reward += news * 0.15\n",
    "\n",
    "    # 4) Specificity (10%)\n",
    "    spec = 0.0\n",
    "    price_targets = re.findall(r'\\$\\d+[,\\d]*', response)\n",
    "    pct_targets = re.findall(r'\\d+\\.?\\d*%', response)\n",
    "    if price_targets or pct_targets: spec += 0.10\n",
    "    tfs = ['within.*day','next.*week','coming.*month','short.*term','long.*term','by.*end','Q[1-4]','next.*year']\n",
    "    if sum(1 for tf in tfs if re.search(tf, rl)) > 0: spec += 0.05\n",
    "    total_reward += spec * 0.10\n",
    "\n",
    "    # 5) Structured output (25%)\n",
    "    structured = 0.0\n",
    "    rj = parse_trading_output(response)\n",
    "    gj = parse_trading_output(ground_truth) if ground_truth else None\n",
    "    if rj:\n",
    "        structured += 0.05\n",
    "        req = ['action','confidence','stop_loss','take_profit','forecast_10d']\n",
    "        present = sum(1 for f in req if f in rj)\n",
    "        structured += (present/len(req)) * 0.10\n",
    "        if gj:\n",
    "            if rj.get('action','').upper() == gj.get('action','').upper():\n",
    "                structured += 0.08\n",
    "            rc, gc = rj.get('confidence',0), gj.get('confidence',0)\n",
    "            if isinstance(rc,(int,float)) and isinstance(gc,(int,float)):\n",
    "                structured += max(0, 1-abs(rc-gc)/100) * 0.06\n",
    "            for fld in ['stop_loss','take_profit']:\n",
    "                rp, gp = rj.get(fld,0), gj.get(fld,0)\n",
    "                if isinstance(rp,(int,float)) and isinstance(gp,(int,float)) and gp>0:\n",
    "                    structured += max(0, 1-abs((rp-gp)/gp)) * 0.04\n",
    "            rf, gf = rj.get('forecast_10d',[]), gj.get('forecast_10d',[])\n",
    "            if isinstance(rf,list) and isinstance(gf,list) and len(gf)>0:\n",
    "                structured += calculate_forecast_similarity(rf, gf) * 0.08\n",
    "        if 'confidence' in rj:\n",
    "            c = rj.get('confidence',0)\n",
    "            if isinstance(c,(int,float)) and 0 <= c <= 100: structured += 0.02\n",
    "        for fld in ['stop_loss','take_profit']:\n",
    "            p = rj.get(fld,0)\n",
    "            if isinstance(p,(int,float)) and p>0: structured += 0.01\n",
    "    else:\n",
    "        if ground_truth:\n",
    "            rt = set(response.lower().split())\n",
    "            gt = set(ground_truth.lower().split())\n",
    "            if len(gt)>0:\n",
    "                structured += (len(rt & gt)/len(rt | gt)) * 0.15\n",
    "    total_reward += structured * 0.25\n",
    "\n",
    "    # 6) AI reward (10%)\n",
    "    if reward_model is not None and reward_tokenizer is not None:\n",
    "        try:\n",
    "            text = f\"Bitcoin Prediction Analysis: {response[:400]}\"\n",
    "            inputs = reward_tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512, padding=True)\n",
    "            inputs = {k: v.to(next(reward_model.parameters()).device) for k,v in inputs.items()}\n",
    "            with torch.no_grad():\n",
    "                out = reward_model(**inputs)\n",
    "                logits = getattr(out, 'logits', None)\n",
    "                ai_r = torch.sigmoid(logits.mean()).item() if logits is not None else 0.5\n",
    "                total_reward += ai_r * 0.10\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    # 7) Bonuses\n",
    "    bonus = 0.0\n",
    "    if any(m in response for m in ['1.','2.','3.','â€¢','-','Analysis:','Prediction:','Factors:']):\n",
    "        bonus += 0.06\n",
    "    if any(t in rl for t in ['not financial advice','high risk','volatile','dyor','past performance']):\n",
    "        bonus += 0.04\n",
    "    total_reward += bonus\n",
    "\n",
    "    # 8) Penalties\n",
    "    pen = 0.0\n",
    "    pen -= min(0.15, sum(0.03 for t in ['guaranteed','definitely will','certain','100%','no doubt'] if t in rl))\n",
    "    hedges = sum(1 for t in ['maybe','might','could be','possibly','perhaps'] if t in rl)\n",
    "    if hedges > 2: pen -= 0.05\n",
    "    total_reward += pen\n",
    "\n",
    "    return float(max(0.0, min(1.0, total_reward)))\n",
    "\n",
    "print(\"Reward function ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b5bd8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Trainer integrating the reward\n",
    "class CustomGRPOTrainer(GRPOTrainer):\n",
    "    def __init__(self, reward_model=None, reward_tokenizer=None, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.reward_model = reward_model\n",
    "        self.reward_tokenizer = reward_tokenizer\n",
    "\n",
    "    def compute_reward(self, model_output, reference_output=None):\n",
    "        try:\n",
    "            # model_output may be text already depending on TRL internals; be robust\n",
    "            if isinstance(model_output, str):\n",
    "                resp_text = model_output\n",
    "            elif hasattr(model_output, 'sequences'):\n",
    "                resp_text = self.tokenizer.decode(model_output.sequences[0], skip_special_tokens=True)\n",
    "            else:\n",
    "                resp_text = str(model_output)\n",
    "            ref_text = reference_output if isinstance(reference_output, str) else (str(reference_output) if reference_output is not None else None)\n",
    "            r = calculate_comprehensive_prediction_reward(resp_text, ref_text, self.reward_model, self.reward_tokenizer)\n",
    "            return torch.tensor([r], dtype=torch.float32)\n",
    "        except Exception as e:\n",
    "            print(\"Reward error:\", e)\n",
    "            return torch.tensor([0.5], dtype=torch.float32)\n",
    "\n",
    "print(\"CustomGRPOTrainer ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f639a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training setup\n",
    "args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    num_train_epochs=EPOCHS,\n",
    "    per_device_train_batch_size=PER_DEVICE_TRAIN_BATCH_SIZE,\n",
    "    gradient_accumulation_steps=GRAD_ACCUM_STEPS,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    logging_steps=10,\n",
    "    save_steps=100,\n",
    "    save_strategy=\"steps\",\n",
    "    evaluation_strategy=\"no\",\n",
    "    warmup_ratio=0.1,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    optim=\"adamw_8bit\",\n",
    "    weight_decay=0.01,\n",
    "    max_grad_norm=1.0,\n",
    "    fp16=(torch.cuda.is_available() and not torch.cuda.is_bf16_supported()),\n",
    "    bf16=(torch.cuda.is_available() and torch.cuda.is_bf16_supported()),\n",
    "    remove_unused_columns=False,\n",
    "    dataloader_num_workers=2,\n",
    "    seed=SEED,\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "# Formatting function for TRL GRPO\n",
    "\n",
    "def formatting_func(example):\n",
    "    return example[\"conversations\"]\n",
    "\n",
    "trainer = CustomGRPOTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_ds,\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=MAX_LENGTH,\n",
    "    max_prompt_length=MAX_PROMPT_LENGTH,\n",
    "    beta=BETA,\n",
    "    formatting_func=formatting_func,\n",
    "    reward_model=reward_model,\n",
    "    reward_tokenizer=reward_tokenizer,\n",
    ")\n",
    "\n",
    "print(\"Trainer initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a48ee9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replaced by GRPOConfig above\n",
    "print(\"Using TRL GRPOConfig for GRPO training.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21aaa37a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    "print(\"ðŸš€ Starting GRPO training (no Unsloth)...\")\n",
    "start_time = datetime.now()\n",
    "train_stats = trainer.train()\n",
    "end_time = datetime.now()\n",
    "duration = end_time - start_time\n",
    "print(\"ðŸŽ‰ Training complete.\")\n",
    "print(\"Final loss:\", getattr(train_stats, 'training_loss', None))\n",
    "print(\"Steps:\", getattr(train_stats, 'global_step', None))\n",
    "print(\"Duration:\", duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dffdda8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model and summary\n",
    "save_dir = f\"{OUTPUT_DIR}/final_model\"\n",
    "model.save_pretrained(save_dir)\n",
    "tokenizer.save_pretrained(save_dir)\n",
    "print(\"Saved model to\", save_dir)\n",
    "\n",
    "summary = {\n",
    "    \"model\": BASE_MODEL,\n",
    "    \"adapter\": f\"{ADAPTER_PATH}/{CHECKPOINT}\",\n",
    "    \"dataset\": DATASET_NAME,\n",
    "    \"samples\": len(train_ds),\n",
    "    \"epochs\": EPOCHS,\n",
    "    \"lr\": LEARNING_RATE,\n",
    "    \"beta\": BETA,\n",
    "    \"max_length\": MAX_LENGTH,\n",
    "    \"max_prompt_length\": MAX_PROMPT_LENGTH,\n",
    "    \"time\": {\n",
    "        \"start\": start_time.isoformat(),\n",
    "        \"end\": end_time.isoformat(),\n",
    "        \"duration\": str(duration),\n",
    "    },\n",
    "}\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "with open(os.path.join(OUTPUT_DIR, \"training_summary.json\"), \"w\") as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "print(\"Summary saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd8ca1b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick inference test (structured output)\n",
    "model.eval()\n",
    "\n",
    "def chat(messages):\n",
    "    # Simple prompt builder (no Unsloth template); Qwen supports chat format tokens but we'll use plain concat\n",
    "    prompt = \"\\n\".join([f\"{m['role'].upper()}: {m['content']}\" for m in messages]) + \"\\nASSISTANT:\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    with torch.no_grad():\n",
    "        out = model.generate(**inputs, max_new_tokens=256, temperature=0.7, do_sample=True, pad_token_id=tokenizer.eos_token_id, eos_token_id=tokenizer.eos_token_id)\n",
    "    text = tokenizer.decode(out[0][inputs.input_ids.shape[1]:], skip_special_tokens=True)\n",
    "    return text\n",
    "\n",
    "messages = [\n",
    "    {\"role\":\"system\",\"content\":\"You are an expert Bitcoin market analyst. Output ONLY valid JSON with keys action, confidence, stop_loss, take_profit, forecast_10d (10 numbers).\"},\n",
    "    {\"role\":\"user\",\"content\":\"Give your Bitcoin trading decision for the next 10 days with stop loss, take profit and a 10-day price forecast.\"}\n",
    "]\n",
    "resp = chat(messages)\n",
    "print(\"Model raw response:\\n\", resp)\n",
    "print(\"Parsed:\", parse_trading_output(resp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dfe8814",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRPO reward function wrapper\n",
    "# Tries to handle different shapes passed by TRL\n",
    "\n",
    "def grpo_reward_func(texts=None, prompts=None, samples=None, **kwargs):\n",
    "    rewards = []\n",
    "    if texts is not None and isinstance(texts, list) and (len(texts) == 0 or isinstance(texts[0], str)):\n",
    "        # Only model responses provided\n",
    "        for resp in (texts or []):\n",
    "            rewards.append(calculate_comprehensive_prediction_reward(resp or \"\", ground_truth=\"\", reward_model=reward_model, reward_tokenizer=reward_tokenizer))\n",
    "        return rewards\n",
    "    \n",
    "    if samples is not None and isinstance(samples, list):\n",
    "        for s in samples:\n",
    "            # Try to extract response and ground truth\n",
    "            resp = s.get('response') or s.get('text') or s.get('output') or \"\"\n",
    "            gt = s.get('ground_truth') or \"\"\n",
    "            # If conversations exist, last assistant is GT\n",
    "            conv = s.get('conversations')\n",
    "            if not gt and isinstance(conv, list) and len(conv) > 0 and isinstance(conv[-1], dict) and conv[-1].get('role') == 'assistant':\n",
    "                gt = conv[-1].get('content', '')\n",
    "            rewards.append(calculate_comprehensive_prediction_reward(resp, gt, reward_model=reward_model, reward_tokenizer=reward_tokenizer))\n",
    "        return rewards\n",
    "    \n",
    "    # Fallback\n",
    "    return [0.5] * (len(texts) if isinstance(texts, list) else 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5358bcfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attach reward function if trainer supports it\n",
    "try:\n",
    "    trainer.reward_func = grpo_reward_func\n",
    "    print(\"Custom reward function attached to trainer.\")\n",
    "except Exception as e:\n",
    "    print(\"Could not attach reward function explicitly:\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03e6a501",
   "metadata": {},
   "source": [
    "## How to use\n",
    "\n",
    "1) Optionally install dependencies in Cell 2.\n",
    "2) Run imports and config cells (Cells 3â€“5).\n",
    "3) Run dataset load (Cell 6).\n",
    "4) Run reward function cells (Cells 7â€“8).\n",
    "5) Initialize trainer (Cell 9), then attach reward (Cell 14).\n",
    "6) Train (Cell 10), save (Cell 11), and test inference (Cell 12).\n",
    "\n",
    "Note: This notebook does not use Unsloth anywhere."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a01ab9ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use TRL GRPOConfig instead of HF TrainingArguments\n",
    "train_config = GRPOConfig(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    num_train_epochs=EPOCHS,\n",
    "    per_device_train_batch_size=PER_DEVICE_TRAIN_BATCH_SIZE,\n",
    "    gradient_accumulation_steps=GRAD_ACCUM_STEPS,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    logging_steps=10,\n",
    "    save_steps=100,\n",
    "    save_strategy=\"steps\",\n",
    "    evaluation_strategy=\"no\",\n",
    "    warmup_ratio=0.1,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    optim=\"adamw_8bit\",\n",
    "    weight_decay=0.01,\n",
    "    max_grad_norm=1.0,\n",
    "    fp16=(torch.cuda.is_available() and not torch.cuda.is_bf16_supported()),\n",
    "    bf16=(torch.cuda.is_available() and torch.cuda.is_bf16_supported()),\n",
    "    dataloader_num_workers=2,\n",
    "    seed=SEED,\n",
    "    report_to=[]\n",
    ")\n",
    "\n",
    "# Recreate trainer with GRPOConfig\n",
    "trainer = CustomGRPOTrainer(\n",
    "    model=model,\n",
    "    args=train_config,\n",
    "    train_dataset=train_ds,\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=MAX_LENGTH,\n",
    "    max_prompt_length=MAX_PROMPT_LENGTH,\n",
    "    beta=BETA,\n",
    "    formatting_func=formatting_func,\n",
    "    reward_model=reward_model,\n",
    "    reward_tokenizer=reward_tokenizer,\n",
    ")\n",
    "print(\"Trainer re-initialized with TRL GRPOConfig\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
