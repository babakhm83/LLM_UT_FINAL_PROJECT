{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b7eb871",
   "metadata": {
    "papermill": {
     "duration": 0.012057,
     "end_time": "2025-09-11T14:36:54.742679",
     "exception": false,
     "start_time": "2025-09-11T14:36:54.730622",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Unsloth GRPO Training for Bitcoin Enhanced Prediction\n",
    "\n",
    "This notebook implements Group Relative Policy Optimization (GRPO) using Unsloth for comprehensive Bitcoin prediction.\n",
    "\n",
    "**Dataset**: `bitcoin-enhanced-prediction-dataset-with-local-comprehensive-news`\n",
    "\n",
    "**Training Method**: Unsloth GRPO\n",
    "- Built-in preference learning optimization\n",
    "- Efficient memory usage with Unsloth\n",
    "- Streamlined training pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c992054f",
   "metadata": {
    "papermill": {
     "duration": 0.006994,
     "end_time": "2025-09-11T14:36:54.760728",
     "exception": false,
     "start_time": "2025-09-11T14:36:54.753734",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Install Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fcd8a9cf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-11T14:36:54.783465Z",
     "iopub.status.busy": "2025-09-11T14:36:54.782819Z",
     "iopub.status.idle": "2025-09-11T14:36:54.789461Z",
     "shell.execute_reply": "2025-09-11T14:36:54.788410Z"
    },
    "papermill": {
     "duration": 0.0184,
     "end_time": "2025-09-11T14:36:54.792207",
     "exception": false,
     "start_time": "2025-09-11T14:36:54.773807",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install -U \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
    "# !pip install -U xformers trl peft accelerate bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "532b37b8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-11T14:36:54.817244Z",
     "iopub.status.busy": "2025-09-11T14:36:54.816697Z",
     "iopub.status.idle": "2025-09-11T14:36:54.821877Z",
     "shell.execute_reply": "2025-09-11T14:36:54.820972Z"
    },
    "papermill": {
     "duration": 0.020283,
     "end_time": "2025-09-11T14:36:54.823752",
     "exception": false,
     "start_time": "2025-09-11T14:36:54.803469",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ensure protobuf uses pure-Python implementation early to avoid descriptor errors\n",
    "# import os as _osThe code snippet you provided is setting environment variables using the `os.environ.setdefault()` method.\n",
    "\n",
    "# _os.environ.setdefault(\"PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION\", \"python\")\n",
    "# _os.environ.setdefault(\"TOKENIZERS_PARALLELISM\", \"false\")\n",
    "\n",
    "# !pip install -U \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
    "# !pip install -U xformers trl peft accelerate bitsandbytes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "393dee77",
   "metadata": {
    "papermill": {
     "duration": 0.009558,
     "end_time": "2025-09-11T14:36:54.840867",
     "exception": false,
     "start_time": "2025-09-11T14:36:54.831309",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7173964b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-11T14:36:54.863373Z",
     "iopub.status.busy": "2025-09-11T14:36:54.862813Z",
     "iopub.status.idle": "2025-09-11T14:37:12.846647Z",
     "shell.execute_reply": "2025-09-11T14:37:12.845103Z"
    },
    "papermill": {
     "duration": 17.998713,
     "end_time": "2025-09-11T14:37:12.849085",
     "exception": false,
     "start_time": "2025-09-11T14:36:54.850372",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/moein_salimi/users/babak/IdeaGeneration/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-11 18:07:10 [__init__.py:235] Automatically detected platform cuda.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ GRPO classes imported successfully\n"
     ]
    }
   ],
   "source": [
    "# from unsloth import FastLanguageModel, is_bfloat16_supported\n",
    "# from unsloth.chat_templates import get_chat_template\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import TrainingArguments, AutoTokenizer, AutoModel\n",
    "from peft import PeftModel\n",
    "import torch, random, os\n",
    "import json\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "# Try to import GRPO classes, with fallback to available alternatives\n",
    "try:\n",
    "    from trl import GRPOTrainer, GRPOConfig\n",
    "    GRPO_AVAILABLE = True\n",
    "    print(\"‚úÖ GRPO classes imported successfully\")\n",
    "except ImportError:\n",
    "    try:\n",
    "        # Try alternative imports (GRPO might be under different names)\n",
    "        from trl import PPOTrainer, PPOConfig\n",
    "        from trl import SFTTrainer, SFTConfig\n",
    "        GRPO_AVAILABLE = False\n",
    "        print(\"‚ö†Ô∏è GRPOTrainer not found, will use SFTTrainer as fallback\")\n",
    "    except ImportError:\n",
    "        # Last resort - use basic trainer\n",
    "        from transformers import Trainer\n",
    "        GRPO_AVAILABLE = False\n",
    "        print(\"‚ö†Ô∏è Advanced TRL classes not found, using basic Trainer\")\n",
    "\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae6f1a9",
   "metadata": {
    "papermill": {
     "duration": 0.009401,
     "end_time": "2025-09-11T14:37:12.874367",
     "exception": false,
     "start_time": "2025-09-11T14:37:12.864966",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "71a336c9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-11T14:37:12.894889Z",
     "iopub.status.busy": "2025-09-11T14:37:12.894133Z",
     "iopub.status.idle": "2025-09-11T14:37:12.904692Z",
     "shell.execute_reply": "2025-09-11T14:37:12.903367Z"
    },
    "papermill": {
     "duration": 0.025783,
     "end_time": "2025-09-11T14:37:12.909460",
     "exception": false,
     "start_time": "2025-09-11T14:37:12.883677",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Model configuration\n",
    "BASE_MODEL_NAME = \"./Qwen3-8B\"  # Preferred local base model (if available)\n",
    "FALLBACK_MODEL_NAME = \"unsloth/Qwen2.5-7B-Instruct-bnb-4bit\"  # Fallback HF model if local path missing\n",
    "ADAPTER_PATH = \"./my-awesome-model_final_bitcoin-enhanced-prediction-dataset-with-local-comprehensive-news-v2\"  # Pre-trained adapter (folder)\n",
    "CHECKPOINT = \"checkpoint-400\"  # Specific checkpoint within adapter folder\n",
    "MAX_SEQ_LENGTH = 2048\n",
    "DTYPE = torch.bfloat16  # Auto-detection\n",
    "LOAD_IN_4BIT = True\n",
    "\n",
    "# LoRA configuration\n",
    "LORA_R = 32\n",
    "LORA_ALPHA = 32\n",
    "LORA_DROPOUT = 0.0\n",
    "TARGET_MODULES = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"]\n",
    "\n",
    "# GRPO configuration\n",
    "OUTPUT_DIR = \"./qwen_bitcoin_enhanced_grpo_unsloth_pretrained_from_sft\"\n",
    "LEARNING_RATE = 3e-7  # Lower for pre-trained model\n",
    "NUM_TRAIN_EPOCHS = 1\n",
    "PER_DEVICE_TRAIN_BATCH_SIZE = 1\n",
    "GRADIENT_ACCUMULATION_STEPS = 8\n",
    "MAX_LENGTH = 1024\n",
    "MAX_PROMPT_LENGTH = 512\n",
    "BETA = 0.1\n",
    "\n",
    "# Quick sanity run controls (useful for CI or first launch)\n",
    "SANITY_RUN = False            # Set True to run a very short sanity training\n",
    "SANITY_MAX_STEPS = 30         # Number of steps for sanity run\n",
    "SANITY_DATASET_SIZE = 256     # Subset size for sanity run\n",
    "\n",
    "# Dataset\n",
    "DATASET_NAME = \"tahamajs/bitcoin-enhanced-prediction-dataset-with-local-comprehensive-news\"\n",
    "\n",
    "# Reward model for comprehensive analysis\n",
    "REWARD_MODEL_NAME = \"microsoft/DialoGPT-medium\"  # Good for conversational quality assessment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32b42590",
   "metadata": {
    "papermill": {
     "duration": 0.00633,
     "end_time": "2025-09-11T14:37:13.309206",
     "exception": false,
     "start_time": "2025-09-11T14:37:13.302876",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Load Model and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f29f5123",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-11T14:37:13.324408Z",
     "iopub.status.busy": "2025-09-11T14:37:13.323718Z",
     "iopub.status.idle": "2025-09-11T14:38:15.205522Z",
     "shell.execute_reply": "2025-09-11T14:38:15.204039Z"
    },
    "papermill": {
     "duration": 61.893401,
     "end_time": "2025-09-11T14:38:15.209046",
     "exception": false,
     "start_time": "2025-09-11T14:37:13.315645",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_160097/1638062879.py:5: UserWarning: WARNING: Unsloth should be imported before trl, transformers, peft to ensure all optimizations are applied. Your code may run slower or encounter memory issues without these optimizations.\n",
      "\n",
      "Please restructure your imports with 'import unsloth' at the top of your file.\n",
      "  from unsloth import FastLanguageModel, get_chat_template\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü¶• Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Loading base model: ./Qwen3-8B\n",
      "==((====))==  Unsloth 2025.7.11: Fast Qwen3 patching. Transformers: 4.53.3. vLLM: 0.10.0.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 3090. Num GPUs = 1. Max memory: 23.559 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.7.1+cu126. CUDA: 8.6. CUDA Toolkit: 12.6. Triton: 3.3.1\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.31. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Loading checkpoint shards:   0%|                                                                                                         | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Loading checkpoint shards:  20%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                                                                             | 1/5 [00:06<00:25,  6.33s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Loading checkpoint shards:  40%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                                                          | 2/5 [00:16<00:25,  8.59s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Loading checkpoint shards:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                                      | 3/5 [00:24<00:16,  8.18s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Loading checkpoint shards:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                   | 4/5 [00:29<00:06,  6.99s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:29<00:00,  4.61s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:29<00:00,  5.95s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Loading pre-trained adapter: ./my-awesome-model_final_bitcoin-enhanced-prediction-dataset-with-local-comprehensive-news-v2/checkpoint-400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Will map <|im_end|> to EOS = <|im_end|>.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è Could not load adapter on first try: Error(s) in loading state_dict for PeftModelForCausalLM:\n",
      "\tsize mismatch for base_model.model.model.embed_tokens.weight: copying a param with shape torch.Size([151672, 4096]) from checkpoint, the shape in current model is torch.Size([151936, 4096]).\n",
      "\tsize mismatch for base_model.model.lm_head.weight: copying a param with shape torch.Size([151672, 4096]) from checkpoint, the shape in current model is torch.Size([151936, 4096]).\n",
      "‚ÑπÔ∏è Vocab sizes match but other mismatch detected. Proceeding without adapter.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Preparing model for new LoRA adapter training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/moein_salimi/users/babak/IdeaGeneration/venv/lib/python3.10/site-packages/peft/mapping_func.py:73: UserWarning: You are trying to modify a model with PEFT for a second time. If you want to reload the model with a different config, make sure to call `.unload()` before.\n",
      "  warnings.warn(\n",
      "/home/moein_salimi/users/babak/IdeaGeneration/venv/lib/python3.10/site-packages/peft/tuners/tuners_utils.py:190: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2025.7.11 patched 36 layers with 36 QKV layers, 36 O layers and 36 MLP layers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ New LoRA adapter initialized successfully for training\n",
      "\n",
      "üîÑ Loading reward model: microsoft/DialoGPT-medium\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Reward model loaded successfully\n",
      "\n",
      "üìä Model Configuration:\n",
      "  Base model: ./Qwen3-8B\n",
      "  First adapter (loaded, not merged): Not loaded\n",
      "  New LoRA adapter initialized for training with rank: 32\n",
      "  Max sequence length: 2048\n",
      "  Load in 4bit: True\n",
      "  Data type: torch.bfloat16\n",
      "  Reward model: microsoft/DialoGPT-medium\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from pathlib import Path\n",
    "from peft import PeftModel\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from unsloth import FastLanguageModel, get_chat_template\n",
    "\n",
    "# # --- User-defined variables ---\n",
    "# BASE_MODEL_NAME = \"unsloth/mistral-7b-instruct-v0.2-bnb-4bit\"\n",
    "# FALLBACK_MODEL_NAME = \"mistralai/Mistral-7b-Instruct-v0.2\"\n",
    "# ADAPTER_PATH = \"path/to/your/first/adapter\"  # <<-- IMPORTANT: Set this path\n",
    "# CHECKPOINT = \"checkpoint-final\"\n",
    "# MAX_SEQ_LENGTH = 2048\n",
    "# DTYPE = torch.bfloat16  # Set the desired data type here\n",
    "# LOAD_IN_4BIT = True\n",
    "# LORA_R = 16\n",
    "# TARGET_MODULES = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"]\n",
    "# LORA_ALPHA = 16\n",
    "# LORA_DROPOUT = 0\n",
    "# SEED = 3407\n",
    "# REWARD_MODEL_NAME = \"starling-lm/reward-model\"\n",
    "# # --- End of user-defined variables ---\n",
    "\n",
    "\n",
    "preferred_path = Path(BASE_MODEL_NAME)\n",
    "chosen_model_name = BASE_MODEL_NAME if preferred_path.exists() else FALLBACK_MODEL_NAME\n",
    "if chosen_model_name != BASE_MODEL_NAME:\n",
    "    print(f\"‚ÑπÔ∏è Local model path not found at {BASE_MODEL_NAME}. Falling back to {FALLBACK_MODEL_NAME}\")\n",
    "\n",
    "print(f\"üîÑ Loading base model: {chosen_model_name}\")\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=chosen_model_name,\n",
    "    max_seq_length=MAX_SEQ_LENGTH,\n",
    "    dtype=DTYPE,  # Use the configured DTYPE variable\n",
    "    load_in_4bit=LOAD_IN_4BIT,\n",
    ")\n",
    "\n",
    "# Ensure pad token exists and align embeddings to tokenizer size\n",
    "if getattr(tokenizer, \"pad_token\", None) is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Load pre-trained adapter with vocab-size auto-alignment on mismatch\n",
    "first_adapter_loaded = False\n",
    "adapter_path = f\"{ADAPTER_PATH}/{CHECKPOINT}\"\n",
    "if not Path(adapter_path).exists():\n",
    "    print(f\"‚ÑπÔ∏è First adapter checkpoint not found at {adapter_path}. Skipping adapter load.\")\n",
    "else:\n",
    "    print(f\"üîÑ Loading pre-trained adapter: {adapter_path}\")\n",
    "    try:\n",
    "        # Load the first adapter without merging\n",
    "        model = PeftModel.from_pretrained(model, adapter_path)\n",
    "        first_adapter_loaded = True\n",
    "        print(f\"‚úÖ Successfully loaded adapter from {adapter_path}\")\n",
    "    except Exception as e:\n",
    "        err = str(e)\n",
    "        print(f\"‚ö†Ô∏è Could not load adapter on first try: {e}\")\n",
    "\n",
    "        # Try to detect and fix vocab size mismatch\n",
    "        import re\n",
    "        try:\n",
    "            expected_match = re.search(r\"copying a param with shape torch\\.Size\\(\\[(\\d+),\", err)\n",
    "            current_match = re.search(r\"current model is torch\\.Size\\(\\[(\\d+),\", err)\n",
    "            if expected_match and current_match:\n",
    "                expected = int(expected_match.group(1))\n",
    "                current = int(current_match.group(1))\n",
    "                missing = expected - current\n",
    "                if missing > 0:\n",
    "                    print(f\"üîß Detected vocab mismatch. Adding {missing} special token(s) to align.\")\n",
    "                    extra_tokens = [f\"<|extra_{i}|>\" for i in range(missing)]\n",
    "                    tokenizer.add_special_tokens({\"additional_special_tokens\": extra_tokens})\n",
    "                    model.resize_token_embeddings(len(tokenizer))\n",
    "                    model = PeftModel.from_pretrained(model, adapter_path)\n",
    "                    first_adapter_loaded = True\n",
    "                    print(f\"‚úÖ Adapter loaded after aligning vocab size to {len(tokenizer)}\")\n",
    "                else:\n",
    "                     print(\"‚ÑπÔ∏è Vocab sizes match but other mismatch detected. Proceeding without adapter.\")\n",
    "            else:\n",
    "                print(\"‚ÑπÔ∏è No clear vocab mismatch pattern found. Proceeding without adapter.\")\n",
    "        except Exception as e2:\n",
    "            print(f\"‚ö†Ô∏è Auto-alignment failed: {e2}. Proceeding without adapter.\")\n",
    "\n",
    "# ‚ö†Ô∏è IMPORTANT: Merging into a 4-bit model is not recommended.\n",
    "# Instead of merging, we keep the first adapter loaded and active.\n",
    "# The new LoRA adapter for training will be applied on top.\n",
    "if first_adapter_loaded:\n",
    "    print(\"‚úÖ First adapter is loaded and active. Skipping merge step for 4-bit model.\")\n",
    "\n",
    "# Apply chat template\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template=\"chatml\",\n",
    ")\n",
    "\n",
    "# Prepare model for new LoRA adapter training\n",
    "print(\"üîß Preparing model for new LoRA adapter training...\")\n",
    "try:\n",
    "    # This will add a *new* adapter for training, while keeping the first one active.\n",
    "    model = FastLanguageModel.get_peft_model(\n",
    "        model,\n",
    "        r=LORA_R,\n",
    "        target_modules=TARGET_MODULES,\n",
    "        lora_alpha=LORA_ALPHA,\n",
    "        lora_dropout=LORA_DROPOUT,\n",
    "        bias=\"none\",\n",
    "        use_gradient_checkpointing=\"unsloth\",\n",
    "        random_state=SEED,\n",
    "        use_rslora=False,\n",
    "        loftq_config=None,\n",
    "    )\n",
    "    print(\"‚úÖ New LoRA adapter initialized successfully for training\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Error initializing new LoRA adapter: {e}\")\n",
    "    print(\"‚ÑπÔ∏è Continuing with current model state...\")\n",
    "\n",
    "# Load reward model with consistent data type\n",
    "print(f\"\\nüîÑ Loading reward model: {REWARD_MODEL_NAME}\")\n",
    "try:\n",
    "    reward_tokenizer = AutoTokenizer.from_pretrained(REWARD_MODEL_NAME)\n",
    "    reward_model = AutoModel.from_pretrained(\n",
    "        REWARD_MODEL_NAME,\n",
    "        torch_dtype=DTYPE, # Load in the same precision to save memory\n",
    "        device_map=\"auto\",\n",
    "    )\n",
    "    reward_model.eval()\n",
    "    print(f\"‚úÖ Reward model loaded successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Could not load reward model, using rule-based rewards: {e}\")\n",
    "    reward_model = None\n",
    "    reward_tokenizer = None\n",
    "\n",
    "print(f\"\\nüìä Model Configuration:\")\n",
    "print(f\"  Base model: {chosen_model_name}\")\n",
    "print(f\"  First adapter (loaded, not merged): {adapter_path if first_adapter_loaded else 'Not loaded'}\")\n",
    "print(f\"  New LoRA adapter initialized for training with rank: {LORA_R}\")\n",
    "print(f\"  Max sequence length: {MAX_SEQ_LENGTH}\")\n",
    "print(f\"  Load in 4bit: {LOAD_IN_4BIT}\")\n",
    "print(f\"  Data type: {DTYPE}\")\n",
    "print(f\"  Reward model: {REWARD_MODEL_NAME if reward_model else 'Rule-based only'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2263607f",
   "metadata": {
    "papermill": {
     "duration": 0.008952,
     "end_time": "2025-09-11T14:38:15.229713",
     "exception": false,
     "start_time": "2025-09-11T14:38:15.220761",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Load and Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "30c2c24b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-11T14:38:15.250028Z",
     "iopub.status.busy": "2025-09-11T14:38:15.249401Z",
     "iopub.status.idle": "2025-09-11T14:38:20.837066Z",
     "shell.execute_reply": "2025-09-11T14:38:20.836204Z"
    },
    "papermill": {
     "duration": 5.601157,
     "end_time": "2025-09-11T14:38:20.840090",
     "exception": false,
     "start_time": "2025-09-11T14:38:15.238933",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded: tahamajs/bitcoin-enhanced-prediction-dataset-with-local-comprehensive-news\n",
      "Total samples: 2,303\n",
      "\n",
      "=== Sample Data ===\n",
      "instruction: CONTEXT DATE: 2018-01-31\n",
      "\n",
      "ANALYSIS FRAMEWORK:\n",
      "‚Ä¢ Technical Analysis: Use price trends, volatility, and momentum indicators\n",
      "‚Ä¢ Macro Analysis: Consider g...\n",
      "input: Daily Context ‚Äî 2018-01-31\n",
      "\n",
      "[Technical Price Analysis]\n",
      "- Current Price: $10,106.30\n",
      "- 60-Day Range: $10,106.30 ‚Üí $17,527.00\n",
      "- 1D Return: -10.54%\n",
      "- 7D R...\n",
      "output: {\"action\":\"SELL\",\"confidence\":99,\"stop_loss\":10548.72,\"take_profit\":9272.00,\"forecast_10d\":[9170.54, 8830.75, 9174.91, 8277.01, 6955.27, 7754.00, 7621...\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "dataset = load_dataset(DATASET_NAME, split=\"train\")\n",
    "print(f\"Dataset loaded: {DATASET_NAME}\")\n",
    "print(f\"Total samples: {len(dataset):,}\")\n",
    "\n",
    "# Show sample\n",
    "print(\"\\n=== Sample Data ===\")\n",
    "sample = dataset[0]\n",
    "for key, value in sample.items():\n",
    "    print(f\"{key}: {str(value)[:150]}{'...' if len(str(value)) > 150 else ''}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c2ffd99",
   "metadata": {
    "papermill": {
     "duration": 0.00946,
     "end_time": "2025-09-11T14:38:20.861864",
     "exception": false,
     "start_time": "2025-09-11T14:38:20.852404",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Format Dataset for GRPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1ea341fb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-11T14:38:20.888328Z",
     "iopub.status.busy": "2025-09-11T14:38:20.887729Z",
     "iopub.status.idle": "2025-09-11T14:38:33.395830Z",
     "shell.execute_reply": "2025-09-11T14:38:33.394375Z"
    },
    "papermill": {
     "duration": 12.525901,
     "end_time": "2025-09-11T14:38:33.397443",
     "exception": false,
     "start_time": "2025-09-11T14:38:20.871542",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìù Formatting dataset for Unsloth GRPO (prompt-only mode)...\n",
      "Formatted dataset samples: 2,303\n",
      "Columns after formatting: ['prompt']\n",
      "\n",
      "=== Formatted Sample Prompt (Text) ===\n",
      "<|im_start|>system\n",
      "CONTEXT DATE: 2018-01-31\n",
      "\n",
      "ANALYSIS FRAMEWORK:\n",
      "‚Ä¢ Technical Analysis: Use price trends, volatility, and momentum indicators\n",
      "‚Ä¢ Macro Analysis: Consider gold/oil prices for broader market context\n",
      "‚Ä¢ News Analysis: Integrate comprehensive daily news summaries for market catalysts\n",
      "\n",
      "OUTPUT FORMAT (JSON ONLY):\n",
      "Return a single JSON object with EXACTLY these keys:\n",
      "{\"action\":\"BUY|SELL|HOLD\",\"confidence\":<int 1-99>,\"stop_loss\":<price 2dp>,\"take_profit\":<price 2dp>,\"forecast_10d\":[<10 price\n",
      "‚úÖ Dataset text formatting complete.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚ö° Tokenizing the dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Map:   0%|                                                                                                                     | 0/2303 [00:00<?, ? examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Map:  43%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                                           | 1000/2303 [00:04<00:05, 238.30 examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Map:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè             | 2000/2303 [00:10<00:01, 192.37 examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2303/2303 [00:11<00:00, 190.23 examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2303/2303 [00:11<00:00, 193.59 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized dataset samples: 2,303\n",
      "‚úÖ Final columns passed to trainer: ['input_ids', 'attention_mask', 'labels']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Assume 'dataset' is your loaded dataset\n",
    "# dataset = load_dataset(...) \n",
    "\n",
    "# Assume you have your model's max length\n",
    "# For example, for Llama 3 it's 8192\n",
    "MAX_LENGTH = 2048 \n",
    "\n",
    "# ==============================================================================\n",
    "# STEP 1: Format the text prompts (Your original code - this is perfect!)\n",
    "# ==============================================================================\n",
    "def formatting_prompts_func(examples):\n",
    "    \"\"\"\n",
    "    Format examples for GRPO training.\n",
    "    GRPOTrainer expects a single 'prompt' per sample and generates completions internally.\n",
    "    The reference answer is saved in a 'target' column for offline evaluation.\n",
    "    \"\"\"\n",
    "    instructions = examples.get(\"instruction\", [\"\"] * len(examples.get(\"input\", [])))\n",
    "    inputs = examples.get(\"input\", [])\n",
    "    outputs = examples.get(\"output\", [])\n",
    "\n",
    "    prompts = []\n",
    "    targets = []\n",
    "    for instruction, user_input, output in zip(instructions, inputs, outputs):\n",
    "        system_msg = instruction or \"You are a helpful Bitcoin market analyst.\"\n",
    "        user_msg = user_input or \"\"\n",
    "        # Build prompt ending right before the assistant's turn\n",
    "        prompt = (\n",
    "            f\"<|im_start|>system\\n{system_msg}<|im_end|>\\n\"\n",
    "            f\"<|im_start|>user\\n{user_msg}<|im_end|>\\n\"\n",
    "            f\"<|im_start|>assistant\\n\"\n",
    "        )\n",
    "        prompts.append(prompt)\n",
    "        # Store the ground truth completion separately\n",
    "        targets.append((output or \"\") + \"<|im_end|>\")\n",
    "        \n",
    "    return {\"prompt\": prompts, \"target\": targets}\n",
    "\n",
    "print(\"üìù Formatting dataset for Unsloth GRPO (prompt-only mode)...\")\n",
    "formatted_dataset = dataset.map(\n",
    "    formatting_prompts_func,\n",
    "    batched=True,\n",
    "    remove_columns=dataset.column_names,\n",
    "    desc=\"Formatting prompts\"\n",
    ")\n",
    "\n",
    "# Keep a separate copy of targets for evaluation, then remove from the training set\n",
    "reference_targets = formatted_dataset[\"target\"]\n",
    "formatted_dataset = formatted_dataset.remove_columns([\"target\"])\n",
    "\n",
    "print(f\"Formatted dataset samples: {len(formatted_dataset):,}\")\n",
    "print(\"Columns after formatting:\", formatted_dataset.column_names)\n",
    "\n",
    "print(\"\\n=== Formatted Sample Prompt (Text) ===\")\n",
    "print(formatted_dataset[0]['prompt'][:500])\n",
    "print(\"‚úÖ Dataset text formatting complete.\")\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# STEP 2: Tokenize the formatted prompts (The required fix)\n",
    "# ==============================================================================\n",
    "# Make sure to use the correct model name for your tokenizer\n",
    "# Since you use Unsloth and ChatML format, a model like Mistral-Instruct is a good guess\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"unsloth/mistral-7b-instruct-v0.2-bnb-4bit\")\n",
    "\n",
    "# Define the tokenization function\n",
    "def tokenize_function(examples):\n",
    "    # This will process the text in the 'prompt' column\n",
    "    tokenized_output = tokenizer(\n",
    "        examples[\"prompt\"],\n",
    "        truncation=True,\n",
    "        max_length=2048,\n",
    "    )\n",
    "    # Create the 'labels' column by cloning 'input_ids'\n",
    "    tokenized_output[\"labels\"] = tokenized_output[\"input_ids\"][:]\n",
    "    return tokenized_output\n",
    "\n",
    "print(\"\\n‚ö° Tokenizing the dataset...\")\n",
    "tokenized_dataset = formatted_dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=[\"prompt\"] # It's good practice to remove the old column\n",
    ")\n",
    "\n",
    "print(f\"Tokenized dataset samples: {len(tokenized_dataset):,}\")\n",
    "print(\"‚úÖ Final columns passed to trainer:\", tokenized_dataset.column_names)\n",
    "\n",
    "# Now, 'tokenized_dataset' is ready to be passed to your trainer or DataLoader.\n",
    "# It contains 'input_ids' and 'attention_mask', which is exactly what the\n",
    "# data collator expects, and this will solve the ValueError."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "08f158c6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-11T14:38:33.416098Z",
     "iopub.status.busy": "2025-09-11T14:38:33.415508Z",
     "iopub.status.idle": "2025-09-11T14:38:33.436373Z",
     "shell.execute_reply": "2025-09-11T14:38:33.435199Z"
    },
    "papermill": {
     "duration": 0.033036,
     "end_time": "2025-09-11T14:38:33.439134",
     "exception": false,
     "start_time": "2025-09-11T14:38:33.406098",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Sample keys: ['prompt']\n",
      "Prompt length: 26122\n",
      "‚úÇÔ∏è Truncating overlong prompts for safety...\n",
      "‚úÖ Truncation pass complete\n"
     ]
    }
   ],
   "source": [
    "# Define a simple identity collator so Trainer doesn't try to pad raw text\n",
    "class IdentityCollator:\n",
    "    \"\"\"Pass-through collator for raw prompt samples.\n",
    "    Returns list[dict] unchanged so GRPOTrainer can tokenize internally.\"\"\"\n",
    "    def __call__(self, features):\n",
    "        return features\n",
    "\n",
    "raw_text_collator = IdentityCollator()\n",
    "\n",
    "# Quick sanity check: show keys of first sample\n",
    "first = formatted_dataset[0]\n",
    "print(\"üîç Sample keys:\", list(first.keys()))\n",
    "print(\"Prompt length:\", len(first['prompt']))\n",
    "\n",
    "# Optional: truncate very long samples (safeguard)\n",
    "MAX_PROMPT_CHARS = 4000\n",
    "if any(len(r['prompt']) > MAX_PROMPT_CHARS for r in formatted_dataset.select(range(min(50, len(formatted_dataset))))):\n",
    "    def truncate_func(examples):\n",
    "        prompts = []\n",
    "        for p in examples['prompt']:\n",
    "            if len(p) > MAX_PROMPT_CHARS:\n",
    "                p = p[:MAX_PROMPT_CHARS] + \"...\"\n",
    "            prompts.append(p)\n",
    "        return {\"prompt\": prompts}\n",
    "    print(\"‚úÇÔ∏è Truncating overlong prompts for safety...\")\n",
    "    formatted_dataset = formatted_dataset.map(truncate_func, batched=True)\n",
    "    print(\"‚úÖ Truncation pass complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "452de834",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-11T14:38:33.457379Z",
     "iopub.status.busy": "2025-09-11T14:38:33.456803Z",
     "iopub.status.idle": "2025-09-11T14:38:33.481740Z",
     "shell.execute_reply": "2025-09-11T14:38:33.480441Z"
    },
    "papermill": {
     "duration": 0.03578,
     "end_time": "2025-09-11T14:38:33.483344",
     "exception": false,
     "start_time": "2025-09-11T14:38:33.447564",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Helper functions for structured output parsing defined\n"
     ]
    }
   ],
   "source": [
    "# Helper Functions for Structured Output Parsing\n",
    "def parse_trading_output(text):\n",
    "    \"\"\"\n",
    "    Parse trading output JSON from text response.\n",
    "    Expected format: {\"action\":\"SELL\",\"confidence\":99,\"stop_loss\":10668.23,\"take_profit\":9377.95,\"forecast_10d\":[...]}\n",
    "    \"\"\"\n",
    "    if not text:\n",
    "        return None\n",
    "    \n",
    "    import json\n",
    "    import re\n",
    "    \n",
    "    try:\n",
    "        # Try direct JSON parsing first\n",
    "        return json.loads(text.strip())\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    try:\n",
    "        # Look for JSON-like structure in the text\n",
    "        json_pattern = r'\\{[^{}]*\"action\"[^{}]*\\}'\n",
    "        matches = re.findall(json_pattern, text, re.IGNORECASE | re.DOTALL)\n",
    "        \n",
    "        if matches:\n",
    "            # Try to parse the most complete match\n",
    "            for match in matches:\n",
    "                try:\n",
    "                    # Clean up the match and try parsing\n",
    "                    cleaned = match.strip()\n",
    "                    return json.loads(cleaned)\n",
    "                except:\n",
    "                    continue\n",
    "        \n",
    "        # Alternative: Extract components manually\n",
    "        result = {}\n",
    "        \n",
    "        # Extract action\n",
    "        action_match = re.search(r'\"action\"\\s*:\\s*\"([^\"]+)\"', text, re.IGNORECASE)\n",
    "        if action_match:\n",
    "            result['action'] = action_match.group(1).upper()\n",
    "        \n",
    "        # Extract confidence\n",
    "        conf_match = re.search(r'\"confidence\"\\s*:\\s*(\\d+(?:\\.\\d+)?)', text, re.IGNORECASE)\n",
    "        if conf_match:\n",
    "            result['confidence'] = float(conf_match.group(1))\n",
    "        \n",
    "        # Extract stop_loss\n",
    "        sl_match = re.search(r'\"stop_loss\"\\s*:\\s*(\\d+(?:\\.\\d+)?)', text, re.IGNORECASE)\n",
    "        if sl_match:\n",
    "            result['stop_loss'] = float(sl_match.group(1))\n",
    "        \n",
    "        # Extract take_profit\n",
    "        tp_match = re.search(r'\"take_profit\"\\s*:\\s*(\\d+(?:\\.\\d+)?)', text, re.IGNORECASE)\n",
    "        if tp_match:\n",
    "            result['take_profit'] = float(tp_match.group(1))\n",
    "        \n",
    "        # Extract forecast_10d array\n",
    "        forecast_match = re.search(r'\"forecast_10d\"\\s*:\\s*\\[([^\\]]+)\\]', text, re.IGNORECASE)\n",
    "        if forecast_match:\n",
    "            try:\n",
    "                forecast_str = forecast_match.group(1)\n",
    "                forecast_values = [float(x.strip()) for x in forecast_str.split(',')]\n",
    "                result['forecast_10d'] = forecast_values\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        return result if result else None\n",
    "        \n",
    "    except Exception as e:\n",
    "        return None\n",
    "\n",
    "def calculate_forecast_similarity(resp_forecast, gt_forecast):\n",
    "    \"\"\"\n",
    "    Calculate similarity between two forecast arrays.\n",
    "    Uses multiple metrics: correlation, directional accuracy, and magnitude similarity.\n",
    "    \"\"\"\n",
    "    if not resp_forecast or not gt_forecast:\n",
    "        return 0.0\n",
    "    \n",
    "    import numpy as np\n",
    "    \n",
    "    try:\n",
    "        # Ensure both are numeric arrays\n",
    "        resp_arr = np.array([float(x) for x in resp_forecast if isinstance(x, (int, float))])\n",
    "        gt_arr = np.array([float(x) for x in gt_forecast if isinstance(x, (int, float))])\n",
    "        \n",
    "        if len(resp_arr) == 0 or len(gt_arr) == 0:\n",
    "            return 0.0\n",
    "        \n",
    "        # Align lengths (take minimum)\n",
    "        min_len = min(len(resp_arr), len(gt_arr))\n",
    "        resp_arr = resp_arr[:min_len]\n",
    "        gt_arr = gt_arr[:min_len]\n",
    "        \n",
    "        if min_len < 2:\n",
    "            return 0.0\n",
    "        \n",
    "        similarity_score = 0.0\n",
    "        \n",
    "        # 1. Correlation similarity (40%)\n",
    "        try:\n",
    "            corr = np.corrcoef(resp_arr, gt_arr)[0, 1]\n",
    "            if not np.isnan(corr):\n",
    "                similarity_score += abs(corr) * 0.4\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        # 2. Directional accuracy (30%)\n",
    "        resp_directions = np.diff(resp_arr) > 0  # True for up, False for down\n",
    "        gt_directions = np.diff(gt_arr) > 0\n",
    "        if len(resp_directions) > 0:\n",
    "            directional_accuracy = np.mean(resp_directions == gt_directions)\n",
    "            similarity_score += directional_accuracy * 0.3\n",
    "        \n",
    "        # 3. Magnitude similarity (30%)\n",
    "        try:\n",
    "            # Normalize both arrays to compare relative changes\n",
    "            resp_norm = (resp_arr - np.mean(resp_arr)) / (np.std(resp_arr) + 1e-8)\n",
    "            gt_norm = (gt_arr - np.mean(gt_arr)) / (np.std(gt_arr) + 1e-8)\n",
    "            \n",
    "            # Calculate mean squared error and convert to similarity\n",
    "            mse = np.mean((resp_norm - gt_norm) ** 2)\n",
    "            magnitude_similarity = max(0, 1 - (mse / 4))  # Normalize MSE\n",
    "            similarity_score += magnitude_similarity * 0.3\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        return min(1.0, max(0.0, similarity_score))\n",
    "        \n",
    "    except Exception as e:\n",
    "        return 0.0\n",
    "\n",
    "print(\"‚úÖ Helper functions for structured output parsing defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c6724234",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-11T14:38:33.501115Z",
     "iopub.status.busy": "2025-09-11T14:38:33.500543Z",
     "iopub.status.idle": "2025-09-11T14:38:33.516272Z",
     "shell.execute_reply": "2025-09-11T14:38:33.515055Z"
    },
    "papermill": {
     "duration": 0.026932,
     "end_time": "2025-09-11T14:38:33.518331",
     "exception": false,
     "start_time": "2025-09-11T14:38:33.491399",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "def parse_trading_output(response_text):\n",
    "    \"\"\"\n",
    "    Parses a JSON object from a string, looking for the content between ```json and ```.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Find the JSON block within the response\n",
    "        match = re.search(r\"```json\\s*([\\s\\S]+?)\\s*```\", response_text)\n",
    "        if match:\n",
    "            json_str = match.group(1)\n",
    "            return json.loads(json_str)\n",
    "    except (json.JSONDecodeError, TypeError):\n",
    "        # Handle cases where parsing fails or input is not a string\n",
    "        pass\n",
    "    return None\n",
    "\n",
    "def calculate_forecast_similarity(predicted, actual):\n",
    "    \"\"\"\n",
    "    Calculates the similarity between two forecast arrays using Mean Absolute Percentage Error (MAPE).\n",
    "    A lower MAPE results in a higher similarity score (reward).\n",
    "    \"\"\"\n",
    "    if not predicted or not actual:\n",
    "        return 0.0\n",
    "    \n",
    "    # Ensure lists have the same length for comparison\n",
    "    min_len = min(len(predicted), len(actual))\n",
    "    if min_len == 0:\n",
    "        return 0.0\n",
    "        \n",
    "    predicted = predicted[:min_len]\n",
    "    actual = actual[:min_len]\n",
    "    \n",
    "    errors = []\n",
    "    for p, a in zip(predicted, actual):\n",
    "        if a > 0: # Avoid division by zero\n",
    "            errors.append(abs((p - a) / a))\n",
    "    \n",
    "    if not errors:\n",
    "        return 0.0\n",
    "        \n",
    "    mean_absolute_percentage_error = sum(errors) / len(errors)\n",
    "    \n",
    "    # Convert error to a similarity score (reward). 1.0 is perfect, 0.0 is high error.\n",
    "    # An error of 10% (0.1) would result in a 0.9 reward.\n",
    "    similarity = max(0, 1 - mean_absolute_percentage_error)\n",
    "    return similarity\n",
    "\n",
    "def calculate_price_prediction_reward(response, ground_truth):\n",
    "    \"\"\"\n",
    "    Calculates a reward based ONLY on the accuracy of numerical price predictions\n",
    "    (stop_loss, take_profit, and forecast_10d) against the ground truth.\n",
    "    \n",
    "    The final reward is a value between 0.0 and 1.0.\n",
    "    \"\"\"\n",
    "    response_json = parse_trading_output(response)\n",
    "    ground_truth_json = parse_trading_output(ground_truth)\n",
    "    \n",
    "    # If we don't have valid JSON in both, no reward can be calculated.\n",
    "    if not response_json or not ground_truth_json:\n",
    "        return 0.0\n",
    "        \n",
    "    price_rewards = []\n",
    "    \n",
    "    # 1. Price Level Accuracy (Stop Loss and Take Profit)\n",
    "    for price_field in ['stop_loss', 'take_profit']:\n",
    "        resp_price = response_json.get(price_field)\n",
    "        gt_price = ground_truth_json.get(price_field)\n",
    "        \n",
    "        # Ensure both are valid, positive numbers before calculating reward\n",
    "        if isinstance(resp_price, (int, float)) and isinstance(gt_price, (int, float)) and gt_price > 0:\n",
    "            # Calculate the percentage difference\n",
    "            price_diff_pct = abs((resp_price - gt_price) / gt_price)\n",
    "            \n",
    "            # Convert the percentage difference into a reward score (0 to 1)\n",
    "            # A perfect match gets 1.0. A 20% difference gets a 0.8 reward.\n",
    "            price_similarity = max(0, 1 - price_diff_pct)\n",
    "            price_rewards.append(price_similarity)\n",
    "            \n",
    "    # 2. Forecast Accuracy (10-day prediction array)\n",
    "    resp_forecast = response_json.get('forecast_10d')\n",
    "    gt_forecast = ground_truth_json.get('forecast_10d')\n",
    "    \n",
    "    # Ensure both are lists\n",
    "    if isinstance(resp_forecast, list) and isinstance(gt_forecast, list):\n",
    "        forecast_similarity = calculate_forecast_similarity(resp_forecast, gt_forecast)\n",
    "        price_rewards.append(forecast_similarity)\n",
    "        \n",
    "    # If no price fields were rewarded, the total reward is 0.\n",
    "    if not price_rewards:\n",
    "        return 0.0\n",
    "        \n",
    "    # The final reward is the average of all calculated price rewards.\n",
    "    total_reward = sum(price_rewards) / len(price_rewards)\n",
    "    \n",
    "    return total_reward\n",
    "\n",
    "### Example Usage ###\n",
    "\n",
    "# --- Test Case 1: Close match ---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6679bcec",
   "metadata": {
    "papermill": {
     "duration": 0.007441,
     "end_time": "2025-09-11T14:38:33.533945",
     "exception": false,
     "start_time": "2025-09-11T14:38:33.526504",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Enhanced Reward Function with Structured Output Parsing\n",
    "\n",
    "The reward function has been enhanced to handle structured JSON trading outputs with the following format:\n",
    "```json\n",
    "{\n",
    "  \"action\": \"SELL\",\n",
    "  \"confidence\": 99,\n",
    "  \"stop_loss\": 10668.23,\n",
    "  \"take_profit\": 9377.95,\n",
    "  \"forecast_10d\": [8830.75, 9174.91, 8277.01, 6955.27, 7754.00, 7621.30, 8265.59, 8736.98, 8621.90, 8129.97]\n",
    "}\n",
    "```\n",
    "\n",
    "### Reward Distribution:\n",
    "- **Structured Output Parsing (25%)**: Parses and validates JSON format, compares action/confidence/prices/forecast with ground truth\n",
    "- **Prediction Quality (20%)**: Keywords, length, comprehensive analysis indicators\n",
    "- **Technical Analysis (15%)**: Technical indicators, chart patterns, trading signals\n",
    "- **News Integration (15%)**: News impact assessment, multi-factor analysis\n",
    "- **Specificity (10%)**: Price targets, timeframes, confidence levels\n",
    "- **AI Assessment (10%)**: Conversational quality using reward model\n",
    "- **Bonuses (5%)**: Structure, disclaimers, professional formatting\n",
    "\n",
    "### Key Features:\n",
    "- **JSON Parsing**: Extracts structured trading data from model responses\n",
    "- **Action Matching**: Compares trading actions (BUY/SELL/HOLD) with ground truth\n",
    "- **Confidence Scoring**: Rewards confidence levels close to expected values\n",
    "- **Price Accuracy**: Evaluates stop_loss and take_profit price levels\n",
    "- **Forecast Similarity**: Multi-metric comparison of 10-day price predictions using correlation, directional accuracy, and magnitude similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "259ef602",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-11T14:38:33.551525Z",
     "iopub.status.busy": "2025-09-11T14:38:33.550956Z",
     "iopub.status.idle": "2025-09-11T14:38:33.567894Z",
     "shell.execute_reply": "2025-09-11T14:38:33.566672Z"
    },
    "papermill": {
     "duration": 0.027389,
     "end_time": "2025-09-11T14:38:33.569330",
     "exception": false,
     "start_time": "2025-09-11T14:38:33.541941",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Corrected CustomGRPOTrainer created.\n"
     ]
    }
   ],
   "source": [
    "# Custom GRPO Trainer with Enhanced Reward Integration\n",
    "class CustomGRPOTrainer(GRPOTrainer):\n",
    "    \"\"\"\n",
    "    Custom GRPO Trainer that integrates the enhanced reward function\n",
    "    with structured output parsing for Bitcoin trading predictions.\n",
    "    This version correctly processes batches and is structured robustly.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, reward_model=None, reward_tokenizer=None, **kwargs):\n",
    "        # Pass the reward function method to the parent class\n",
    "        kwargs['reward_funcs'] = [self._compute_reward_batch]\n",
    "        \n",
    "        super().__init__(**kwargs)\n",
    "        \n",
    "        # Now self.tokenizer and other attributes are safely initialized\n",
    "        self.reward_model = reward_model\n",
    "        self.reward_tokenizer = reward_tokenizer\n",
    "\n",
    "    def _compute_reward_batch(self, prompts=None, completions=None, **kwargs_inner):\n",
    "        \"\"\"\n",
    "        This is our main reward logic, now defined as a class method.\n",
    "        It correctly loops through a batch of completions and returns a list of rewards.\n",
    "        \"\"\"\n",
    "        rewards = []\n",
    "        try:\n",
    "            # 'completions' is a list of generated text strings from the model\n",
    "            for completion_text in completions:\n",
    "                # Calculate reward for EACH completion individually\n",
    "                reward_score = calculate_comprehensive_prediction_reward(\n",
    "                    response=completion_text,\n",
    "                    ground_truth=None,  # Correct for GRPO, no ground truth needed here\n",
    "                    reward_model=self.reward_model,\n",
    "                    reward_tokenizer=self.reward_tokenizer\n",
    "                )\n",
    "                rewards.append(reward_score)\n",
    "            \n",
    "            return rewards # Return the list of calculated rewards for the batch\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Error during batch reward computation: {e}\")\n",
    "            # If an error occurs, return a list of fallback rewards\n",
    "            # that matches the batch size to avoid crashing the trainer.\n",
    "            batch_size = len(completions) if completions is not None else 0\n",
    "            return [0.5] * batch_size\n",
    "    \n",
    "    def log_reward_details(self, response, reward_score):\n",
    "        \"\"\"\n",
    "        Log detailed reward breakdown for debugging and analysis.\n",
    "        (This method requires no changes)\n",
    "        \"\"\"\n",
    "        print(f\"\\n=== Reward Analysis ===\")\n",
    "        print(f\"Response length: {len(response)} chars\")\n",
    "        print(f\"Reward score: {reward_score:.4f}\")\n",
    "        \n",
    "        response_json = parse_trading_output(response)\n",
    "        \n",
    "        if response_json:\n",
    "            print(f\"Structured output found:\")\n",
    "            print(f\"  Action: {response_json.get('action', 'N/A')}\")\n",
    "            print(f\"  Confidence: {response_json.get('confidence', 'N/A')}\")\n",
    "            print(f\"  Stop Loss: {response_json.get('stop_loss', 'N/A')}\")\n",
    "            print(f\"  Take Profit: {response_json.get('take_profit', 'N/A')}\")\n",
    "            forecast = response_json.get('forecast_10d', [])\n",
    "            print(f\"  Forecast: {forecast[:3]}... ({len(forecast)} values)\")\n",
    "        else:\n",
    "            print(\"No structured output found - using text-based scoring\")\n",
    "        \n",
    "        print(\"=\" * 25)\n",
    "\n",
    "print(\"‚úÖ Corrected CustomGRPOTrainer created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "22b76c9b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-11T14:38:33.587731Z",
     "iopub.status.busy": "2025-09-11T14:38:33.587160Z",
     "iopub.status.idle": "2025-09-11T14:38:33.594587Z",
     "shell.execute_reply": "2025-09-11T14:38:33.593286Z"
    },
    "papermill": {
     "duration": 0.018603,
     "end_time": "2025-09-11T14:38:33.595890",
     "exception": false,
     "start_time": "2025-09-11T14:38:33.577287",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Test Enhanced Reward Function with Structured Output\n",
    "# print(\"üß™ Testing Enhanced Reward Function with Structured Output\")\n",
    "# print(\"=\" * 60)\n",
    "\n",
    "# # Example model response with structured output\n",
    "# example_response = '''Based on my analysis of Bitcoin's current market conditions, technical indicators, and recent news sentiment, here is my prediction:\n",
    "\n",
    "# {\"action\":\"SELL\",\"confidence\":85,\"stop_loss\":11200.50,\"take_profit\":9500.75,\"forecast_10d\":[10450.20, 10100.85, 9750.40, 9500.75, 9200.30, 8950.10, 9150.60, 9400.25, 9300.80, 9150.45]}\n",
    "\n",
    "# This prediction is based on bearish divergence in RSI, declining institutional interest, and regulatory concerns affecting market sentiment.'''\n",
    "\n",
    "# # Example ground truth for comparison\n",
    "# example_ground_truth = '''{\"action\":\"SELL\",\"confidence\":90,\"stop_loss\":11000.00,\"take_profit\":9400.00,\"forecast_10d\":[10400.00, 10050.00, 9700.00, 9450.00, 9200.00, 8900.00, 9100.00, 9350.00, 9250.00, 9100.00]}'''\n",
    "\n",
    "# # Test parsing\n",
    "# print(\"üìä Testing JSON Parsing:\")\n",
    "# parsed_response = parse_trading_output(example_response)\n",
    "# parsed_gt = parse_trading_output(example_ground_truth)\n",
    "\n",
    "# print(\"Response JSON:\", parsed_response)\n",
    "# print(\"Ground Truth JSON:\", parsed_gt)\n",
    "\n",
    "# # Test reward calculation\n",
    "# print(f\"\\nüèÜ Testing Reward Calculation:\")\n",
    "# reward_score = calculate_comprehensive_prediction_reward(\n",
    "#     response=example_response,\n",
    "#     ground_truth=example_ground_truth,\n",
    "#     reward_model=reward_model,\n",
    "#     reward_tokenizer=reward_tokenizer\n",
    "# )\n",
    "\n",
    "# print(f\"Reward Score: {reward_score:.4f}\")\n",
    "\n",
    "# # Test forecast similarity\n",
    "# if parsed_response and parsed_gt:\n",
    "#     forecast_sim = calculate_forecast_similarity(\n",
    "#         parsed_response.get('forecast_10d', []),\n",
    "#         parsed_gt.get('forecast_10d', [])\n",
    "#     )\n",
    "#     print(f\"Forecast Similarity: {forecast_sim:.4f}\")\n",
    "\n",
    "# print(\"\\n‚úÖ Enhanced reward function testing completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5786cd7e",
   "metadata": {
    "papermill": {
     "duration": 0.008733,
     "end_time": "2025-09-11T14:38:33.612819",
     "exception": false,
     "start_time": "2025-09-11T14:38:33.604086",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Setup GRPO Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a072b0d8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-11T14:38:33.631408Z",
     "iopub.status.busy": "2025-09-11T14:38:33.630789Z",
     "iopub.status.idle": "2025-09-11T14:38:33.722097Z",
     "shell.execute_reply": "2025-09-11T14:38:33.720739Z"
    },
    "papermill": {
     "duration": 0.102867,
     "end_time": "2025-09-11T14:38:33.723955",
     "exception": false,
     "start_time": "2025-09-11T14:38:33.621088",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ Training Configuration:\n"
     ]
    }
   ],
   "source": [
    "# Training arguments\n",
    "max_steps = SANITY_MAX_STEPS if SANITY_RUN else -1\n",
    "\n",
    "grpo_args = GRPOConfig(\n",
    "    # FIX 1: Corrected parameter name\n",
    "    output_dir=OUTPUT_DIR,\n",
    "\n",
    "    # FIX 2: Set max_steps and remove conditional logic for epochs\n",
    "    max_steps=max_steps,\n",
    "    num_train_epochs=NUM_TRAIN_EPOCHS,\n",
    "    bf16=True,  # You are enabling bfloat16 training\n",
    "\n",
    "    per_device_train_batch_size=PER_DEVICE_TRAIN_BATCH_SIZE,\n",
    "    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    logging_steps=10,\n",
    "    save_steps=0 if SANITY_RUN else 100,\n",
    "    save_strategy=\"no\" if SANITY_RUN else \"steps\",\n",
    "    warmup_ratio=0.1,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    optim=\"adamw_8bit\",\n",
    "    weight_decay=0.01,\n",
    "    max_grad_norm=1.0,\n",
    "        # max_new_tokens=1024,  # Limit to 50 new tokens generated\n",
    "\n",
    "    # fp16=not is_bfloat16_supported(),\n",
    "    # bf16=is_bfloat16_supported(),\n",
    "    remove_unused_columns=False,\n",
    "    dataloader_num_workers=2,\n",
    "    seed=SEED,\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "# Your print statements for verification are good and will still work.\n",
    "print(f\"üéØ Training Configuration:\")\n",
    "# ... (rest of your print statements)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf3fa56",
   "metadata": {
    "papermill": {
     "duration": 0.008385,
     "end_time": "2025-09-11T14:38:33.740876",
     "exception": false,
     "start_time": "2025-09-11T14:38:33.732491",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Initialize GRPO Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d44834be",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-11T14:38:33.764071Z",
     "iopub.status.busy": "2025-09-11T14:38:33.763475Z",
     "iopub.status.idle": "2025-09-11T14:38:33.796472Z",
     "shell.execute_reply": "2025-09-11T14:38:33.795570Z"
    },
    "papermill": {
     "duration": 0.048821,
     "end_time": "2025-09-11T14:38:33.798079",
     "exception": false,
     "start_time": "2025-09-11T14:38:33.749258",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Initializing GRPO Trainer with pre-tokenized dataset...\n",
      "‚ö†Ô∏è Failed to initialize GRPO trainer: GRPOTrainer.__init__() got an unexpected keyword argument 'tokenizer'\n",
      "üîÑ Falling back to standard Trainer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Fallback Trainer initialized with tokenized data\n",
      "üéØ Training ready with Trainer\n",
      "üìä Training dataset: 2,303 samples\n",
      "üîß Trainer configuration:\n",
      "  ‚Ä¢ Effective batch size: 8\n",
      "  ‚Ä¢ Total training steps: 287\n",
      "  ‚Ä¢ Learning rate: 3e-07\n",
      "  ‚Ä¢ Dataloader workers: 0 (avoiding multiprocessing issues)\n",
      "  ‚Ä¢ Sanity run: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_160097/911182622.py:52: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  grpo_trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "# Initialize GRPO trainer with tokenized dataset\n",
    "print(\"üîß Initializing GRPO Trainer with pre-tokenized dataset...\")\n",
    "generation_kwargs = {\n",
    "    \"max_new_tokens\": 1024,  # <-- Set your generation length here\n",
    "    \"do_sample\": True,\n",
    "    \"top_k\": 50,\n",
    "    \"temperature\": 0.7,\n",
    "}\n",
    "\n",
    "if GRPO_AVAILABLE:\n",
    "    try:\n",
    "        # Note: Use the DataCollatorForLanguageModeling to pad tokenized batches correctly\n",
    "        from transformers import DataCollatorForLanguageModeling\n",
    "        \n",
    "        # This collator properly handles tokenized data (input_ids, attention_mask) for language modeling\n",
    "        data_collator = DataCollatorForLanguageModeling(\n",
    "            tokenizer=tokenizer,\n",
    "            mlm=False  # We're doing causal LM, not masked LM\n",
    "        )\n",
    "        \n",
    "        # Use the CustomGRPOTrainer with pre-tokenized data\n",
    "        grpo_trainer = CustomGRPOTrainer(\n",
    "            model=model,\n",
    "            tokenizer=tokenizer,\n",
    "            args=grpo_args,\n",
    "            train_dataset=tokenized_dataset,  # Now contains input_ids, attention_mask\n",
    "            reward_model=reward_model,\n",
    "            reward_tokenizer=reward_tokenizer,\n",
    "            max_length=MAX_LENGTH,\n",
    "            max_prompt_length=MAX_PROMPT_LENGTH,\n",
    "            beta=BETA,\n",
    "                # generation_kwargs=generation_kwargs, # <-- The correct argument\n",
    "\n",
    "            # Pass the appropriate collator for tokenized data\n",
    "            data_collator=data_collator\n",
    "        )\n",
    "        print(\"‚úÖ CustomGRPOTrainer initialized successfully\")\n",
    "        print(f\"üìä Dataset columns: {formatted_dataset.column_names}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Failed to initialize GRPO trainer: {e}\")\n",
    "        print(\"üîÑ Falling back to standard Trainer\")\n",
    "        \n",
    "        # For fallback, we can use the same tokenized dataset\n",
    "        from transformers import Trainer, DataCollatorForLanguageModeling\n",
    "        \n",
    "        data_collator = DataCollatorForLanguageModeling(\n",
    "            tokenizer=tokenizer,\n",
    "            mlm=False  # We're doing causal LM, not masked LM\n",
    "        )\n",
    "        \n",
    "        grpo_trainer = Trainer(\n",
    "            model=model,\n",
    "            tokenizer=tokenizer,\n",
    "            args=grpo_args,\n",
    "            train_dataset=formatted_dataset,\n",
    "            data_collator=data_collator,\n",
    "        )\n",
    "        print(\"‚úÖ Fallback Trainer initialized with tokenized data\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è GRPO not available, using standard Trainer\")\n",
    "    \n",
    "    from transformers import Trainer, DataCollatorForLanguageModeling\n",
    "    \n",
    "    data_collator = DataCollatorForLanguageModeling(\n",
    "        tokenizer=tokenizer,\n",
    "        mlm=False  # We're doing causal LM, not masked LM\n",
    "    )\n",
    "    \n",
    "    grpo_trainer = Trainer(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        args=grpo_args,\n",
    "        train_dataset=tokenized_dataset,\n",
    "        data_collator=data_collator,\n",
    "    )\n",
    "    print(\"‚úÖ Standard Trainer initialized with tokenized data\")\n",
    "\n",
    "print(f\"üéØ Training ready with {type(grpo_trainer).__name__}\")\n",
    "print(f\"üìä Training dataset: {len(formatted_dataset):,} samples\")\n",
    "print(f\"üîß Trainer configuration:\")\n",
    "print(f\"  ‚Ä¢ Effective batch size: {PER_DEVICE_TRAIN_BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS}\")\n",
    "print(f\"  ‚Ä¢ Total training steps: {len(formatted_dataset) // (PER_DEVICE_TRAIN_BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS) * (0 if SANITY_RUN else NUM_TRAIN_EPOCHS)}\")\n",
    "print(f\"  ‚Ä¢ Learning rate: {LEARNING_RATE}\")\n",
    "print(f\"  ‚Ä¢ Dataloader workers: 0 (avoiding multiprocessing issues)\")\n",
    "print(f\"  ‚Ä¢ Sanity run: {SANITY_RUN}\")\n",
    "\n",
    "if SANITY_RUN:\n",
    "    print(f\"  ‚Ä¢ Sanity max steps: {SANITY_MAX_STEPS}\")\n",
    "    print(\"‚ö†Ô∏è SANITY_RUN is enabled - this will be a short test run\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a35fa2b3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-11T14:38:33.820600Z",
     "iopub.status.busy": "2025-09-11T14:38:33.820010Z",
     "iopub.status.idle": "2025-09-11T14:38:33.847075Z",
     "shell.execute_reply": "2025-09-11T14:38:33.845835Z"
    },
    "papermill": {
     "duration": 0.042429,
     "end_time": "2025-09-11T14:38:33.849382",
     "exception": false,
     "start_time": "2025-09-11T14:38:33.806953",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Initializing GRPO Trainer...\n",
      "‚úÖ Final check: Columns being passed to trainer: ['input_ids', 'attention_mask', 'labels']\n",
      "‚ö†Ô∏è Failed to initialize GRPO trainer: GRPOTrainer.__init__() got an unexpected keyword argument 'tokenizer'\n",
      "üîÑ Falling back to standard trainer\n",
      "‚úÖ Fallback trainer initialized with DataCollatorWithPadding\n",
      "üéØ Training ready with Trainer\n",
      "üìä Training dataset: 2,303 samples\n",
      "üîß Trainer configuration:\n",
      "  ‚Ä¢ Effective batch size: 8\n",
      "  ‚Ä¢ Total training steps: 287\n",
      "  ‚Ä¢ Learning rate: 3e-07\n",
      "  ‚Ä¢ Sanity run: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_160097/3243683351.py:39: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  grpo_trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "# Initialize GRPO trainer with enhanced reward function\n",
    "print(\"üîß Initializing GRPO Trainer...\")\n",
    "print(\"‚úÖ Final check: Columns being passed to trainer:\", tokenized_dataset.column_names)\n",
    "\n",
    "if GRPO_AVAILABLE:\n",
    "    try:\n",
    "        # Initialize CustomGRPOTrainer with enhanced reward integration\n",
    "        grpo_trainer = CustomGRPOTrainer(\n",
    "            model=model,\n",
    "            tokenizer=tokenizer,\n",
    "            args=grpo_args,  # Use GRPOConfig\n",
    "    train_dataset=tokenized_dataset,  # <--- CORRECTED\n",
    "            reward_model=reward_model,\n",
    "            reward_tokenizer=reward_tokenizer,\n",
    "            # GRPO-specific parameters\n",
    "            max_length=MAX_LENGTH,\n",
    "            max_prompt_length=MAX_PROMPT_LENGTH,\n",
    "            beta=BETA,\n",
    "            # Let GRPO handle its own data collation for tokenized data\n",
    "        )\n",
    "        \n",
    "        print(\"‚úÖ CustomGRPOTrainer initialized successfully\")\n",
    "        print(f\"üìä Training dataset format: {formatted_dataset.column_names}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Failed to initialize GRPO trainer: {e}\")\n",
    "        print(\"üîÑ Falling back to standard trainer\")\n",
    "        \n",
    "        # Fallback initialization with correct data collator for tokenized data\n",
    "        from transformers import Trainer, DataCollatorWithPadding\n",
    "        \n",
    "        # Use DataCollatorWithPadding for pre-tokenized data\n",
    "        data_collator = DataCollatorWithPadding(\n",
    "            tokenizer=tokenizer,\n",
    "            padding=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        grpo_trainer = Trainer(\n",
    "            model=model,\n",
    "            tokenizer=tokenizer,\n",
    "            args=grpo_args,\n",
    "            train_dataset=tokenized_dataset,\n",
    "            data_collator=data_collator,\n",
    "        )\n",
    "        print(\"‚úÖ Fallback trainer initialized with DataCollatorWithPadding\")\n",
    "\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è GRPO not available, using standard training\")\n",
    "    from transformers import Trainer, DataCollatorWithPadding\n",
    "    \n",
    "    # Use DataCollatorWithPadding for pre-tokenized data\n",
    "    data_collator = DataCollatorWithPadding(\n",
    "        tokenizer=tokenizer,\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    \n",
    "    grpo_trainer = Trainer(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        args=grpo_args,\n",
    "        train_dataset=tokenized_dataset,\n",
    "        data_collator=data_collator,\n",
    "    )\n",
    "    print(\"‚úÖ Standard trainer initialized with DataCollatorWithPadding\")\n",
    "\n",
    "print(f\"üéØ Training ready with {type(grpo_trainer).__name__}\")\n",
    "print(f\"üìä Training dataset: {len(formatted_dataset):,} samples\")\n",
    "print(f\"üîß Trainer configuration:\")\n",
    "print(f\"  ‚Ä¢ Effective batch size: {PER_DEVICE_TRAIN_BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS}\")\n",
    "print(f\"  ‚Ä¢ Total training steps: {len(formatted_dataset) // (PER_DEVICE_TRAIN_BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS) * (0 if SANITY_RUN else NUM_TRAIN_EPOCHS)}\")\n",
    "print(f\"  ‚Ä¢ Learning rate: {LEARNING_RATE}\")\n",
    "print(f\"  ‚Ä¢ Sanity run: {SANITY_RUN}\")\n",
    "\n",
    "if SANITY_RUN:\n",
    "    print(f\"  ‚Ä¢ Sanity max steps: {SANITY_MAX_STEPS}\")\n",
    "    print(\"‚ö†Ô∏è SANITY_RUN is enabled - this will be a short test run\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "503f06f8",
   "metadata": {
    "papermill": {
     "duration": 0.008129,
     "end_time": "2025-09-11T14:38:33.866922",
     "exception": false,
     "start_time": "2025-09-11T14:38:33.858793",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Start GRPO Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7f170b4e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-11T14:38:33.885004Z",
     "iopub.status.busy": "2025-09-11T14:38:33.884423Z",
     "iopub.status.idle": "2025-09-11T15:24:54.001410Z",
     "shell.execute_reply": "2025-09-11T15:24:53.999721Z"
    },
    "papermill": {
     "duration": 2777.829858,
     "end_time": "2025-09-11T15:24:51.704839",
     "exception": false,
     "start_time": "2025-09-11T14:38:33.874981",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting Unsloth GRPO Training...\n",
      "Training 1 epoch(s) on 2,303 samples\n",
      "============================================================\n",
      "Training started at: 2025-09-11 18:08:33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 2,303 | Num Epochs = 1 | Total steps = 288\n",
      "O^O/ \\_/ \\    Batch size per device = 1 | Gradient accumulation steps = 8\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (1 x 8 x 1) = 8\n",
      " \"-____-\"     Trainable parameters = 87,293,952 of 8,278,029,312 (1.05% trained)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Will smartly offload gradients to save VRAM!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='193' max='288' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [193/288 45:54 < 22:49, 0.07 it/s, Epoch 0.67/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>7.446500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>7.502100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>7.501800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>7.422800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>7.505300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>7.459500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>7.490200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>7.480500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>7.317100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>7.351100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>7.483900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>7.482700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>7.264500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>7.370400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>7.413600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>7.411700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>7.340200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>7.409100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>7.392800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining started at: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstart_time\u001b[38;5;241m.\u001b[39mstrftime(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY-\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm-\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mH:\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mM:\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mS\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m trainer_stats \u001b[38;5;241m=\u001b[39m \u001b[43mgrpo_trainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Record end time\u001b[39;00m\n\u001b[1;32m     14\u001b[0m end_time \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mnow()\n",
      "File \u001b[0;32m~/users/babak/IdeaGeneration/venv/lib/python3.10/site-packages/transformers/trainer.py:2206\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2204\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2205\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2206\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2207\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2208\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2209\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2210\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2211\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m<string>:323\u001b[0m, in \u001b[0;36m_fast_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n",
      "File \u001b[0;32m<string>:82\u001b[0m, in \u001b[0;36m_unsloth_training_step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n",
      "File \u001b[0;32m~/users/babak/IdeaGeneration/venv/lib/python3.10/site-packages/accelerate/accelerator.py:2578\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   2576\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlomo_backward(loss, learning_rate)\n\u001b[1;32m   2577\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2578\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/users/babak/IdeaGeneration/venv/lib/python3.10/site-packages/torch/_tensor.py:648\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    638\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    639\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    640\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    641\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    646\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    647\u001b[0m     )\n\u001b[0;32m--> 648\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    649\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    650\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/users/babak/IdeaGeneration/venv/lib/python3.10/site-packages/torch/autograd/__init__.py:353\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    348\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    350\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    351\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    352\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 353\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    356\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    357\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    358\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    359\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    360\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    361\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/users/babak/IdeaGeneration/venv/lib/python3.10/site-packages/torch/autograd/graph.py:824\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    822\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    823\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 824\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    825\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    826\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    827\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    828\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Start training\n",
    "print(\"üöÄ Starting Unsloth GRPO Training...\")\n",
    "print(f\"Training {0 if SANITY_RUN else NUM_TRAIN_EPOCHS} epoch(s) on {len(formatted_dataset):,} samples\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Record start time\n",
    "start_time = datetime.now()\n",
    "print(f\"Training started at: {start_time.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "# Train the model\n",
    "trainer_stats = grpo_trainer.train()\n",
    "\n",
    "# Record end time\n",
    "end_time = datetime.now()\n",
    "training_duration = end_time - start_time\n",
    "\n",
    "# Extract safe stats\n",
    "final_loss = None\n",
    "steps_done = None\n",
    "try:\n",
    "    final_loss = getattr(trainer_stats, \"training_loss\", None)\n",
    "    steps_done = getattr(trainer_stats, \"global_step\", None)\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üéâ GRPO Training Completed!\")\n",
    "print(f\"Training finished at: {end_time.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"Total training time: {training_duration}\")\n",
    "print(f\"Final training loss: {final_loss if final_loss is not None else 'N/A'}\")\n",
    "print(f\"Training steps: {steps_done if steps_done is not None else 'N/A'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89bef97b",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d9c028",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"üíæ Saving trained model...\")\n",
    "\n",
    "# Save model and tokenizer\n",
    "model.save_pretrained(f\"{OUTPUT_DIR}/final_model\")\n",
    "tokenizer.save_pretrained(f\"{OUTPUT_DIR}/final_model\")\n",
    "\n",
    "print(f\"‚úÖ Model saved to: {OUTPUT_DIR}/final_model\")\n",
    "\n",
    "# Save training summary\n",
    "training_method = \"Unsloth GRPO\" if GRPO_AVAILABLE else \"Unsloth SFT/Basic\"\n",
    "training_summary = {\n",
    "    \"base_model_name\": chosen_model_name,\n",
    "    \"adapter_path\": f\"{ADAPTER_PATH}/{CHECKPOINT}\",\n",
    "    \"adapter_loaded\": adapter_loaded,\n",
    "    \"dataset\": DATASET_NAME,\n",
    "    \"training_method\": training_method,\n",
    "    \"grpo_available\": GRPO_AVAILABLE,\n",
    "    \"total_samples\": len(formatted_dataset),\n",
    "    \"training_config\": {\n",
    "        \"epochs\": training_config.num_train_epochs,\n",
    "        \"max_steps\": training_config.max_steps if training_config.max_steps > 0 else None,\n",
    "        \"learning_rate\": training_config.learning_rate,\n",
    "        \"batch_size\": training_config.per_device_train_batch_size,\n",
    "        \"gradient_accumulation_steps\": training_config.gradient_accumulation_steps,\n",
    "        \"sanity_run\": SANITY_RUN,\n",
    "        \"sanity_max_steps\": SANITY_MAX_STEPS if SANITY_RUN else None,\n",
    "        \"sanity_dataset_size\": SANITY_DATASET_SIZE if SANITY_RUN else None,\n",
    "    },\n",
    "    \"training_results\": {\n",
    "        \"final_loss\": final_loss,\n",
    "        \"total_steps\": steps_done,\n",
    "        \"training_duration\": str(training_duration),\n",
    "    },\n",
    "    \"timestamps\": {\n",
    "        \"start_time\": start_time.isoformat(),\n",
    "        \"end_time\": end_time.isoformat(),\n",
    "    },\n",
    "    \"model_path\": f\"{OUTPUT_DIR}/final_model\",\n",
    "}\n",
    "\n",
    "# Add GRPO-specific config if available\n",
    "if GRPO_AVAILABLE and hasattr(training_config, 'beta'):\n",
    "    training_summary[\"training_config\"][\"grpo_beta\"] = training_config.beta\n",
    "    training_summary[\"training_config\"][\"max_length\"] = training_config.max_length\n",
    "    training_summary[\"training_config\"][\"max_prompt_length\"] = training_config.max_prompt_length\n",
    "\n",
    "# Save summary\n",
    "with open(f\"{OUTPUT_DIR}/training_summary.json\", \"w\") as f:\n",
    "    json.dump(training_summary, f, indent=2)\n",
    "\n",
    "print(f\"Training summary saved to: {OUTPUT_DIR}/training_summary.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9efd7251",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Test Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6984cec6",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def test_trained_model(model, tokenizer):\n",
    "    \"\"\"\n",
    "    Generates a response from the trained model to a test prompt.\n",
    "    \"\"\"\n",
    "    print(\"üß™ Testing the trained GRPO model...\")\n",
    "\n",
    "    # Prepare model for inference\n",
    "    FastLanguageModel.for_inference(model)\n",
    "\n",
    "    # Test sample\n",
    "    test_messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are an expert Bitcoin market analyst. Provide accurate and insightful analysis.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Based on recent market trends and news, what is your Bitcoin price prediction for the next week? Please provide detailed analysis.\"}\n",
    "    ]\n",
    "\n",
    "    # Format with chat template\n",
    "    test_prompt = tokenizer.apply_chat_template(\n",
    "        test_messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "\n",
    "    print(\"Test prompt:\")\n",
    "    print(test_prompt)\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "\n",
    "    # Generate response\n",
    "    inputs = tokenizer(test_prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=1024,\n",
    "            temperature=0.7,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "\n",
    "    # Decode response\n",
    "    response = tokenizer.decode(\n",
    "        outputs[0][len(inputs.input_ids[0]):],\n",
    "        skip_special_tokens=True\n",
    "    )\n",
    "\n",
    "    print(\"Model Response:\")\n",
    "    print(response)\n",
    "    print(\"\\n‚úÖ Model testing completed!\")\n",
    "\n",
    "# Run the test\n",
    "test_trained_model(model, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4446bded",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Training Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff117e11",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"üìä Unsloth GRPO Training Summary\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"ü§ñ Model: {MODEL_NAME}\")\n",
    "print(f\"üìö Dataset: {DATASET_NAME}\")\n",
    "print(f\"üìà Training method: Unsloth GRPO (Group Relative Policy Optimization)\")\n",
    "print(f\"üìù Total samples: {len(formatted_dataset):,}\")\n",
    "print()\n",
    "print(\"üéØ Training Configuration:\")\n",
    "print(f\"  ‚Ä¢ Epochs: {NUM_TRAIN_EPOCHS}\")\n",
    "print(f\"  ‚Ä¢ Learning rate: {LEARNING_RATE}\")\n",
    "print(f\"  ‚Ä¢ Batch size: {PER_DEVICE_TRAIN_BATCH_SIZE}\")\n",
    "print(f\"  ‚Ä¢ Gradient accumulation: {GRADIENT_ACCUMULATION_STEPS}\")\n",
    "print(f\"  ‚Ä¢ Effective batch size: {PER_DEVICE_TRAIN_BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS}\")\n",
    "print(f\"  ‚Ä¢ Max length: {MAX_LENGTH}\")\n",
    "print(f\"  ‚Ä¢ Max prompt length: {MAX_PROMPT_LENGTH}\")\n",
    "print(f\"  ‚Ä¢ Beta (KL penalty): {BETA}\")\n",
    "print(f\"  ‚Ä¢ LoRA rank: {LORA_R}\")\n",
    "print()\n",
    "print(\"üìä Training Results:\")\n",
    "print(f\"  ‚Ä¢ Final loss: {trainer_stats.training_loss:.4f}\")\n",
    "print(f\"  ‚Ä¢ Training steps: {trainer_stats.global_step:,}\")\n",
    "print(f\"  ‚Ä¢ Training duration: {training_duration}\")\n",
    "print()\n",
    "print(\"üíæ Outputs:\")\n",
    "print(f\"  ‚Ä¢ Model saved to: {OUTPUT_DIR}/final_model\")\n",
    "print(f\"  ‚Ä¢ Summary saved to: {OUTPUT_DIR}/training_summary.json\")\n",
    "print()\n",
    "print(\"üî¨ Key Features:\")\n",
    "print(\"  ‚úÖ Unsloth-optimized GRPO training\")\n",
    "print(\"  ‚úÖ Memory-efficient 4-bit quantization\")\n",
    "print(\"  ‚úÖ LoRA parameter-efficient fine-tuning\")\n",
    "print(\"  ‚úÖ Preference learning for Bitcoin analysis\")\n",
    "print(\"  ‚úÖ Chat template formatting\")\n",
    "print(\"  ‚úÖ Gradient checkpointing for memory optimization\")\n",
    "print()\n",
    "print(\"üéâ Unsloth GRPO training completed successfully!\")\n",
    "print(\"üìà Model ready for Bitcoin prediction tasks!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 2881.794474,
   "end_time": "2025-09-11T15:24:55.338588",
   "environment_variables": {},
   "exception": null,
   "input_path": "Unsloth_GRPO_bitcoin-enhanced-prediction_final_from_sft.ipynb",
   "output_path": "train_grpo_from_sft.ipynb",
   "parameters": {},
   "start_time": "2025-09-11T14:36:53.544114",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
