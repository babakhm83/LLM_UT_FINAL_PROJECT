{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd65497f",
   "metadata": {},
   "source": [
    "# GRPO Training for Bitcoin Price Prediction\n",
    "\n",
    "This notebook implements Group Relative Policy Optimization (GRPO) for enhanced Bitcoin price prediction performance.\n",
    "\n",
    "**Dataset**: `bitcoin-enhanced-prediction-dataset-with-local-comprehensive-news`\n",
    "\n",
    "**Training Method**: Group Relative Policy Optimization (GRPO)\n",
    "- Preference learning through relative comparisons\n",
    "- Custom reward system for Bitcoin prediction quality\n",
    "- Multi-response generation and ranking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cba8daf6",
   "metadata": {},
   "source": [
    "## Install Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b45d7212",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U unsloth\n",
    "# !pip install trl\n",
    "# !pip install accelerate\n",
    "# !pip install datasets\n",
    "# !pip install transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "306f1002",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "430c9f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "from datasets import load_dataset, Dataset\n",
    "import torch, random, os\n",
    "from typing import Dict, List, Any\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import AdamW\n",
    "from tqdm import tqdm\n",
    "\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f02cefe8",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45349c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model and training configuration\n",
    "MODEL_CONFIG = {\n",
    "    \"model_path\": \"./Qwen3-8B\",  # Base model path\n",
    "    # \"model_path\": \"qwen_bitcoin_sft_enhanced/lora_adapter\",  # Use this if loading from SFT checkpoint\n",
    "    \"max_seq_length\": 4096,\n",
    "    \"dtype\": torch.float16,\n",
    "    \"load_in_4bit\": True,\n",
    "    \"lora_r\": 32,\n",
    "    \"lora_alpha\": 32,\n",
    "    \"lora_dropout\": 0.0,\n",
    "    \"target_modules\": [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"]\n",
    "}\n",
    "\n",
    "GRPO_CONFIG = {\n",
    "    \"output_dir\": \"qwen_bitcoin_grpo_only_enhanced\",\n",
    "    \"num_train_epochs\": 3,\n",
    "    \"per_device_train_batch_size\": 2,\n",
    "    \"gradient_accumulation_steps\": 4,\n",
    "    \"learning_rate\": 1e-5,\n",
    "    \"logging_steps\": 5,\n",
    "    \"save_steps\": 100,\n",
    "    \"warmup_ratio\": 0.1,\n",
    "    \"group_size\": 4,  # Number of responses to compare per group\n",
    "    \"temperature\": 0.7,  # For response generation\n",
    "}\n",
    "\n",
    "DATASET_NAME = \"tahamajs/bitcoin-enhanced-prediction-dataset-with-local-comprehensive-news\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19047d32",
   "metadata": {},
   "source": [
    "## Load Model and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ad2642",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    MODEL_CONFIG[\"model_path\"],\n",
    "    max_seq_length=MODEL_CONFIG[\"max_seq_length\"],\n",
    "    dtype=MODEL_CONFIG[\"dtype\"],\n",
    "    load_in_4bit=MODEL_CONFIG[\"load_in_4bit\"],\n",
    ")\n",
    "\n",
    "# Prepare model for training\n",
    "FastLanguageModel.for_training(model)\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=MODEL_CONFIG[\"lora_r\"],\n",
    "    target_modules=MODEL_CONFIG[\"target_modules\"],\n",
    "    lora_alpha=MODEL_CONFIG[\"lora_alpha\"],\n",
    "    lora_dropout=MODEL_CONFIG[\"lora_dropout\"],\n",
    "    use_rslora=True,\n",
    ")\n",
    "\n",
    "print(f\"Model loaded: {MODEL_CONFIG['model_path']}\")\n",
    "print(f\"Total parameters: {model.num_parameters():,}\")\n",
    "print(f\"Trainable parameters: {model.num_parameters(only_trainable=True):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb7afaf3",
   "metadata": {},
   "source": [
    "## Special Tokens Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8bf079d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define special tokens\n",
    "SPECIAL_TOKENS = [\"<|response|>\", \"<|analysis|>\", \"<|forecast|>\", \"<|thinking|>\"]\n",
    "num_added = tokenizer.add_special_tokens({\"additional_special_tokens\": SPECIAL_TOKENS})\n",
    "if num_added > 0:\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "    print(f\"Added {num_added} special tokens\")\n",
    "\n",
    "RESPONSE_TAG = \"<|response|>\"\n",
    "ANALYSIS_TAG = \"<|analysis|>\"\n",
    "FORECAST_TAG = \"<|forecast|>\"\n",
    "THINKING_TAG = \"<|thinking|>\"\n",
    "\n",
    "response_token_id = tokenizer.convert_tokens_to_ids(RESPONSE_TAG)\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.truncation_side = \"left\"\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "print(f\"Special tokens: {SPECIAL_TOKENS}\")\n",
    "print(f\"Response token ID: {response_token_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1436be2",
   "metadata": {},
   "source": [
    "## Load and Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9faa00f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "raw_dataset = load_dataset(DATASET_NAME)\n",
    "print(f\"Loaded dataset: {DATASET_NAME}\")\n",
    "print(f\"Dataset structure: {raw_dataset}\")\n",
    "\n",
    "train_data = raw_dataset[\"train\"]\n",
    "print(f\"Total training samples: {len(train_data)}\")\n",
    "\n",
    "# Show sample data\n",
    "print(\"\\n=== Sample Data ===\")\n",
    "sample = train_data[0]\n",
    "for key, value in sample.items():\n",
    "    print(f\"{key}: {str(value)[:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d40003",
   "metadata": {},
   "source": [
    "## Data Processing for GRPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e008b4ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_chat_grpo(ex):\n",
    "    \"\"\"Format example for GRPO training (input only for generation)\"\"\"\n",
    "    instruction = ex.get(\"instruction\", \"\") or \"\"\n",
    "    user_input = ex.get(\"input\", \"\") or \"\"\n",
    "    \n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": instruction},\n",
    "        {\"role\": \"user\", \"content\": user_input},\n",
    "    ]\n",
    "    return {\n",
    "        \"query\": tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True),\n",
    "        \"ground_truth\": ex.get(\"output\", \"\")\n",
    "    }\n",
    "\n",
    "# Format data for GRPO\n",
    "grpo_formatted = train_data.map(format_chat_grpo, remove_columns=train_data.column_names)\n",
    "print(f\"GRPO data prepared: {len(grpo_formatted)} samples\")\n",
    "\n",
    "# Create batches for GRPO training\n",
    "def create_grpo_batches(dataset, batch_size=2):\n",
    "    batches = []\n",
    "    for i in range(0, len(dataset), batch_size):\n",
    "        batch = dataset[i:i + batch_size]\n",
    "        queries = [item[\"query\"] for item in batch]\n",
    "        ground_truths = [item[\"ground_truth\"] for item in batch]\n",
    "        batches.append((queries, ground_truths))\n",
    "    return batches\n",
    "\n",
    "grpo_batches = create_grpo_batches(grpo_formatted, batch_size=GRPO_CONFIG[\"per_device_train_batch_size\"])\n",
    "print(f\"Created {len(grpo_batches)} GRPO batches\")\n",
    "\n",
    "# Show sample GRPO data\n",
    "print(\"\\n=== Sample GRPO Query ===\")\n",
    "print(grpo_formatted[0][\"query\"][:300] + \"...\")\n",
    "print(f\"\\nGround truth: {grpo_formatted[0]['ground_truth'][:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5629a93",
   "metadata": {},
   "source": [
    "## GRPO Trainer Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "807a2e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRPOTrainer:\n",
    "    def __init__(self, model, tokenizer, config):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.config = config\n",
    "        self.device = model.device\n",
    "        \n",
    "    def generate_responses(self, queries, num_responses=4, temperature=0.7, max_length=512):\n",
    "        \"\"\"\n",
    "        Generate multiple responses for each query\n",
    "        \"\"\"\n",
    "        self.model.eval()\n",
    "        responses = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for query in queries:\n",
    "                query_responses = []\n",
    "                inputs = self.tokenizer(query, return_tensors=\"pt\", truncation=True, max_length=2048)\n",
    "                inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "                \n",
    "                for _ in range(num_responses):\n",
    "                    outputs = self.model.generate(\n",
    "                        **inputs,\n",
    "                        max_length=inputs[\"input_ids\"].shape[1] + max_length,\n",
    "                        temperature=temperature,\n",
    "                        do_sample=True,\n",
    "                        pad_token_id=self.tokenizer.eos_token_id,\n",
    "                        eos_token_id=self.tokenizer.eos_token_id,\n",
    "                    )\n",
    "                    \n",
    "                    response = self.tokenizer.decode(\n",
    "                        outputs[0][inputs[\"input_ids\"].shape[1]:], \n",
    "                        skip_special_tokens=True\n",
    "                    )\n",
    "                    query_responses.append(response.strip())\n",
    "                \n",
    "                responses.append(query_responses)\n",
    "        \n",
    "        return responses\n",
    "    \n",
    "    def compute_rewards(self, queries, responses, ground_truths):\n",
    "        \"\"\"\n",
    "        Compute rewards for responses based on Bitcoin prediction quality\n",
    "        \"\"\"\n",
    "        rewards = []\n",
    "        \n",
    "        for query_responses, gt in zip(responses, ground_truths):\n",
    "            query_rewards = []\n",
    "            \n",
    "            for response in query_responses:\n",
    "                reward = self._calculate_response_reward(response, gt)\n",
    "                query_rewards.append(reward)\n",
    "            \n",
    "            rewards.append(query_rewards)\n",
    "        \n",
    "        return rewards\n",
    "    \n",
    "    def _calculate_response_reward(self, response, ground_truth):\n",
    "        \"\"\"\n",
    "        Calculate reward for a response based on Bitcoin prediction criteria\n",
    "        \"\"\"\n",
    "        reward = 0.0\n",
    "        \n",
    "        # Length penalty/reward\n",
    "        if 50 <= len(response) <= 500:\n",
    "            reward += 0.1\n",
    "        \n",
    "        # Format check (contains numbers/predictions)\n",
    "        if any(char.isdigit() for char in response):\n",
    "            reward += 0.2\n",
    "        \n",
    "        # Contains forecast tag\n",
    "        if FORECAST_TAG in response:\n",
    "            reward += 0.3\n",
    "        \n",
    "        # Contains analysis keywords\n",
    "        bitcoin_keywords = ['bitcoin', 'price', 'forecast', 'prediction', 'market', 'trend']\n",
    "        response_lower = response.lower()\n",
    "        keyword_count = sum(1 for keyword in bitcoin_keywords if keyword in response_lower)\n",
    "        reward += min(0.2, keyword_count * 0.05)\n",
    "        \n",
    "        # Simple text similarity with ground truth\n",
    "        common_words = set(response.lower().split()) & set(ground_truth.lower().split())\n",
    "        if len(common_words) > 0:\n",
    "            reward += min(0.4, len(common_words) * 0.02)\n",
    "        \n",
    "        return reward\n",
    "    \n",
    "    def compute_grpo_loss(self, queries, responses, rewards):\n",
    "        \"\"\"\n",
    "        Compute GRPO loss based on relative preferences within groups\n",
    "        \"\"\"\n",
    "        self.model.train()\n",
    "        total_loss = 0.0\n",
    "        num_pairs = 0\n",
    "        \n",
    "        for query, query_responses, query_rewards in zip(queries, responses, rewards):\n",
    "            if len(query_responses) < 2:\n",
    "                continue\n",
    "            \n",
    "            # Tokenize query and responses\n",
    "            query_tokens = self.tokenizer(query, return_tensors=\"pt\", truncation=True, max_length=2048)\n",
    "            query_tokens = {k: v.to(self.device) for k, v in query_tokens.items()}\n",
    "            \n",
    "            response_logprobs = []\n",
    "            \n",
    "            for response in query_responses:\n",
    "                # Create full text (query + response)\n",
    "                full_text = query + \" \" + response\n",
    "                tokens = self.tokenizer(full_text, return_tensors=\"pt\", truncation=True, max_length=4096)\n",
    "                tokens = {k: v.to(self.device) for k, v in tokens.items()}\n",
    "                \n",
    "                # Get model outputs\n",
    "                outputs = self.model(**tokens)\n",
    "                logits = outputs.logits\n",
    "                \n",
    "                # Calculate log probabilities for the response part\n",
    "                query_length = query_tokens[\"input_ids\"].shape[1]\n",
    "                response_logits = logits[0, query_length-1:-1]  # Shift for next token prediction\n",
    "                response_tokens = tokens[\"input_ids\"][0, query_length:]\n",
    "                \n",
    "                if response_tokens.numel() > 0:\n",
    "                    log_probs = F.log_softmax(response_logits, dim=-1)\n",
    "                    response_log_prob = log_probs.gather(1, response_tokens.unsqueeze(-1)).squeeze(-1)\n",
    "                    avg_log_prob = response_log_prob.mean()\n",
    "                    response_logprobs.append(avg_log_prob)\n",
    "                else:\n",
    "                    response_logprobs.append(torch.tensor(0.0, device=self.device))\n",
    "            \n",
    "            if len(response_logprobs) >= 2:\n",
    "                # Compute pairwise losses\n",
    "                for i in range(len(response_logprobs)):\n",
    "                    for j in range(i + 1, len(response_logprobs)):\n",
    "                        if query_rewards[i] != query_rewards[j]:  # Only if different rewards\n",
    "                            # Preference: higher reward should have higher log prob\n",
    "                            if query_rewards[i] > query_rewards[j]:\n",
    "                                preferred_logprob = response_logprobs[i]\n",
    "                                dispreferred_logprob = response_logprobs[j]\n",
    "                            else:\n",
    "                                preferred_logprob = response_logprobs[j]\n",
    "                                dispreferred_logprob = response_logprobs[i]\n",
    "                            \n",
    "                            # GRPO loss: negative log sigmoid of difference\n",
    "                            diff = preferred_logprob - dispreferred_logprob\n",
    "                            loss = -F.logsigmoid(diff)\n",
    "                            total_loss += loss\n",
    "                            num_pairs += 1\n",
    "        \n",
    "        return total_loss / max(num_pairs, 1)\n",
    "    \n",
    "    def train_step(self, batch_queries, batch_ground_truths):\n",
    "        \"\"\"\n",
    "        Perform one training step of GRPO\n",
    "        \"\"\"\n",
    "        # Generate responses\n",
    "        responses = self.generate_responses(\n",
    "            batch_queries, \n",
    "            num_responses=self.config[\"group_size\"],\n",
    "            temperature=self.config[\"temperature\"]\n",
    "        )\n",
    "        \n",
    "        # Compute rewards\n",
    "        rewards = self.compute_rewards(batch_queries, responses, batch_ground_truths)\n",
    "        \n",
    "        # Compute and return GRPO loss\n",
    "        loss = self.compute_grpo_loss(batch_queries, responses, rewards)\n",
    "        \n",
    "        return loss, responses, rewards\n",
    "\n",
    "print(\"✅ GRPO Trainer class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9efea9f",
   "metadata": {},
   "source": [
    "## GRPO Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71fc66a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize GRPO trainer\n",
    "grpo_trainer = GRPOTrainer(model, tokenizer, GRPO_CONFIG)\n",
    "\n",
    "# Setup optimizer for GRPO\n",
    "optimizer = AdamW(model.parameters(), lr=GRPO_CONFIG[\"learning_rate\"])\n",
    "\n",
    "print(\"🚀 Starting GRPO training...\")\n",
    "print(f\"Training on {len(grpo_batches)} batches for {GRPO_CONFIG['num_train_epochs']} epochs\")\n",
    "\n",
    "# Training loop\n",
    "training_logs = []\n",
    "global_step = 0\n",
    "\n",
    "for epoch in range(GRPO_CONFIG[\"num_train_epochs\"]):\n",
    "    epoch_loss = 0.0\n",
    "    epoch_batches = 0\n",
    "    \n",
    "    progress_bar = tqdm(grpo_batches, desc=f\"GRPO Epoch {epoch+1}/{GRPO_CONFIG['num_train_epochs']}\")\n",
    "    \n",
    "    for batch_queries, batch_ground_truths in progress_bar:\n",
    "        try:\n",
    "            # Perform training step\n",
    "            loss, responses, rewards = grpo_trainer.train_step(batch_queries, batch_ground_truths)\n",
    "            \n",
    "            if loss.requires_grad:\n",
    "                # Backward pass\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                \n",
    "                # Gradient clipping\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "                \n",
    "                optimizer.step()\n",
    "                \n",
    "                epoch_loss += loss.item()\n",
    "                epoch_batches += 1\n",
    "                global_step += 1\n",
    "                \n",
    "                # Update progress bar\n",
    "                progress_bar.set_postfix({\n",
    "                    'loss': f'{loss.item():.4f}',\n",
    "                    'avg_loss': f'{epoch_loss/epoch_batches:.4f}'\n",
    "                })\n",
    "                \n",
    "                # Logging\n",
    "                if global_step % GRPO_CONFIG[\"logging_steps\"] == 0:\n",
    "                    log_entry = {\n",
    "                        \"epoch\": epoch,\n",
    "                        \"step\": global_step,\n",
    "                        \"loss\": loss.item(),\n",
    "                        \"avg_epoch_loss\": epoch_loss / epoch_batches,\n",
    "                        \"timestamp\": datetime.now().isoformat()\n",
    "                    }\n",
    "                    training_logs.append(log_entry)\n",
    "                    \n",
    "                    print(f\"\\nStep {global_step}: Loss = {loss.item():.4f}\")\n",
    "                    print(f\"Sample responses for batch:\")\n",
    "                    for i, (query_responses, query_rewards) in enumerate(zip(responses[:1], rewards[:1])):\n",
    "                        print(f\"  Query {i+1} responses:\")\n",
    "                        for j, (resp, rew) in enumerate(zip(query_responses[:2], query_rewards[:2])):\n",
    "                            print(f\"    Response {j+1} (reward: {rew:.3f}): {resp[:100]}...\")\n",
    "                \n",
    "                # Save checkpoint\n",
    "                if global_step % GRPO_CONFIG[\"save_steps\"] == 0:\n",
    "                    checkpoint_dir = f\"{GRPO_CONFIG['output_dir']}/checkpoint-{global_step}\"\n",
    "                    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "                    model.save_pretrained(f\"{checkpoint_dir}/lora_adapter\")\n",
    "                    tokenizer.save_pretrained(checkpoint_dir)\n",
    "                    print(f\"\\n💾 Checkpoint saved at step {global_step}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"\\n⚠️ Error in batch: {e}\")\n",
    "            continue\n",
    "    \n",
    "    avg_epoch_loss = epoch_loss / max(epoch_batches, 1)\n",
    "    print(f\"\\n✅ Epoch {epoch+1} completed. Average loss: {avg_epoch_loss:.4f}\")\n",
    "\n",
    "print(\"\\n🎉 GRPO training completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb968f09",
   "metadata": {},
   "source": [
    "## Save GRPO Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e6d3697",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save final GRPO model\n",
    "final_model_dir = f\"{GRPO_CONFIG['output_dir']}/final_model\"\n",
    "os.makedirs(final_model_dir, exist_ok=True)\n",
    "\n",
    "model.save_pretrained(f\"{final_model_dir}/lora_adapter\")\n",
    "tokenizer.save_pretrained(final_model_dir)\n",
    "\n",
    "print(f\"✅ Final GRPO model saved to {final_model_dir}\")\n",
    "\n",
    "# Save GRPO training logs\n",
    "with open(f\"{GRPO_CONFIG['output_dir']}/grpo_training_logs.json\", \"w\") as f:\n",
    "    json.dump(training_logs, f, indent=2)\n",
    "\n",
    "print(f\"GRPO training logs saved to {GRPO_CONFIG['output_dir']}/grpo_training_logs.json\")\n",
    "\n",
    "# Create training summary\n",
    "training_summary = {\n",
    "    \"dataset\": DATASET_NAME,\n",
    "    \"model_config\": MODEL_CONFIG,\n",
    "    \"grpo_config\": GRPO_CONFIG,\n",
    "    \"total_samples\": len(train_data),\n",
    "    \"total_grpo_steps\": global_step,\n",
    "    \"training_completed\": datetime.now().isoformat(),\n",
    "    \"final_model_path\": final_model_dir\n",
    "}\n",
    "\n",
    "with open(f\"{GRPO_CONFIG['output_dir']}/training_summary.json\", \"w\") as f:\n",
    "    json.dump(training_summary, f, indent=2)\n",
    "\n",
    "print(f\"Training summary saved to {GRPO_CONFIG['output_dir']}/training_summary.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4ae6376",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c46877b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the final model\n",
    "print(\"🧪 Testing the final GRPO model...\")\n",
    "\n",
    "def test_model_generation(model, tokenizer, test_query, max_length=512):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        inputs = tokenizer(test_query, return_tensors=\"pt\", truncation=True, max_length=2048)\n",
    "        inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "        \n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_length=inputs[\"input_ids\"].shape[1] + max_length,\n",
    "            temperature=0.7,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "        \n",
    "        response = tokenizer.decode(\n",
    "            outputs[0][inputs[\"input_ids\"].shape[1]:], \n",
    "            skip_special_tokens=True\n",
    "        )\n",
    "        return response.strip()\n",
    "\n",
    "# Create a test query\n",
    "test_sample = grpo_formatted[0]\n",
    "test_query = test_sample[\"query\"]\n",
    "ground_truth = test_sample[\"ground_truth\"]\n",
    "\n",
    "print(\"=== Test Query ===\")\n",
    "print(test_query[:500] + \"...\")\n",
    "\n",
    "print(\"\\n=== Ground Truth ===\")\n",
    "print(ground_truth)\n",
    "\n",
    "print(\"\\n=== Model Response ===\")\n",
    "response = test_model_generation(model, tokenizer, test_query)\n",
    "print(response)\n",
    "\n",
    "print(\"\\n✅ Model evaluation completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ccd06f",
   "metadata": {},
   "source": [
    "## Training Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b8dd852",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"📊 GRPO Training Summary\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Dataset: {DATASET_NAME}\")\n",
    "print(f\"Total samples: {len(train_data):,}\")\n",
    "print(f\"Training method: Group Relative Policy Optimization (GRPO)\")\n",
    "print(\"\\n🎯 Training Configuration:\")\n",
    "print(f\"GRPO epochs: {GRPO_CONFIG['num_train_epochs']}\")\n",
    "print(f\"Total GRPO steps: {global_step}\")\n",
    "print(f\"Batch size: {GRPO_CONFIG['per_device_train_batch_size']}\")\n",
    "print(f\"Learning rate: {GRPO_CONFIG['learning_rate']}\")\n",
    "print(f\"Group size: {GRPO_CONFIG['group_size']}\")\n",
    "print(\"\\n💾 Model Output:\")\n",
    "print(f\"Final GRPO model: {final_model_dir}\")\n",
    "print(\"\\n🔬 Key Features:\")\n",
    "print(\"✅ Group Relative Policy Optimization (GRPO) for preference learning\")\n",
    "print(\"✅ Custom reward system for Bitcoin prediction quality\")\n",
    "print(\"✅ Multi-response generation and ranking\")\n",
    "print(\"✅ Pairwise preference optimization\")\n",
    "print(\"✅ Special token handling for structured outputs\")\n",
    "print(\"\\n🎉 GRPO training pipeline completed successfully!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
