{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fbc4b481",
   "metadata": {},
   "source": [
    "# Bitcoin Notebook Agent Demo\n",
    "\n",
    "This notebook demonstrates a robust utility layer enabling an agent to operate over `.ipynb` files: parsing, dependency analysis, selective execution, caching, error repair suggestions, and advisory generation integration.\n",
    "\n",
    "Sections follow the required outline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "532cba92",
   "metadata": {},
   "source": [
    "## 1. Environment Setup and Dependency Installation\n",
    "Install required packages (run once). Safe to skip if environment already prepared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "398b4e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Optional) Install runtime dependencies. Comment out after first run.\n",
    "import sys, subprocess, textwrap\n",
    "pkgs = [\n",
    "    'nbformat', 'nbclient', 'rich', 'tqdm', 'black', 'openai', 'google-generativeai'\n",
    "]\n",
    "for p in pkgs:\n",
    "    try:\n",
    "        __import__(p.split('==')[0])\n",
    "    except ImportError:\n",
    "        print(f'Installing {p}...')\n",
    "        subprocess.run([sys.executable, '-m', 'pip', 'install', p], check=False)\n",
    "print('Environment check complete.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33a446d2",
   "metadata": {},
   "source": [
    "## 2. Load and Parse .ipynb File Structure\n",
    "We load a target notebook using nbformat and inspect cell metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7382c98a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nbformat, os, json, hashlib, ast, time\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any, Tuple\n",
    "from rich import print as rprint\n",
    "\n",
    "TARGET_NOTEBOOK = os.environ.get('TARGET_NOTEBOOK', 'agents/pipeline_agent.py')  # can also point to a .ipynb\n",
    "\n",
    "def load_notebook(path: str):\n",
    "    if path.endswith('.ipynb'):\n",
    "        with open(path, 'r', encoding='utf-8') as f:\n",
    "            nb = nbformat.read(f, as_version=4)\n",
    "        rprint(f\"[bold green]Loaded notebook with {len(nb.cells)} cells[/bold green]\")\n",
    "        return nb\n",
    "    else:\n",
    "        rprint('[yellow]Target is a .py file; wrapping into pseudo-notebook cells.[/yellow]')\n",
    "        with open(path, 'r', encoding='utf-8') as f:\n",
    "            code = f.read()\n",
    "        return nbformat.v4.new_notebook(cells=[nbformat.v4.new_code_cell(code)])\n",
    "\n",
    "nb_obj = load_notebook(TARGET_NOTEBOOK)\n",
    "nbformat_minor = nb_obj.get('nbformat_minor', 5)\n",
    "rprint({'nbformat': nb_obj.get('nbformat', 4), 'nbformat_minor': nbformat_minor})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d25d5d",
   "metadata": {},
   "source": [
    "## 3. Extract Code Cells and Metadata\n",
    "Filter only code cells and capture execution counts, ids, tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef102e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_code_cells(nb) -> List[Dict[str, Any]]:\n",
    "    code_cells = []\n",
    "    for idx, cell in enumerate(nb.cells):\n",
    "        if cell.get('cell_type') == 'code':\n",
    "            code_cells.append({\n",
    "                'index': idx,\n",
    "                'id': cell.get('id'),\n",
    "                'execution_count': cell.get('execution_count'),\n",
    "                'source': cell.get('source', ''),\n",
    "                'tags': cell.get('metadata', {}).get('tags', [])\n",
    "            })\n",
    "    return code_cells\n",
    "\n",
    "code_cells = extract_code_cells(nb_obj)\n",
    "print(f\"Discovered {len(code_cells)} code cells\")\n",
    "code_cells[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "677436cd",
   "metadata": {},
   "source": [
    "## 4. Normalize and Clean Cell Source Code\n",
    "We standardize whitespace and optionally format with `black` (if installed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_source(src: str, do_format: bool = True) -> str:\n",
    "    cleaned = '\\n'.join(line.rstrip() for line in src.replace('\\r\\n', '\\n').split('\\n'))\n",
    "    if do_format:\n",
    "        try:\n",
    "            import black\n",
    "            cleaned = black.format_str(cleaned, mode=black.Mode())\n",
    "        except Exception:\n",
    "            pass\n",
    "    return cleaned\n",
    "\n",
    "for c in code_cells:\n",
    "    c['normalized_source'] = normalize_source(c['source'])\n",
    "\n",
    "code_cells[0]['normalized_source'][:300] if code_cells else 'No code cells'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "290c3a4b",
   "metadata": {},
   "source": [
    "## 5. Build Cell Dependency Graph (Imports and Variables)\n",
    "Parse AST to gather imported modules and top-level assigned symbols per cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f51d410",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_cell_symbols(src: str) -> Tuple[set, set]:\n",
    "    imports, assigns = set(), set()\n",
    "    try:\n",
    "        tree = ast.parse(src)\n",
    "    except Exception:\n",
    "        return imports, assigns\n",
    "    for node in ast.walk(tree):\n",
    "        if isinstance(node, (ast.Import, ast.ImportFrom)):\n",
    "            for n in node.names:\n",
    "                imports.add(n.name.split('.')[0])\n",
    "        elif isinstance(node, ast.Assign):\n",
    "            for t in node.targets:\n",
    "                if isinstance(t, ast.Name):\n",
    "                    assigns.add(t.id)\n",
    "                elif isinstance(t, (ast.Tuple, ast.List)):\n",
    "                    for elt in t.elts:\n",
    "                        if isinstance(elt, ast.Name):\n",
    "                            assigns.add(elt.id)\n",
    "        elif isinstance(node, ast.FunctionDef):\n",
    "            assigns.add(node.name)\n",
    "        elif isinstance(node, ast.ClassDef):\n",
    "            assigns.add(node.name)\n",
    "    return imports, assigns\n",
    "\n",
    "for c in code_cells:\n",
    "    imps, defs = analyze_cell_symbols(c['normalized_source'])\n",
    "    c['imports'] = sorted(imps)\n",
    "    c['defines'] = sorted(defs)\n",
    "\n",
    "dep_index = {c['index']: {'imports': c['imports'], 'defines': c['defines']} for c in code_cells}\n",
    "list(dep_index.items())[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61843e09",
   "metadata": {},
   "source": [
    "## 6. Assemble Notebook Context for Agent Prompt\n",
    "We build a trimmed context window summarizing each cell for LLM prompting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "715957b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cell_hash(src: str) -> str:\n",
    "    return hashlib.sha256(src.encode('utf-8')).hexdigest()[:12]\n",
    "\n",
    "MAX_PREVIEW_LINES = 12\n",
    "\n",
    "def assemble_context(cells: List[Dict[str, Any]]) -> str:\n",
    "    blocks = []\n",
    "    for c in cells:\n",
    "        lines = c['normalized_source'].split('\\n')\n",
    "        preview = '\\n'.join(lines[:MAX_PREVIEW_LINES])\n",
    "        blocks.append(\n",
    "            f\"CELL {c['index']} | id={c.get('id')} | hash={cell_hash(c['normalized_source'])}\\n\"\n",
    "            f\"Defines: {', '.join(c['defines']) or '-'} | Imports: {', '.join(c['imports']) or '-'}\\n\"\n",
    "            f\"---\\n{preview}\\n\"\n",
    "        )\n",
    "    return '\\n'.join(blocks)\n",
    "\n",
    "context_snapshot = assemble_context(code_cells)\n",
    "print(context_snapshot[:1000])"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
