{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f570f82",
   "metadata": {},
   "source": [
    "# Bitcoin Investment Advisory Training Dataset Creator\n",
    "\n",
    "This notebook creates structured input-output pairs for training AI models from the comprehensive Bitcoin investment advisory dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d71b4988",
   "metadata": {},
   "source": [
    "## 1. Setup and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a62ca36c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Any\n",
    "import os\n",
    "from datasets import Dataset\n",
    "from huggingface_hub import HfApi, create_repo\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Install required packages if not available\n",
    "try:\n",
    "    from datasets import Dataset\n",
    "except ImportError:\n",
    "    !pip install datasets\n",
    "    from datasets import Dataset\n",
    "\n",
    "try:\n",
    "    from huggingface_hub import HfApi\n",
    "except ImportError:\n",
    "    !pip install huggingface_hub\n",
    "    from huggingface_hub import HfApi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ff16f2",
   "metadata": {},
   "source": [
    "## 2. Load and Analyze Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74fc2232",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Name: Ultra-Comprehensive Bitcoin Investment Advisory Dataset\n",
      "Version: 2.0.ultra-complete\n",
      "Total Samples: 2437\n",
      "Date Range: 2018-01-01 to 2024-12-31\n"
     ]
    }
   ],
   "source": [
    "# Load the comprehensive dataset\n",
    "dataset_path = 'bitcoin_investment_advisory_ULTRA_COMPREHENSIVE_20250907_232548.json'\n",
    "\n",
    "with open(dataset_path, 'r', encoding='utf-8') as f:\n",
    "    raw_data = json.load(f)\n",
    "\n",
    "print(f\"Dataset Name: {raw_data['dataset_metadata']['name']}\")\n",
    "print(f\"Version: {raw_data['dataset_metadata']['version']}\")\n",
    "print(f\"Total Samples: {raw_data['processing_statistics']['execution_summary']['total_advisory_samples']}\")\n",
    "print(f\"Date Range: {raw_data['processing_statistics']['data_coverage_analysis']['date_range']['earliest_date']} to {raw_data['processing_statistics']['data_coverage_analysis']['date_range']['latest_date']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6473832a",
   "metadata": {},
   "source": [
    "## 3. Define Training Data Structure\n",
    "\n",
    "We'll create comprehensive input-output pairs that include all relevant context for training an investment advisory AI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3d27e3f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data formatting functions defined successfully!\n"
     ]
    }
   ],
   "source": [
    "def format_market_data(market_data: Dict) -> str:\n",
    "    \"\"\"Format market data into a readable string for input.\"\"\"\n",
    "    \n",
    "    # Format future prices\n",
    "    next_10_prices = market_data.get('next_10_day_prices', [])\n",
    "    next_60_prices = market_data.get('next_60_day_prices', [])\n",
    "    \n",
    "    market_text = f\"\"\"\n",
    "MARKET DATA:\n",
    "- Current Price: ${market_data.get('current_price', 'N/A')}\n",
    "- Price Range: Min: ${market_data.get('price_range', {}).get('min', 'N/A')}, Max: ${market_data.get('price_range', {}).get('max', 'N/A')}\n",
    "- Next 10-Day Price Trend: {', '.join([f'${p:.2f}' for p in next_10_prices])}\n",
    "- Next 60-Day Price Outlook: {', '.join([f'${p:.2f}' for p in next_60_prices])}\n",
    "\"\"\"\n",
    "    return market_text.strip()\n",
    "\n",
    "def format_news_analysis(news_analysis: Dict) -> str:\n",
    "    \"\"\"Format news analysis into a readable string for input.\"\"\"\n",
    "    \n",
    "    sentiment = news_analysis.get('market_sentiment', {})\n",
    "    high_impact_news = news_analysis.get('high_impact_news', [])\n",
    "    \n",
    "    news_text = f\"\"\"\n",
    "NEWS ANALYSIS:\n",
    "- Total News Items: {news_analysis.get('total_news_items', 0)}\n",
    "- Market Sentiment: {sentiment.get('overall_sentiment', 'Neutral')}\n",
    "- Bull Probability: {sentiment.get('bull_probability', 0):.1%}\n",
    "- Bear Probability: {sentiment.get('bear_probability', 0):.1%}\n",
    "- High Impact News Count: {len(high_impact_news)}\n",
    "\n",
    "KEY NEWS ITEMS:\n",
    "\"\"\"\n",
    "    \n",
    "    for i, news in enumerate(high_impact_news, 1):  # All news items\n",
    "        news_text += f\"\"\"\n",
    "{i}. {news.get('title', 'No title')}\n",
    "   Summary: {news.get('summary', 'No summary')}\n",
    "   Direction: {news.get('direction', 'neutral').upper()}\n",
    "   Impact: {news.get('magnitude', 'medium').upper()}\n",
    "   Confidence: {news.get('confidence', 0):.0%}\n",
    "   Impact Tags: {', '.join(news.get('impact_tags', []))}\n",
    "\"\"\"\n",
    "    \n",
    "    return news_text.strip()\n",
    "\n",
    "def format_daily_analysis(daily_analysis: Dict) -> str:\n",
    "    \"\"\"Format daily analysis into a readable string for input.\"\"\"\n",
    "    \n",
    "    analysis_text = f\"\"\"\n",
    "DAILY MARKET ANALYSIS:\n",
    "- Market Summary: {daily_analysis.get('summary', 'No summary available')}\n",
    "- Aggregated Effects: {daily_analysis.get('aggregated_effects', 'No effects data')}\n",
    "- Key Events: {len(daily_analysis.get('key_events', []))} events identified\n",
    "- Price Drivers: {len(daily_analysis.get('price_drivers', []))} factors analyzed\n",
    "\"\"\"\n",
    "    \n",
    "    # Add key events if available\n",
    "    key_events = daily_analysis.get('key_events', [])\n",
    "    if key_events:\n",
    "        analysis_text += \"\\nKEY EVENTS:\\n\"\n",
    "        for i, event in enumerate(key_events, 1):\n",
    "            analysis_text += f\"{i}. {event}\\n\"\n",
    "    \n",
    "    # Add price drivers if available\n",
    "    price_drivers = daily_analysis.get('price_drivers', [])\n",
    "    if price_drivers:\n",
    "        analysis_text += \"\\nPRICE DRIVERS:\\n\"\n",
    "        for i, driver in enumerate(price_drivers, 1):\n",
    "            analysis_text += f\"{i}. {driver}\\n\"\n",
    "    \n",
    "    return analysis_text.strip()\n",
    "\n",
    "def create_comprehensive_input(sample: Dict) -> str:\n",
    "    \"\"\"Create a comprehensive input prompt from a sample.\"\"\"\n",
    "    \n",
    "    date = sample.get('date', 'Unknown')\n",
    "    market_data = sample.get('market_data', {})\n",
    "    news_analysis = sample.get('news_analysis', {})\n",
    "    daily_analysis = sample.get('daily_analysis', {})\n",
    "    \n",
    "    input_text = f\"\"\"\n",
    "You are an elite institutional Bitcoin investment advisor. Please provide a comprehensive investment advisory based on the following market intelligence for {date}.\n",
    "\n",
    "{format_market_data(market_data)}\n",
    "\n",
    "{format_news_analysis(news_analysis)}\n",
    "\n",
    "{format_daily_analysis(daily_analysis)}\n",
    "\n",
    "TASK:\n",
    "Provide a detailed, institutional-grade Bitcoin investment advisory that includes:\n",
    "1. Executive Summary & Market Overview\n",
    "2. Investment Recommendation (Short/Medium/Long-term)\n",
    "3. Risk Assessment & Management\n",
    "4. Price Targets & Scenarios\n",
    "5. Trading Strategy & Execution\n",
    "6. Technical and Fundamental Analysis\n",
    "7. Portfolio Integration Advice\n",
    "\n",
    "Format your response as a professional investment advisory suitable for institutional clients.\n",
    "\"\"\"\n",
    "    \n",
    "    return input_text.strip()\n",
    "\n",
    "def extract_advisory_output(sample: Dict) -> str:\n",
    "    \"\"\"Extract the advisory output from a sample.\"\"\"\n",
    "    \n",
    "    advisory_output = sample.get('advisory_output', {})\n",
    "    return advisory_output.get('advisory_text', '')\n",
    "\n",
    "print(\"Training data formatting functions defined successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "172e6a23",
   "metadata": {},
   "source": [
    "## 4. Create Training Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a7d6c6d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 2437 samples...\n",
      "Processed 0/2437 samples\n",
      "Processed 100/2437 samples\n",
      "Processed 200/2437 samples\n",
      "Processed 300/2437 samples\n",
      "Processed 400/2437 samples\n",
      "Processed 500/2437 samples\n",
      "Processed 600/2437 samples\n",
      "Processed 700/2437 samples\n",
      "Processed 800/2437 samples\n",
      "Processed 900/2437 samples\n",
      "Processed 1000/2437 samples\n",
      "Processed 1100/2437 samples\n",
      "Processed 1200/2437 samples\n",
      "Processed 1300/2437 samples\n",
      "Processed 1400/2437 samples\n",
      "Processed 1500/2437 samples\n",
      "Processed 1600/2437 samples\n",
      "Processed 1700/2437 samples\n",
      "Processed 1800/2437 samples\n",
      "Processed 1900/2437 samples\n",
      "Processed 2000/2437 samples\n",
      "Processed 2100/2437 samples\n",
      "Processed 2200/2437 samples\n",
      "Processed 2300/2437 samples\n",
      "Processed 2400/2437 samples\n",
      "Created 2437 training samples\n",
      "\n",
      "Training Dataset Summary:\n",
      "Total samples: 2437\n",
      "Average input length: 4865 characters\n",
      "Average output length: 8643 characters\n",
      "Average quality score: 0.90\n",
      "Samples with news data: 2419\n",
      "Samples with price data: 2437\n"
     ]
    }
   ],
   "source": [
    "def create_training_samples(raw_data: Dict) -> List[Dict]:\n",
    "    \"\"\"Create training samples from raw data.\"\"\"\n",
    "    \n",
    "    samples = raw_data.get('comprehensive_samples', [])\n",
    "    training_data = []\n",
    "    \n",
    "    print(f\"Processing {len(samples)} samples...\")\n",
    "    \n",
    "    for i, sample in enumerate(samples):\n",
    "        if i % 100 == 0:\n",
    "            print(f\"Processed {i}/{len(samples)} samples\")\n",
    "        \n",
    "        # Create comprehensive input\n",
    "        input_text = create_comprehensive_input(sample)\n",
    "        \n",
    "        # Extract output\n",
    "        output_text = extract_advisory_output(sample)\n",
    "        \n",
    "        # Skip samples with insufficient data\n",
    "        if len(input_text.strip()) < 100 or len(output_text.strip()) < 100:\n",
    "            continue\n",
    "        \n",
    "        # Create training sample\n",
    "        training_sample = {\n",
    "            'date': sample.get('date', ''),\n",
    "            'input': input_text,\n",
    "            'output': output_text,\n",
    "            'instruction': 'You are an elite institutional Bitcoin investment advisor. Provide comprehensive investment advisory based on the given market intelligence.',\n",
    "            \n",
    "            # Additional metadata\n",
    "            'sample_index': sample.get('sample_index', i),\n",
    "            'input_length': len(input_text),\n",
    "            'output_length': len(output_text),\n",
    "            'has_news_data': len(sample.get('news_analysis', {}).get('high_impact_news', [])) > 0,\n",
    "            'has_price_data': len(sample.get('market_data', {}).get('next_10_day_prices', [])) > 0,\n",
    "            \n",
    "            # Quality metrics\n",
    "            'quality_score': calculate_quality_score(sample),\n",
    "        }\n",
    "        \n",
    "        training_data.append(training_sample)\n",
    "    \n",
    "    print(f\"Created {len(training_data)} training samples\")\n",
    "    return training_data\n",
    "\n",
    "def calculate_quality_score(sample: Dict) -> float:\n",
    "    \"\"\"Calculate a quality score for the sample based on data completeness.\"\"\"\n",
    "    \n",
    "    score = 0.0\n",
    "    \n",
    "    # Check market data quality\n",
    "    market_data = sample.get('market_data', {})\n",
    "    if len(market_data.get('next_10_day_prices', [])) > 0:\n",
    "        score += 0.3\n",
    "    if len(market_data.get('next_60_day_prices', [])) > 0:\n",
    "        score += 0.2\n",
    "    \n",
    "    # Check news data quality\n",
    "    news_analysis = sample.get('news_analysis', {})\n",
    "    if len(news_analysis.get('high_impact_news', [])) > 0:\n",
    "        score += 0.2\n",
    "    if news_analysis.get('total_news_items', 0) > 0:\n",
    "        score += 0.1\n",
    "    \n",
    "    # Check advisory quality\n",
    "    advisory = sample.get('advisory_output', {})\n",
    "    if advisory.get('advisory_length_chars', 0) > 1000:\n",
    "        score += 0.1\n",
    "    if advisory.get('generation_successful', False):\n",
    "        score += 0.1\n",
    "    \n",
    "    return min(score, 1.0)\n",
    "\n",
    "# Create training samples\n",
    "training_samples = create_training_samples(raw_data)\n",
    "\n",
    "print(f\"\\nTraining Dataset Summary:\")\n",
    "print(f\"Total samples: {len(training_samples)}\")\n",
    "print(f\"Average input length: {np.mean([s['input_length'] for s in training_samples]):.0f} characters\")\n",
    "print(f\"Average output length: {np.mean([s['output_length'] for s in training_samples]):.0f} characters\")\n",
    "print(f\"Average quality score: {np.mean([s['quality_score'] for s in training_samples]):.2f}\")\n",
    "print(f\"Samples with news data: {sum(s['has_news_data'] for s in training_samples)}\")\n",
    "print(f\"Samples with price data: {sum(s['has_price_data'] for s in training_samples)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29ca46ec",
   "metadata": {},
   "source": [
    "## 5. Create Different Training Formats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a3fb4102",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created training data in multiple formats:\n",
      "- Instruction format: 2437 samples\n",
      "- Chat format: 2437 samples\n",
      "- Alpaca format: 2437 samples\n"
     ]
    }
   ],
   "source": [
    "def create_instruction_format(samples: List[Dict]) -> List[Dict]:\n",
    "    \"\"\"Create instruction-tuning format.\"\"\"\n",
    "    \n",
    "    instruction_data = []\n",
    "    \n",
    "    for sample in samples:\n",
    "        instruction_sample = {\n",
    "            'instruction': sample['instruction'],\n",
    "            'input': sample['input'],\n",
    "            'output': sample['output'],\n",
    "            'date': sample['date'],\n",
    "            'quality_score': sample['quality_score']\n",
    "        }\n",
    "        instruction_data.append(instruction_sample)\n",
    "    \n",
    "    return instruction_data\n",
    "\n",
    "def create_chat_format(samples: List[Dict]) -> List[Dict]:\n",
    "    \"\"\"Create chat format for conversational training.\"\"\"\n",
    "    \n",
    "    chat_data = []\n",
    "    \n",
    "    for sample in samples:\n",
    "        chat_sample = {\n",
    "            'messages': [\n",
    "                {\n",
    "                    'role': 'system',\n",
    "                    'content': 'You are an elite institutional Bitcoin investment advisor with deep expertise in cryptocurrency markets, technical analysis, and portfolio management. You provide comprehensive, data-driven investment advice to institutional clients.'\n",
    "                },\n",
    "                {\n",
    "                    'role': 'user',\n",
    "                    'content': sample['input']\n",
    "                },\n",
    "                {\n",
    "                    'role': 'assistant',\n",
    "                    'content': sample['output']\n",
    "                }\n",
    "            ],\n",
    "            'date': sample['date'],\n",
    "            'quality_score': sample['quality_score']\n",
    "        }\n",
    "        chat_data.append(chat_sample)\n",
    "    \n",
    "    return chat_data\n",
    "\n",
    "def create_alpaca_format(samples: List[Dict]) -> List[Dict]:\n",
    "    \"\"\"Create Alpaca format for fine-tuning.\"\"\"\n",
    "    \n",
    "    alpaca_data = []\n",
    "    \n",
    "    for sample in samples:\n",
    "        alpaca_sample = {\n",
    "            'instruction': sample['instruction'],\n",
    "            'input': sample['input'],\n",
    "            'output': sample['output']\n",
    "        }\n",
    "        alpaca_data.append(alpaca_sample)\n",
    "    \n",
    "    return alpaca_data\n",
    "\n",
    "# Create different formats\n",
    "instruction_format = create_instruction_format(training_samples)\n",
    "chat_format = create_chat_format(training_samples)\n",
    "alpaca_format = create_alpaca_format(training_samples)\n",
    "\n",
    "print(f\"Created training data in multiple formats:\")\n",
    "print(f\"- Instruction format: {len(instruction_format)} samples\")\n",
    "print(f\"- Chat format: {len(chat_format)} samples\")\n",
    "print(f\"- Alpaca format: {len(alpaca_format)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8385a746",
   "metadata": {},
   "source": [
    "## 6. Save Training Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b925e584",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved instruction format: bitcoin_training_datasets/bitcoin_investment_training_instruction_20250908_072751.json\n",
      "Saved chat format: bitcoin_training_datasets/bitcoin_investment_training_chat_20250908_072751.json\n",
      "Saved alpaca format: bitcoin_training_datasets/bitcoin_investment_training_alpaca_20250908_072751.json\n",
      "Saved comprehensive format: bitcoin_training_datasets/bitcoin_investment_training_comprehensive_20250908_072751.json\n",
      "\n",
      "All training datasets saved in directory: bitcoin_training_datasets\n"
     ]
    }
   ],
   "source": [
    "# Create output directory\n",
    "output_dir = \"bitcoin_training_datasets\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Generate timestamp for file naming\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "# Save as JSON files\n",
    "formats_and_data = {\n",
    "    'instruction': instruction_format,\n",
    "    'chat': chat_format,\n",
    "    'alpaca': alpaca_format,\n",
    "    'comprehensive': training_samples\n",
    "}\n",
    "\n",
    "saved_files = {}\n",
    "\n",
    "for format_name, data in formats_and_data.items():\n",
    "    # JSON format\n",
    "    json_filename = f\"{output_dir}/bitcoin_investment_training_{format_name}_{timestamp}.json\"\n",
    "    with open(json_filename, 'w', encoding='utf-8') as f:\n",
    "        json.dump(data, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    # JSONL format for some formats\n",
    "    if format_name in ['chat', 'instruction']:\n",
    "        jsonl_filename = f\"{output_dir}/bitcoin_investment_training_{format_name}_{timestamp}.jsonl\"\n",
    "        with open(jsonl_filename, 'w', encoding='utf-8') as f:\n",
    "            for sample in data:\n",
    "                f.write(json.dumps(sample, ensure_ascii=False) + '\\n')\n",
    "        saved_files[f'{format_name}_jsonl'] = jsonl_filename\n",
    "    \n",
    "    saved_files[f'{format_name}_json'] = json_filename\n",
    "    print(f\"Saved {format_name} format: {json_filename}\")\n",
    "\n",
    "print(f\"\\nAll training datasets saved in directory: {output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5726d19",
   "metadata": {},
   "source": [
    "## 7. Create Dataset Statistics and Quality Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22376ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset_report(samples: List[Dict]) -> Dict:\n",
    "    \"\"\"Create a comprehensive dataset quality report.\"\"\"\n",
    "    \n",
    "    df = pd.DataFrame(samples)\n",
    "    \n",
    "    report = {\n",
    "        'total_samples': len(samples),\n",
    "        'date_range': {\n",
    "            'earliest': df['date'].min(),\n",
    "            'latest': df['date'].max(),\n",
    "            'unique_dates': df['date'].nunique()\n",
    "        },\n",
    "        'length_statistics': {\n",
    "            'input_length': {\n",
    "                'mean': df['input_length'].mean(),\n",
    "                'median': df['input_length'].median(),\n",
    "                'min': df['input_length'].min(),\n",
    "                'max': df['input_length'].max()\n",
    "            },\n",
    "            'output_length': {\n",
    "                'mean': df['output_length'].mean(),\n",
    "                'median': df['output_length'].median(),\n",
    "                'min': df['output_length'].min(),\n",
    "                'max': df['output_length'].max()\n",
    "            }\n",
    "        },\n",
    "        'data_quality': {\n",
    "            'average_quality_score': df['quality_score'].mean(),\n",
    "            'high_quality_samples': (df['quality_score'] >= 0.7).sum(),\n",
    "            'samples_with_news': df['has_news_data'].sum(),\n",
    "            'samples_with_prices': df['has_price_data'].sum()\n",
    "        },\n",
    "        'quality_distribution': {\n",
    "            'excellent': (df['quality_score'] >= 0.9).sum(),\n",
    "            'good': ((df['quality_score'] >= 0.7) & (df['quality_score'] < 0.9)).sum(),\n",
    "            'fair': ((df['quality_score'] >= 0.5) & (df['quality_score'] < 0.7)).sum(),\n",
    "            'poor': (df['quality_score'] < 0.5).sum()\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return report\n",
    "\n",
    "# Generate report\n",
    "dataset_report = create_dataset_report(training_samples)\n",
    "\n",
    "# Save report\n",
    "report_filename = f\"{output_dir}/bitcoin_training_dataset_report_{timestamp}.json\"\n",
    "with open(report_filename, 'w', encoding='utf-8') as f:\n",
    "    json.dump(dataset_report, f, indent=2, default=str)\n",
    "\n",
    "# Display report\n",
    "print(\"\\n=== DATASET QUALITY REPORT ===\")\n",
    "print(f\"Total Samples: {dataset_report['total_samples']:,}\")\n",
    "print(f\"Date Range: {dataset_report['date_range']['earliest']} to {dataset_report['date_range']['latest']}\")\n",
    "print(f\"Unique Dates: {dataset_report['date_range']['unique_dates']:,}\")\n",
    "print(\"\\nLength Statistics:\")\n",
    "print(f\"  Input - Mean: {dataset_report['length_statistics']['input_length']['mean']:.0f}, Max: {dataset_report['length_statistics']['input_length']['max']:,}\")\n",
    "print(f\"  Output - Mean: {dataset_report['length_statistics']['output_length']['mean']:.0f}, Max: {dataset_report['length_statistics']['output_length']['max']:,}\")\n",
    "print(\"\\nData Quality:\")\n",
    "print(f\"  Average Quality Score: {dataset_report['data_quality']['average_quality_score']:.2f}\")\n",
    "print(f\"  High Quality Samples (‚â•0.7): {dataset_report['data_quality']['high_quality_samples']:,}\")\n",
    "print(f\"  Samples with News Data: {dataset_report['data_quality']['samples_with_news']:,}\")\n",
    "print(f\"  Samples with Price Data: {dataset_report['data_quality']['samples_with_prices']:,}\")\n",
    "print(\"\\nQuality Distribution:\")\n",
    "print(f\"  Excellent (‚â•0.9): {dataset_report['quality_distribution']['excellent']:,}\")\n",
    "print(f\"  Good (0.7-0.9): {dataset_report['quality_distribution']['good']:,}\")\n",
    "print(f\"  Fair (0.5-0.7): {dataset_report['quality_distribution']['fair']:,}\")\n",
    "print(f\"  Poor (<0.5): {dataset_report['quality_distribution']['poor']:,}\")\n",
    "\n",
    "print(f\"\\nReport saved to: {report_filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a56d369e",
   "metadata": {},
   "source": [
    "## 8. Sample Training Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fce2e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display sample training examples\n",
    "print(\"\\n=== SAMPLE TRAINING EXAMPLES ===\")\n",
    "\n",
    "# Show a high-quality sample\n",
    "high_quality_samples = [s for s in training_samples if s['quality_score'] >= 0.8]\n",
    "if high_quality_samples:\n",
    "    sample = high_quality_samples[0]\n",
    "    \n",
    "    print(f\"\\n--- HIGH QUALITY SAMPLE (Quality Score: {sample['quality_score']:.2f}) ---\")\n",
    "    print(f\"Date: {sample['date']}\")\n",
    "    print(f\"Input Length: {sample['input_length']:,} characters\")\n",
    "    print(f\"Output Length: {sample['output_length']:,} characters\")\n",
    "    print(\"\\nINPUT (truncated):\")\n",
    "    print(sample['input'][:500] + \"...\")\n",
    "    print(\"\\nOUTPUT (truncated):\")\n",
    "    print(sample['output'][:500] + \"...\")\n",
    "\n",
    "# Show instruction format example\n",
    "print(\"\\n\\n--- INSTRUCTION FORMAT EXAMPLE ---\")\n",
    "instruction_sample = instruction_format[0]\n",
    "print(f\"Instruction: {instruction_sample['instruction']}\")\n",
    "print(f\"Input (truncated): {instruction_sample['input'][:200]}...\")\n",
    "print(f\"Output (truncated): {instruction_sample['output'][:200]}...\")\n",
    "\n",
    "# Show chat format example\n",
    "print(\"\\n\\n--- CHAT FORMAT EXAMPLE ---\")\n",
    "chat_sample = chat_format[0]\n",
    "print(f\"System: {chat_sample['messages'][0]['content']}\")\n",
    "print(f\"User (truncated): {chat_sample['messages'][1]['content'][:200]}...\")\n",
    "print(f\"Assistant (truncated): {chat_sample['messages'][2]['content'][:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ad66cb",
   "metadata": {},
   "source": [
    "## 9. Upload to Hugging Face Hub (Optional)\n",
    "\n",
    "**Note**: Make sure you have a Hugging Face account and are logged in before running this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bbd36c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration for Hugging Face upload\n",
    "HF_REPO_NAME = \"bitcoin-investment-advisory-training-dataset\"  # Change this to your desired repo name\n",
    "HF_USERNAME = \"your-username\"  # Change this to your Hugging Face username\n",
    "UPLOAD_TO_HF = False  # Set to True to upload\n",
    "\n",
    "if UPLOAD_TO_HF:\n",
    "    try:\n",
    "        # Initialize Hugging Face API\n",
    "        api = HfApi()\n",
    "        \n",
    "        # Create repository\n",
    "        repo_id = f\"{HF_USERNAME}/{HF_REPO_NAME}\"\n",
    "        \n",
    "        try:\n",
    "            create_repo(repo_id, repo_type=\"dataset\", exist_ok=True)\n",
    "            print(f\"Created repository: {repo_id}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Repository might already exist: {e}\")\n",
    "        \n",
    "        # Convert to Hugging Face Dataset format\n",
    "        hf_datasets = {}\n",
    "        \n",
    "        for format_name, data in formats_and_data.items():\n",
    "            if format_name == 'comprehensive':\n",
    "                continue  # Skip comprehensive format for HF upload\n",
    "            \n",
    "            hf_dataset = Dataset.from_list(data)\n",
    "            hf_datasets[format_name] = hf_dataset\n",
    "            \n",
    "            # Upload each format as a separate configuration\n",
    "            hf_dataset.push_to_hub(\n",
    "                repo_id,\n",
    "                config_name=format_name,\n",
    "                commit_message=f\"Upload {format_name} format training data\"\n",
    "            )\n",
    "            \n",
    "            print(f\"Uploaded {format_name} format to Hugging Face Hub\")\n",
    "        \n",
    "        # Create and upload README\n",
    "        readme_content = f\"\"\"\n",
    "# Bitcoin Investment Advisory Training Dataset\n",
    "\n",
    "This dataset contains comprehensive Bitcoin investment advisory training data for fine-tuning language models.\n",
    "\n",
    "## Dataset Description\n",
    "\n",
    "- **Total Samples**: {dataset_report['total_samples']:,}\n",
    "- **Date Range**: {dataset_report['date_range']['earliest']} to {dataset_report['date_range']['latest']}\n",
    "- **Average Quality Score**: {dataset_report['data_quality']['average_quality_score']:.2f}\n",
    "\n",
    "## Formats Available\n",
    "\n",
    "- **instruction**: Standard instruction-tuning format\n",
    "- **chat**: Conversational format with system/user/assistant roles\n",
    "- **alpaca**: Alpaca-style format for fine-tuning\n",
    "\n",
    "## Usage\n",
    "\n",
    "```python\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load instruction format\n",
    "dataset = load_dataset(\"{repo_id}\", \"instruction\")\n",
    "\n",
    "# Load chat format\n",
    "dataset = load_dataset(\"{repo_id}\", \"chat\")\n",
    "```\n",
    "\n",
    "## License\n",
    "\n",
    "Research and Educational Use Only\n",
    "\n",
    "## Citation\n",
    "\n",
    "If you use this dataset, please cite:\n",
    "\n",
    "```\n",
    "@dataset{{bitcoin_investment_advisory_dataset,\n",
    "  title={{Bitcoin Investment Advisory Training Dataset}},\n",
    "  author={{{HF_USERNAME}}},\n",
    "  year={{2025}},\n",
    "  url={{https://huggingface.co/datasets/{repo_id}}}\n",
    "}}\n",
    "```\n",
    "\"\"\"\n",
    "        \n",
    "        # Save README locally\n",
    "        readme_path = f\"{output_dir}/README.md\"\n",
    "        with open(readme_path, 'w', encoding='utf-8') as f:\n",
    "            f.write(readme_content)\n",
    "        \n",
    "        # Upload README\n",
    "        api.upload_file(\n",
    "            path_or_fileobj=readme_path,\n",
    "            path_in_repo=\"README.md\",\n",
    "            repo_id=repo_id,\n",
    "            repo_type=\"dataset\"\n",
    "        )\n",
    "        \n",
    "        print(f\"\\n‚úÖ Successfully uploaded dataset to Hugging Face Hub: https://huggingface.co/datasets/{repo_id}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error uploading to Hugging Face: {e}\")\n",
    "        print(\"Make sure you are logged in with 'huggingface-cli login' and have the correct permissions.\")\n",
    "        \n",
    "else:\n",
    "    print(\"Hugging Face upload disabled. Set UPLOAD_TO_HF = True and configure HF_USERNAME to upload.\")\n",
    "    print(f\"\\nTo upload later, you can use the saved files in: {output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31eb20b0",
   "metadata": {},
   "source": [
    "## 10. Create Training Instructions and Guidelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86cba45d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive training instructions\n",
    "training_instructions = {\n",
    "    \"dataset_overview\": {\n",
    "        \"name\": \"Bitcoin Investment Advisory Training Dataset\",\n",
    "        \"purpose\": \"Train AI models to provide institutional-grade Bitcoin investment advice\",\n",
    "        \"total_samples\": len(training_samples),\n",
    "        \"formats_available\": list(formats_and_data.keys()),\n",
    "        \"quality_score_range\": [0.0, 1.0],\n",
    "        \"recommended_min_quality\": 0.7\n",
    "    },\n",
    "    \"training_recommendations\": {\n",
    "        \"model_types\": [\n",
    "            \"Large Language Models (LLMs) like GPT, LLaMA, Mistral\",\n",
    "            \"Instruction-tuned models\",\n",
    "            \"Chat-based models\"\n",
    "        ],\n",
    "        \"training_approaches\": [\n",
    "            \"Fine-tuning on instruction format\",\n",
    "            \"Supervised fine-tuning (SFT)\",\n",
    "            \"Parameter-Efficient Fine-Tuning (PEFT) with LoRA\"\n",
    "        ],\n",
    "        \"hyperparameters\": {\n",
    "            \"learning_rate\": \"1e-5 to 5e-5\",\n",
    "            \"batch_size\": \"4-16 depending on GPU memory\",\n",
    "            \"epochs\": \"3-5 for fine-tuning\",\n",
    "            \"max_sequence_length\": \"4096-8192 tokens\",\n",
    "            \"warmup_steps\": \"10% of total steps\"\n",
    "        },\n",
    "        \"data_splitting\": {\n",
    "            \"train\": \"80% (chronologically earlier dates)\",\n",
    "            \"validation\": \"10% (middle dates)\",\n",
    "            \"test\": \"10% (most recent dates)\"\n",
    "        }\n",
    "    },\n",
    "    \"evaluation_metrics\": [\n",
    "        \"BLEU score for text generation quality\",\n",
    "        \"ROUGE scores for summarization quality\",\n",
    "        \"Human evaluation for investment advice quality\",\n",
    "        \"Perplexity for language modeling performance\",\n",
    "        \"Custom metrics for investment recommendation accuracy\"\n",
    "    ],\n",
    "    \"usage_guidelines\": {\n",
    "        \"data_preprocessing\": [\n",
    "            \"Filter samples by quality score (recommended: ‚â•0.7)\",\n",
    "            \"Tokenize inputs and outputs appropriately\",\n",
    "            \"Handle long sequences with truncation or chunking\",\n",
    "            \"Apply consistent formatting across samples\"\n",
    "        ],\n",
    "        \"training_best_practices\": [\n",
    "            \"Use gradient accumulation for large effective batch sizes\",\n",
    "            \"Implement early stopping based on validation loss\",\n",
    "            \"Monitor for overfitting on specific dates or patterns\",\n",
    "            \"Use mixed precision training to optimize memory usage\"\n",
    "        ],\n",
    "        \"ethical_considerations\": [\n",
    "            \"This dataset is for research and educational purposes only\",\n",
    "            \"Models trained on this data should include financial disclaimers\",\n",
    "            \"Investment advice generated should be clearly marked as AI-generated\",\n",
    "            \"Consider regulatory compliance in deployment contexts\"\n",
    "        ]\n",
    "    },\n",
    "    \"sample_training_code\": {\n",
    "        \"huggingface_transformers\": \"\"\"\n",
    "# Example training with Hugging Face Transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load dataset\n",
    "dataset = load_dataset('path/to/dataset', 'instruction')\n",
    "\n",
    "# Load model and tokenizer\n",
    "model_name = 'microsoft/DialoGPT-medium'  # or your preferred base model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# Set padding token\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Tokenization function\n",
    "def tokenize_function(examples):\n",
    "    # Combine instruction, input, and output\n",
    "    texts = [f\"{inst}\\n{inp}\\n{out}\" for inst, inp, out in \n",
    "             zip(examples['instruction'], examples['input'], examples['output'])]\n",
    "    return tokenizer(texts, truncation=True, padding=True, max_length=2048)\n",
    "\n",
    "# Apply tokenization\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./bitcoin-advisor-model',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=4,\n",
    "    learning_rate=2e-5,\n",
    "    warmup_steps=100,\n",
    "    logging_steps=50,\n",
    "    save_steps=500,\n",
    "    eval_steps=500,\n",
    "    evaluation_strategy='steps',\n",
    "    fp16=True,\n",
    ")\n",
    "\n",
    "# Create trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset['train'],\n",
    "    eval_dataset=tokenized_dataset['validation'],\n",
    ")\n",
    "\n",
    "# Train\n",
    "trainer.train()\n",
    "\"\"\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save training instructions\n",
    "instructions_path = f\"{output_dir}/training_instructions_{timestamp}.json\"\n",
    "with open(instructions_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(training_instructions, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"\\n=== TRAINING INSTRUCTIONS CREATED ===\")\n",
    "print(f\"Saved to: {instructions_path}\")\n",
    "print(\"\\nKey Recommendations:\")\n",
    "print(f\"- Use samples with quality score ‚â• {training_instructions['dataset_overview']['recommended_min_quality']}\")\n",
    "print(f\"- Available formats: {', '.join(training_instructions['dataset_overview']['formats_available'])}\")\n",
    "print(f\"- Recommended learning rate: {training_instructions['training_recommendations']['hyperparameters']['learning_rate']}\")\n",
    "print(f\"- Suggested epochs: {training_instructions['training_recommendations']['hyperparameters']['epochs']}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"DATASET CREATION COMPLETE!\")\n",
    "print(\"=\"*50)\n",
    "print(f\"üìÅ Output directory: {output_dir}\")\n",
    "print(f\"üìä Total samples: {len(training_samples):,}\")\n",
    "print(f\"üìà High quality samples: {dataset_report['data_quality']['high_quality_samples']:,}\")\n",
    "print(f\"üìã Formats created: {len(formats_and_data)}\")\n",
    "print(f\"üìù Files saved: {len(saved_files) + 2}\")\n",
    "print(\"\\nReady for training! üöÄ\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
