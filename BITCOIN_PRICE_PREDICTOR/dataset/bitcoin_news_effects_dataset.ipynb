{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d86006b",
   "metadata": {},
   "source": [
    "# 1. Setup and Dependencies\n",
    "\n",
    "Install required libraries and setup API keys for Gemini Flash analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "09a83f67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "‚úÖ All required libraries imported successfully!\n",
      "‚úÖ All required libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "!pip install -q pandas numpy datasets huggingface_hub\n",
    "!pip install -q tqdm python-dateutil\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import glob\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "from typing import List, Dict, Any, Optional\n",
    "from tqdm.notebook import tqdm\n",
    "from datasets import Dataset\n",
    "from huggingface_hub import login\n",
    "\n",
    "print(\"‚úÖ All required libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "64914f1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ HuggingFace login successful!\n",
      "‚úÖ Setup complete - using existing analyzed data, no API calls needed!\n"
     ]
    }
   ],
   "source": [
    "# HuggingFace token for dataset upload\n",
    "HF_TOKEN = \"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\"\n",
    "\n",
    "# Login to HuggingFace (only needed for dataset upload)\n",
    "try:\n",
    "    login(token=HF_TOKEN, add_to_git_credential=False)\n",
    "    print(\"‚úÖ HuggingFace login successful!\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå HuggingFace login failed: {e}\")\n",
    "\n",
    "print(\"‚úÖ Setup complete - using existing analyzed data, no API calls needed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b2d2d15",
   "metadata": {},
   "source": [
    "# 2. Load Individual News Items\n",
    "\n",
    "Load ALL individual news items (not just daily summaries) from your existing analyzed dataset to create training samples for each news article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "082f5142",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì∞ Loading individual news items from outputs_btc_effects...\n",
      "üìÅ Found 2437 news files\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42a970c6fb9745f9af549965163821a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading individual news items:   0%|          | 0/2437 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded 49171 individual news items\n",
      "üìÖ Date range: 2018-01-01 to 2024-12-31\n",
      "üìä Average news items per day: 20.2\n",
      "üìä Max news items in a day: 33\n",
      "üìä Min news items in a day: 2\n",
      "\\nüìà Timeframe distribution:\n",
      "   short_term: 24764 items (50.4%)\n",
      "   long_term: 24407 items (49.6%)\n",
      "\\nüì∞ Sample individual news item:\n",
      "Date: 2021-07-16\n",
      "Title: PayPal ups its weekly cryptocurrency buy limit to $100,000\n",
      "Direction: bullish | Magnitude: medium | Confidence: 0.70\n",
      "Timeframe: short_term\n",
      "Summary: PayPal has increased its weekly cryptocurrency buy limit to $100,000, signaling continued expansion and confidence in the digital asset space. This mo...\n"
     ]
    }
   ],
   "source": [
    "# Load all individual news items from your existing dataset\n",
    "print(\"üì∞ Loading individual news items from outputs_btc_effects...\")\n",
    "\n",
    "# Path to your existing news data\n",
    "news_data_path = \"/Users/tahamajs/Documents/uni/LLM/Files/Final Project/outputs_btc_effects/per_date/*.json\"\n",
    "all_news_files = glob.glob(news_data_path)\n",
    "\n",
    "if not all_news_files:\n",
    "    print(\"‚ùå No news files found. Please check the path.\")\n",
    "    print(\"Expected path:\", news_data_path)\n",
    "else:\n",
    "    print(f\"üìÅ Found {len(all_news_files)} news files\")\n",
    "\n",
    "# Load ALL individual news items (not just daily summaries)\n",
    "all_individual_news = []\n",
    "\n",
    "for file_path in tqdm(all_news_files, desc=\"Loading individual news items\"):\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "            \n",
    "        # Extract date from filename\n",
    "        date_str = os.path.basename(file_path).split('.')[0]\n",
    "        \n",
    "        # Get daily view context for this date\n",
    "        daily_context = {}\n",
    "        if 'daily_view' in data and data['daily_view']:\n",
    "            daily_view = data['daily_view']\n",
    "            daily_context = {\n",
    "                'scenario_probs': daily_view.get('scenario_probs', {}),\n",
    "                'recommendation_short_term': daily_view.get('recommendation_short_term', {}),\n",
    "                'recommendation_long_term': daily_view.get('recommendation_long_term', {})\n",
    "            }\n",
    "        \n",
    "        # Process ALL long-term news items for this date\n",
    "        long_term_items = data.get('long_term', [])\n",
    "        for item in long_term_items:\n",
    "            news_item = {\n",
    "                'date': date_str,\n",
    "                'title': item.get('title', ''),\n",
    "                'summary': item.get('summary', ''),\n",
    "                'direction': item.get('direction', ''),\n",
    "                'magnitude': item.get('magnitude', ''),\n",
    "                'confidence': item.get('confidence', 0),\n",
    "                'impact_horizon_months': item.get('impact_horizon_months', 0),\n",
    "                'impact_horizon_days': None,  # Long-term items use months\n",
    "                'rationale': item.get('rationale', ''),\n",
    "                'timeframe_type': 'long_term',\n",
    "                'url': item.get('url', ''),\n",
    "                'id': item.get('id', ''),\n",
    "                'impact_tags': item.get('impact_tags', []),\n",
    "                # Add daily context\n",
    "                **daily_context\n",
    "            }\n",
    "            all_individual_news.append(news_item)\n",
    "        \n",
    "        # Process ALL short-term news items for this date  \n",
    "        short_term_items = data.get('short_term', [])\n",
    "        for item in short_term_items:\n",
    "            news_item = {\n",
    "                'date': date_str,\n",
    "                'title': item.get('title', ''),\n",
    "                'summary': item.get('summary', ''),\n",
    "                'direction': item.get('direction', ''),\n",
    "                'magnitude': item.get('magnitude', ''),\n",
    "                'confidence': item.get('confidence', 0),\n",
    "                'impact_horizon_months': None,  # Short-term items use days\n",
    "                'impact_horizon_days': item.get('impact_horizon_days', 0),\n",
    "                'rationale': item.get('rationale', ''),\n",
    "                'timeframe_type': 'short_term',\n",
    "                'url': item.get('url', ''),\n",
    "                'id': item.get('id', ''),\n",
    "                'impact_tags': item.get('impact_tags', []),\n",
    "                # Add daily context\n",
    "                **daily_context\n",
    "            }\n",
    "            all_individual_news.append(news_item)\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Error loading {file_path}: {e}\")\n",
    "        continue\n",
    "\n",
    "# Convert to DataFrame for easier handling\n",
    "df_individual_news = pd.DataFrame(all_individual_news)\n",
    "\n",
    "if not df_individual_news.empty:\n",
    "    df_individual_news['date'] = pd.to_datetime(df_individual_news['date'])\n",
    "    df_individual_news = df_individual_news.sort_values(['date', 'confidence'], ascending=[True, False]).reset_index(drop=True)\n",
    "    \n",
    "    print(f\"‚úÖ Loaded {len(df_individual_news)} individual news items\")\n",
    "    print(f\"üìÖ Date range: {df_individual_news['date'].min().date()} to {df_individual_news['date'].max().date()}\")\n",
    "    \n",
    "    # Show statistics by date\n",
    "    daily_counts = df_individual_news.groupby('date').size()\n",
    "    print(f\"üìä Average news items per day: {daily_counts.mean():.1f}\")\n",
    "    print(f\"üìä Max news items in a day: {daily_counts.max()}\")\n",
    "    print(f\"üìä Min news items in a day: {daily_counts.min()}\")\n",
    "    \n",
    "    # Show timeframe distribution\n",
    "    timeframe_counts = df_individual_news['timeframe_type'].value_counts()\n",
    "    print(f\"\\\\nüìà Timeframe distribution:\")\n",
    "    for timeframe, count in timeframe_counts.items():\n",
    "        print(f\"   {timeframe}: {count} items ({count/len(df_individual_news)*100:.1f}%)\")\n",
    "    \n",
    "    # Display sample news item\n",
    "    print(f\"\\\\nüì∞ Sample individual news item:\")\n",
    "    sample_idx = len(df_individual_news) // 2\n",
    "    sample = df_individual_news.iloc[sample_idx]\n",
    "    print(f\"Date: {sample['date'].date()}\")\n",
    "    print(f\"Title: {sample['title']}\")\n",
    "    print(f\"Direction: {sample['direction']} | Magnitude: {sample['magnitude']} | Confidence: {sample['confidence']:.2f}\")\n",
    "    print(f\"Timeframe: {sample['timeframe_type']}\")\n",
    "    print(f\"Summary: {sample['summary'][:150]}...\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå No individual news items loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89465c3f",
   "metadata": {},
   "source": [
    "# 3. Create Training Data Generator\n",
    "\n",
    "Build a system to convert individual analyzed news items into focused training samples for Bitcoin price prediction models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3d32d96a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Bitcoin Training Data Creator initialized - no API needed!\n"
     ]
    }
   ],
   "source": [
    "class BitcoinTrainingDataCreator:\n",
    "    \"\"\"Create training samples directly from existing analyzed Bitcoin data\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def extract_sentiment_from_probs(self, scenario_probs: Dict[str, float]) -> str:\n",
    "        \"\"\"Extract sentiment from scenario probabilities\"\"\"\n",
    "        bull = scenario_probs.get('bull', 0)\n",
    "        bear = scenario_probs.get('bear', 0)\n",
    "        base = scenario_probs.get('base', 0)\n",
    "        \n",
    "        if bull > bear and bull > base:\n",
    "            return \"bullish\"\n",
    "        elif bear > bull and bear > base:\n",
    "            return \"bearish\"\n",
    "        else:\n",
    "            return \"neutral\"\n",
    "    \n",
    "    def extract_direction_from_recommendation(self, short_rec: Dict, long_rec: Dict) -> str:\n",
    "        \"\"\"Extract price direction from recommendations\"\"\"\n",
    "        short_action = short_rec.get('action', '').lower()\n",
    "        long_action = long_rec.get('action', '').lower()\n",
    "        \n",
    "        if 'buy' in short_action or 'buy' in long_action:\n",
    "            return \"up\"\n",
    "        elif 'sell' in short_action or 'sell' in long_action:\n",
    "            return \"down\"\n",
    "        else:\n",
    "            return \"sideways\"\n",
    "    \n",
    "    def extract_strength_from_analysis(self, long_term_items: List, short_term_items: List) -> str:\n",
    "        \"\"\"Extract impact strength from analysis items\"\"\"\n",
    "        high_impact_count = 0\n",
    "        total_items = len(long_term_items) + len(short_term_items)\n",
    "        \n",
    "        for item in long_term_items + short_term_items:\n",
    "            if item.get('magnitude', '').lower() == 'high':\n",
    "                high_impact_count += 1\n",
    "        \n",
    "        if total_items == 0:\n",
    "            return \"low\"\n",
    "        \n",
    "        high_ratio = high_impact_count / total_items\n",
    "        if high_ratio > 0.6:\n",
    "            return \"high\"\n",
    "        elif high_ratio > 0.3:\n",
    "            return \"medium\"\n",
    "        else:\n",
    "            return \"low\"\n",
    "    \n",
    "    def extract_timeframe_from_analysis(self, long_term_items: List, short_term_items: List) -> str:\n",
    "        \"\"\"Extract primary timeframe from analysis\"\"\"\n",
    "        if len(short_term_items) > len(long_term_items):\n",
    "            return \"short_term\"\n",
    "        elif len(long_term_items) > 0:\n",
    "            return \"medium_term\"\n",
    "        else:\n",
    "            return \"immediate\"\n",
    "    \n",
    "    def calculate_confidence(self, scenario_probs: Dict, recommendations: Dict) -> float:\n",
    "        \"\"\"Calculate confidence based on probabilities and recommendations\"\"\"\n",
    "        # Get the highest probability\n",
    "        max_prob = max(scenario_probs.values()) if scenario_probs else 0.33\n",
    "        \n",
    "        # Get recommendation confidence\n",
    "        rec_conf = recommendations.get('probability', 0.5) if recommendations else 0.5\n",
    "        \n",
    "        # Average them\n",
    "        return (max_prob + rec_conf) / 2\n",
    "    \n",
    "    def extract_key_reason(self, long_term_items: List, short_term_items: List, summary: str) -> str:\n",
    "        \"\"\"Extract key reason from analysis\"\"\"\n",
    "        # Try to get from highest confidence item\n",
    "        all_items = long_term_items + short_term_items\n",
    "        \n",
    "        if all_items:\n",
    "            # Sort by confidence and get top item\n",
    "            sorted_items = sorted(all_items, key=lambda x: x.get('confidence', 0), reverse=True)\n",
    "            top_item = sorted_items[0]\n",
    "            \n",
    "            reason = top_item.get('rationale', '') or top_item.get('title', '')\n",
    "            if reason:\n",
    "                return reason[:100]  # Limit length\n",
    "        \n",
    "        # Fallback to summary snippet\n",
    "        if summary:\n",
    "            return summary[:100]\n",
    "        \n",
    "        return \"Market conditions analysis\"\n",
    "    \n",
    "    def create_training_sample_from_individual_news(self, news_item: Dict[str, Any]) -> Dict[str, str]:\n",
    "        \"\"\"Create training sample from individual analyzed news item\"\"\"\n",
    "        \n",
    "        # Extract individual news item data\n",
    "        title = news_item.get('title', '')\n",
    "        summary = news_item.get('summary', '')\n",
    "        direction = news_item.get('direction', '')\n",
    "        magnitude = news_item.get('magnitude', '')\n",
    "        confidence = news_item.get('confidence', 0)\n",
    "        rationale = news_item.get('rationale', '')\n",
    "        timeframe_type = news_item.get('timeframe_type', '')\n",
    "        impact_tags = news_item.get('impact_tags', [])\n",
    "        \n",
    "        # Extract daily market context\n",
    "        scenario_probs = news_item.get('scenario_probs', {})\n",
    "        short_rec = news_item.get('recommendation_short_term', {})\n",
    "        long_rec = news_item.get('recommendation_long_term', {})\n",
    "        \n",
    "        # Map direction to price direction\n",
    "        if direction.lower() == 'bullish':\n",
    "            price_direction = \"up\"\n",
    "        elif direction.lower() == 'bearish':\n",
    "            price_direction = \"down\"\n",
    "        else:\n",
    "            price_direction = \"sideways\"\n",
    "        \n",
    "        # Map magnitude to impact strength\n",
    "        impact_strength = magnitude.lower() if magnitude.lower() in ['high', 'medium', 'low'] else 'medium'\n",
    "        \n",
    "        # Determine timeframe\n",
    "        if timeframe_type == 'short_term':\n",
    "            timeframe = \"short_term\"\n",
    "        elif timeframe_type == 'long_term':\n",
    "            timeframe = \"medium_term\"\n",
    "        else:\n",
    "            timeframe = \"immediate\"\n",
    "        \n",
    "        # Extract sentiment from direction\n",
    "        sentiment = direction.lower() if direction.lower() in ['bullish', 'bearish'] else 'neutral'\n",
    "        \n",
    "        # Create Bitcoin effects analysis from individual news item\n",
    "        effects_analysis = {\n",
    "            \"sentiment\": sentiment,\n",
    "            \"price_direction\": price_direction,\n",
    "            \"impact_strength\": impact_strength,\n",
    "            \"timeframe\": timeframe,\n",
    "            \"confidence\": float(confidence),\n",
    "            \"key_reason\": rationale[:100] if rationale else title[:100]\n",
    "        }\n",
    "        \n",
    "        # Create simple, clear instruction with JSON structure\n",
    "        instruction = \"\"\"Analyze Bitcoin news and predict price impact. Return JSON with this exact structure:\n",
    "\n",
    "{\n",
    "  \"sentiment\": \"bullish|neutral|bearish\",\n",
    "  \"price_direction\": \"up|sideways|down\",\n",
    "  \"impact_strength\": \"high|medium|low\", \n",
    "  \"timeframe\": \"immediate|short_term|medium_term\",\n",
    "  \"confidence\": 0.75,\n",
    "  \"key_reason\": \"Brief explanation of main factor\"\n",
    "}\"\"\"\n",
    "        \n",
    "        # Create concise input context with individual news\n",
    "        input_context = f\"\"\"News Title: {title}\n",
    "\n",
    "News Summary: {summary}\n",
    "\n",
    "Impact Tags: {', '.join(impact_tags) if impact_tags else 'None'}\n",
    "\n",
    "Market Context:\n",
    "Bull {scenario_probs.get('bull', 0):.0%} | Base {scenario_probs.get('base', 0):.0%} | Bear {scenario_probs.get('bear', 0):.0%}\n",
    "\n",
    "Daily Recommendations:\n",
    "Short-term: {short_rec.get('action', 'Hold')}\n",
    "Long-term: {long_rec.get('action', 'Hold')}\"\"\"\n",
    "        \n",
    "        # Create expected output\n",
    "        output = json.dumps(effects_analysis, ensure_ascii=False)\n",
    "        \n",
    "        return {\n",
    "            'instruction': instruction,\n",
    "            'input': input_context,\n",
    "            'output': output\n",
    "        }\n",
    "\n",
    "# Initialize the training data creator\n",
    "data_creator = BitcoinTrainingDataCreator()\n",
    "print(\"‚úÖ Bitcoin Training Data Creator initialized - no API needed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a32621d4",
   "metadata": {},
   "source": [
    "# 4. Process Individual News Items and Create Training Samples\n",
    "\n",
    "Process each individual news item from your existing analysis to create focused training samples for Bitcoin price prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2500bdc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Creating Bitcoin Training Dataset from Individual News Items\n",
      "============================================================\n",
      "üìä Processing 49171 individual news items\n",
      "üìÖ From 2018-01-01 to 2024-12-31\n",
      "üìä Creating training samples from ~20 news items per day\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3165a7a9974947fda8bdd402ad330ff0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating training samples from individual news:   0%|          | 0/49171 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ Created 1000 training samples so far...\n",
      "   ‚úÖ Created 2000 training samples so far...\n",
      "   ‚úÖ Created 3000 training samples so far...\n",
      "   ‚úÖ Created 4000 training samples so far...\n",
      "   ‚úÖ Created 5000 training samples so far...\n",
      "   ‚úÖ Created 6000 training samples so far...\n",
      "   ‚úÖ Created 7000 training samples so far...\n",
      "   ‚úÖ Created 8000 training samples so far...\n",
      "   ‚úÖ Created 9000 training samples so far...\n",
      "   ‚úÖ Created 10000 training samples so far...\n",
      "   ‚úÖ Created 11000 training samples so far...\n",
      "   ‚úÖ Created 12000 training samples so far...\n",
      "   ‚úÖ Created 13000 training samples so far...\n",
      "   ‚úÖ Created 14000 training samples so far...\n",
      "   ‚úÖ Created 15000 training samples so far...\n",
      "   ‚úÖ Created 16000 training samples so far...\n",
      "   ‚úÖ Created 17000 training samples so far...\n",
      "   ‚úÖ Created 18000 training samples so far...\n",
      "   ‚úÖ Created 19000 training samples so far...\n",
      "   ‚úÖ Created 20000 training samples so far...\n",
      "   ‚úÖ Created 21000 training samples so far...\n",
      "   ‚úÖ Created 22000 training samples so far...\n",
      "   ‚úÖ Created 23000 training samples so far...\n",
      "   ‚úÖ Created 10000 training samples so far...\n",
      "   ‚úÖ Created 11000 training samples so far...\n",
      "   ‚úÖ Created 12000 training samples so far...\n",
      "   ‚úÖ Created 13000 training samples so far...\n",
      "   ‚úÖ Created 14000 training samples so far...\n",
      "   ‚úÖ Created 15000 training samples so far...\n",
      "   ‚úÖ Created 16000 training samples so far...\n",
      "   ‚úÖ Created 17000 training samples so far...\n",
      "   ‚úÖ Created 18000 training samples so far...\n",
      "   ‚úÖ Created 19000 training samples so far...\n",
      "   ‚úÖ Created 20000 training samples so far...\n",
      "   ‚úÖ Created 21000 training samples so far...\n",
      "   ‚úÖ Created 22000 training samples so far...\n",
      "   ‚úÖ Created 23000 training samples so far...\n",
      "   ‚úÖ Created 24000 training samples so far...\n",
      "   ‚úÖ Created 25000 training samples so far...\n",
      "   ‚úÖ Created 26000 training samples so far...\n",
      "   ‚úÖ Created 27000 training samples so far...\n",
      "   ‚úÖ Created 28000 training samples so far...\n",
      "   ‚úÖ Created 29000 training samples so far...\n",
      "   ‚úÖ Created 30000 training samples so far...\n",
      "   ‚úÖ Created 31000 training samples so far...\n",
      "   ‚úÖ Created 32000 training samples so far...\n",
      "   ‚úÖ Created 33000 training samples so far...\n",
      "   ‚úÖ Created 34000 training samples so far...\n",
      "   ‚úÖ Created 35000 training samples so far...\n",
      "   ‚úÖ Created 36000 training samples so far...\n",
      "   ‚úÖ Created 24000 training samples so far...\n",
      "   ‚úÖ Created 25000 training samples so far...\n",
      "   ‚úÖ Created 26000 training samples so far...\n",
      "   ‚úÖ Created 27000 training samples so far...\n",
      "   ‚úÖ Created 28000 training samples so far...\n",
      "   ‚úÖ Created 29000 training samples so far...\n",
      "   ‚úÖ Created 30000 training samples so far...\n",
      "   ‚úÖ Created 31000 training samples so far...\n",
      "   ‚úÖ Created 32000 training samples so far...\n",
      "   ‚úÖ Created 33000 training samples so far...\n",
      "   ‚úÖ Created 34000 training samples so far...\n",
      "   ‚úÖ Created 35000 training samples so far...\n",
      "   ‚úÖ Created 36000 training samples so far...\n",
      "   ‚úÖ Created 37000 training samples so far...\n",
      "   ‚úÖ Created 38000 training samples so far...\n",
      "   ‚úÖ Created 39000 training samples so far...\n",
      "   ‚úÖ Created 40000 training samples so far...\n",
      "   ‚úÖ Created 41000 training samples so far...\n",
      "   ‚úÖ Created 42000 training samples so far...\n",
      "   ‚úÖ Created 43000 training samples so far...\n",
      "   ‚úÖ Created 44000 training samples so far...\n",
      "   ‚úÖ Created 45000 training samples so far...\n",
      "   ‚úÖ Created 46000 training samples so far...\n",
      "   ‚úÖ Created 47000 training samples so far...\n",
      "   ‚úÖ Created 48000 training samples so far...\n",
      "   ‚úÖ Created 49000 training samples so far...\n",
      "\\nüìà Processing Complete!\n",
      "   ‚úÖ Successfully created: 49171 training samples\n",
      "   üìä Total samples: 49171\n",
      "   ‚úÖ Created 37000 training samples so far...\n",
      "   ‚úÖ Created 38000 training samples so far...\n",
      "   ‚úÖ Created 39000 training samples so far...\n",
      "   ‚úÖ Created 40000 training samples so far...\n",
      "   ‚úÖ Created 41000 training samples so far...\n",
      "   ‚úÖ Created 42000 training samples so far...\n",
      "   ‚úÖ Created 43000 training samples so far...\n",
      "   ‚úÖ Created 44000 training samples so far...\n",
      "   ‚úÖ Created 45000 training samples so far...\n",
      "   ‚úÖ Created 46000 training samples so far...\n",
      "   ‚úÖ Created 47000 training samples so far...\n",
      "   ‚úÖ Created 48000 training samples so far...\n",
      "   ‚úÖ Created 49000 training samples so far...\n",
      "\\nüìà Processing Complete!\n",
      "   ‚úÖ Successfully created: 49171 training samples\n",
      "   üìä Total samples: 49171\n",
      "\\nüìã Dataset Statistics:\n",
      "   Total samples: 49171\n",
      "   Avg instruction length: 338 chars\n",
      "   Avg input length: 623 chars\n",
      "   Avg output length: 243 chars\n",
      "\\nüìä Sample Distribution (first 100 items):\n",
      "   Sentiments: {'bullish': 41, 'bearish': 32, 'neutral': 27}\n",
      "   Directions: {'up': 41, 'down': 32, 'sideways': 27}\n",
      "   Strengths: {'medium': 41, 'high': 40, 'low': 19}\n",
      "\\nüìÑ Sample Training Entry:\n",
      "\\nInstruction:\n",
      "Analyze Bitcoin news and predict price impact. Return JSON with this exact structure:\n",
      "\n",
      "{\n",
      "  \"sentiment\": \"bullish|neutral|bearish\",\n",
      "  \"price_direction\": \"up|sideways|down\",\n",
      "  \"impact_strength\": \"high|medium|low\", \n",
      "  \"timeframe\": \"immediate|short_term|medium_term\",\n",
      "  \"confidence\": 0.75,\n",
      "  \"key_reason\": \"Brief explanation of main factor\"\n",
      "}\n",
      "\\nInput:\n",
      "News Title: PayPal ups its weekly cryptocurrency buy limit to $100,000\n",
      "\n",
      "News Summary: PayPal has increased its weekly cryptocurrency buy limit to $100,000, signaling continued expansion and confidence in the digital asset space. This move, following similar expansions by Square and the introduction of crypto checkout options, makes it easier for retail investors to engage with cryptocurrencies. Higher buy limits can increase overall demand.\n",
      "\n",
      "Impact Tags: adoption, liquidity\n",
      "\n",
      "Market Context:\n",
      "Bull 45% | Base 35% | Bear 20%\n",
      "\n",
      "Daily Recommendations:\n",
      "Short-term: HOLD\n",
      "Long-term: BUY\n",
      "\\nOutput:\n",
      "{\"sentiment\": \"bullish\", \"price_direction\": \"up\", \"impact_strength\": \"medium\", \"timeframe\": \"short_term\", \"confidence\": 0.7, \"key_reason\": \"PayPal's increased crypto buy limit makes it more accessible for retail investors to purchase digita\"}\n",
      "\\nüìã Dataset Statistics:\n",
      "   Total samples: 49171\n",
      "   Avg instruction length: 338 chars\n",
      "   Avg input length: 623 chars\n",
      "   Avg output length: 243 chars\n",
      "\\nüìä Sample Distribution (first 100 items):\n",
      "   Sentiments: {'bullish': 41, 'bearish': 32, 'neutral': 27}\n",
      "   Directions: {'up': 41, 'down': 32, 'sideways': 27}\n",
      "   Strengths: {'medium': 41, 'high': 40, 'low': 19}\n",
      "\\nüìÑ Sample Training Entry:\n",
      "\\nInstruction:\n",
      "Analyze Bitcoin news and predict price impact. Return JSON with this exact structure:\n",
      "\n",
      "{\n",
      "  \"sentiment\": \"bullish|neutral|bearish\",\n",
      "  \"price_direction\": \"up|sideways|down\",\n",
      "  \"impact_strength\": \"high|medium|low\", \n",
      "  \"timeframe\": \"immediate|short_term|medium_term\",\n",
      "  \"confidence\": 0.75,\n",
      "  \"key_reason\": \"Brief explanation of main factor\"\n",
      "}\n",
      "\\nInput:\n",
      "News Title: PayPal ups its weekly cryptocurrency buy limit to $100,000\n",
      "\n",
      "News Summary: PayPal has increased its weekly cryptocurrency buy limit to $100,000, signaling continued expansion and confidence in the digital asset space. This move, following similar expansions by Square and the introduction of crypto checkout options, makes it easier for retail investors to engage with cryptocurrencies. Higher buy limits can increase overall demand.\n",
      "\n",
      "Impact Tags: adoption, liquidity\n",
      "\n",
      "Market Context:\n",
      "Bull 45% | Base 35% | Bear 20%\n",
      "\n",
      "Daily Recommendations:\n",
      "Short-term: HOLD\n",
      "Long-term: BUY\n",
      "\\nOutput:\n",
      "{\"sentiment\": \"bullish\", \"price_direction\": \"up\", \"impact_strength\": \"medium\", \"timeframe\": \"short_term\", \"confidence\": 0.7, \"key_reason\": \"PayPal's increased crypto buy limit makes it more accessible for retail investors to purchase digita\"}\n"
     ]
    }
   ],
   "source": [
    "# Create training dataset from ALL individual news items\n",
    "print(\"üöÄ Creating Bitcoin Training Dataset from Individual News Items\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Use all individual news items (each news article gets its own training sample)\n",
    "if not df_individual_news.empty:\n",
    "    print(f\"üìä Processing {len(df_individual_news)} individual news items\")\n",
    "    print(f\"üìÖ From {df_individual_news['date'].min().date()} to {df_individual_news['date'].max().date()}\")\n",
    "    \n",
    "    # Show daily breakdown\n",
    "    daily_counts = df_individual_news.groupby('date').size()\n",
    "    print(f\"üìä Creating training samples from ~{daily_counts.mean():.0f} news items per day\")\n",
    "else:\n",
    "    print(\"‚ùå No individual news data available for processing\")\n",
    "\n",
    "# Process each individual news item to create training samples\n",
    "training_samples = []\n",
    "successful_samples = 0\n",
    "\n",
    "if not df_individual_news.empty:\n",
    "    for idx, news_item in tqdm(df_individual_news.iterrows(), total=len(df_individual_news), desc=\"Creating training samples from individual news\"):\n",
    "        try:\n",
    "            # Create training sample from individual news item\n",
    "            training_sample = data_creator.create_training_sample_from_individual_news(news_item.to_dict())\n",
    "            training_samples.append(training_sample)\n",
    "            successful_samples += 1\n",
    "            \n",
    "            # Show progress every 1000 samples\n",
    "            if (idx + 1) % 1000 == 0:\n",
    "                print(f\"   ‚úÖ Created {successful_samples} training samples so far...\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ö†Ô∏è Error creating sample for news item {idx}: {e}\")\n",
    "            continue\n",
    "\n",
    "print(f\"\\\\nüìà Processing Complete!\")\n",
    "print(f\"   ‚úÖ Successfully created: {successful_samples} training samples\")\n",
    "print(f\"   üìä Total samples: {len(training_samples)}\")\n",
    "\n",
    "# Convert to DataFrame for easier handling\n",
    "if training_samples:\n",
    "    df_training = pd.DataFrame(training_samples)\n",
    "    \n",
    "    # Add metadata\n",
    "    df_training['sample_id'] = range(len(df_training))\n",
    "    df_training['created_at'] = datetime.now().isoformat()\n",
    "    \n",
    "    print(f\"\\\\nüìã Dataset Statistics:\")\n",
    "    print(f\"   Total samples: {len(df_training)}\")\n",
    "    print(f\"   Avg instruction length: {df_training['instruction'].str.len().mean():.0f} chars\")\n",
    "    print(f\"   Avg input length: {df_training['input'].str.len().mean():.0f} chars\")\n",
    "    print(f\"   Avg output length: {df_training['output'].str.len().mean():.0f} chars\")\n",
    "    \n",
    "    # Analyze distribution\n",
    "    try:\n",
    "        outputs = []\n",
    "        for output_str in df_training['output'][:100]:  # Sample first 100 for analysis\n",
    "            output_dict = json.loads(output_str)\n",
    "            outputs.append(output_dict)\n",
    "        \n",
    "        if outputs:\n",
    "            sentiments = [o.get('sentiment', 'unknown') for o in outputs]\n",
    "            directions = [o.get('price_direction', 'unknown') for o in outputs]\n",
    "            strengths = [o.get('impact_strength', 'unknown') for o in outputs]\n",
    "            \n",
    "            print(f\"\\\\nüìä Sample Distribution (first 100 items):\")\n",
    "            print(f\"   Sentiments: {dict(pd.Series(sentiments).value_counts())}\")\n",
    "            print(f\"   Directions: {dict(pd.Series(directions).value_counts())}\")\n",
    "            print(f\"   Strengths: {dict(pd.Series(strengths).value_counts())}\")\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # Display sample\n",
    "    print(f\"\\\\nüìÑ Sample Training Entry:\")\n",
    "    sample_idx = len(df_training) // 2\n",
    "    sample = df_training.iloc[sample_idx]\n",
    "    \n",
    "    print(\"\\\\nInstruction:\")\n",
    "    print(sample['instruction'])\n",
    "    \n",
    "    print(\"\\\\nInput:\")\n",
    "    print(sample['input'])\n",
    "    \n",
    "    print(\"\\\\nOutput:\")\n",
    "    print(sample['output'])\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå No training samples generated\")\n",
    "    df_training = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b582a841",
   "metadata": {},
   "source": [
    "# 5. Create and Upload Individual News Training Dataset\n",
    "\n",
    "Convert ALL individual news training samples into a HuggingFace dataset for Bitcoin price prediction model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "41bb9343",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì¶ Creating HuggingFace Dataset...\n",
      "‚úÖ Dataset created with 49171 samples\n",
      "üöÄ Uploading dataset to tahamajs/bitcoin-individual-news-dataset...\n",
      "‚úÖ Dataset created with 49171 samples\n",
      "üöÄ Uploading dataset to tahamajs/bitcoin-individual-news-dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfeb11d66c804890b91698d0b97ebb60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ? shards/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03d29b3a75a1453b9bf89c210cc6fc7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/50 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "176d156c4da0496da76c262679061cf9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0)                : |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bb428a819f84fa5a160a3c4ab9c8ab1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload                         : |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7215c9276c6f495ab25dd07219759d83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "                                        :  73%|#######3  | 12.2MB / 16.7MB            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\nüéâ Dataset Successfully Uploaded!\n",
      "üîó Dataset URL: https://huggingface.co/datasets/tahamajs/bitcoin-individual-news-dataset\n",
      "üìä Total Samples: 49171\n",
      "\\nüìè Dataset Statistics:\n",
      "   Average instruction length: 338 characters\n",
      "   Average input length: 623 characters\n",
      "   Average output length: 243 characters\n",
      "   Total dataset size: 1204 avg chars per sample\n"
     ]
    }
   ],
   "source": [
    "# Create HuggingFace Dataset and Upload\n",
    "if not df_training.empty:\n",
    "    print(\"üì¶ Creating HuggingFace Dataset...\")\n",
    "    \n",
    "    # Prepare dataset\n",
    "    dataset_dict = {\n",
    "        'instruction': df_training['instruction'].tolist(),\n",
    "        'input': df_training['input'].tolist(),\n",
    "        'output': df_training['output'].tolist(),\n",
    "        'sample_id': df_training['sample_id'].tolist(),\n",
    "        'created_at': df_training['created_at'].tolist()\n",
    "    }\n",
    "    \n",
    "    # Create HuggingFace Dataset\n",
    "    hf_dataset = Dataset.from_dict(dataset_dict)\n",
    "    \n",
    "    print(f\"‚úÖ Dataset created with {len(hf_dataset)} samples\")\n",
    "    \n",
    "    # Upload to HuggingFace Hub\n",
    "    repo_name = \"bitcoin-individual-news-dataset\"\n",
    "    repo_id = f\"tahamajs/{repo_name}\"\n",
    "    \n",
    "    try:\n",
    "        print(f\"üöÄ Uploading dataset to {repo_id}...\")\n",
    "        \n",
    "        hf_dataset.push_to_hub(\n",
    "            repo_id,\n",
    "            commit_message=f\"Bitcoin individual news training dataset with {len(hf_dataset)} samples from individual news items\",\n",
    "        )\n",
    "        \n",
    "        print(f\"\\\\nüéâ Dataset Successfully Uploaded!\")\n",
    "        print(f\"üîó Dataset URL: https://huggingface.co/datasets/{repo_id}\")\n",
    "        print(f\"üìä Total Samples: {len(hf_dataset)}\")\n",
    "        \n",
    "        # Dataset statistics\n",
    "        avg_instruction_length = sum(len(x) for x in dataset_dict['instruction']) / len(dataset_dict['instruction'])\n",
    "        avg_input_length = sum(len(x) for x in dataset_dict['input']) / len(dataset_dict['input'])\n",
    "        avg_output_length = sum(len(x) for x in dataset_dict['output']) / len(dataset_dict['output'])\n",
    "        \n",
    "        print(f\"\\\\nüìè Dataset Statistics:\")\n",
    "        print(f\"   Average instruction length: {avg_instruction_length:.0f} characters\")\n",
    "        print(f\"   Average input length: {avg_input_length:.0f} characters\")\n",
    "        print(f\"   Average output length: {avg_output_length:.0f} characters\")\n",
    "        print(f\"   Total dataset size: {avg_instruction_length + avg_input_length + avg_output_length:.0f} avg chars per sample\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Upload failed: {e}\")\n",
    "        print(\"üí° Saving dataset locally instead...\")\n",
    "        \n",
    "        # Save locally as backup\n",
    "        df_training.to_json(f\"bitcoin_news_effects_dataset_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\", \n",
    "                           orient='records', indent=2)\n",
    "        print(\"üíæ Dataset saved locally as JSON file\")\n",
    "\n",
    "else:\n",
    "    print(\"‚ùå No dataset to upload - training samples list is empty\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac8d82c1",
   "metadata": {},
   "source": [
    "# 6. Sample Analysis and Outputs\n",
    "\n",
    "Display detailed examples of the Bitcoin news effects analysis to understand the quality and format of the generated dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "84804bd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç COMPREHENSIVE SAMPLE ANALYSIS\n",
      "======================================================================\n",
      "\\nüìã INSTRUCTION:\n",
      "--------------------------------------------------\n",
      "Analyze Bitcoin news and predict price impact. Return JSON with this exact structure:\n",
      "\n",
      "{\n",
      "  \"sentiment\": \"bullish|neutral|bearish\",\n",
      "  \"price_direction\": \"up|sideways|down\",\n",
      "  \"impact_strength\": \"high|medium|low\", \n",
      "  \"timeframe\": \"immediate|short_term|medium_term\",\n",
      "  \"confidence\": 0.75,\n",
      "  \"key_reason\": \"Brief explanation of main factor\"\n",
      "}\n",
      "\\nüìä INPUT (News Context):\n",
      "--------------------------------------------------\n",
      "News Title: PayPal ups its weekly cryptocurrency buy limit to $100,000\n",
      "\n",
      "News Summary: PayPal has increased its weekly cryptocurrency buy limit to $100,000, signaling continued expansion and confidence in the digital asset space. This move, following similar expansions by Square and the introduction of crypto checkout options, makes it easier for retail investors to engage with cryptocurrencies. Higher buy limits can increase overall demand.\n",
      "\n",
      "Impact Tags: adoption, liquidity\n",
      "\n",
      "Market Context:\n",
      "Bull 45% | Base 35% | Bear 20%\n",
      "\n",
      "Daily Recommendations:\n",
      "Short-term: HOLD\n",
      "Long-term: BUY\n",
      "\\nüéØ OUTPUT (Bitcoin Effects Analysis):\n",
      "--------------------------------------------------\n",
      "{\n",
      "  \"sentiment\": \"bullish\",\n",
      "  \"price_direction\": \"up\",\n",
      "  \"impact_strength\": \"medium\",\n",
      "  \"timeframe\": \"short_term\",\n",
      "  \"confidence\": 0.7,\n",
      "  \"key_reason\": \"PayPal's increased crypto buy limit makes it more accessible for retail investors to purchase digita\"\n",
      "}\n",
      "\\nüí° KEY INSIGHTS FROM THIS ANALYSIS:\n",
      "----------------------------------------\n",
      "Sentiment: bullish\n",
      "Price Direction: up\n",
      "Impact Strength: medium\n",
      "Timeframe: short_term\n",
      "Confidence: 0.70\n",
      "Key Reason: PayPal's increased crypto buy limit makes it more accessible for retail investors to purchase digita\n",
      "\\nüìà DATASET OVERVIEW:\n",
      "==================================================\n",
      "Total samples: 49171\n",
      "\\nSentiment Distribution:\n",
      "  bullish: 29671 (60.3%)\n",
      "  bearish: 10107 (20.6%)\n",
      "  neutral: 9393 (19.1%)\n",
      "\\nPrice Direction Distribution:\n",
      "  up: 29671 (60.3%)\n",
      "  down: 10107 (20.6%)\n",
      "  sideways: 9393 (19.1%)\n",
      "\\nImpact Strength Distribution:\n",
      "  medium: 21977 (44.7%)\n",
      "  high: 14320 (29.1%)\n",
      "  low: 12874 (26.2%)\n",
      "\\nTimeframe Distribution:\n",
      "  short_term: 24764 (50.4%)\n",
      "  medium_term: 24407 (49.6%)\n",
      "\\nSentiment Distribution:\n",
      "  bullish: 29671 (60.3%)\n",
      "  bearish: 10107 (20.6%)\n",
      "  neutral: 9393 (19.1%)\n",
      "\\nPrice Direction Distribution:\n",
      "  up: 29671 (60.3%)\n",
      "  down: 10107 (20.6%)\n",
      "  sideways: 9393 (19.1%)\n",
      "\\nImpact Strength Distribution:\n",
      "  medium: 21977 (44.7%)\n",
      "  high: 14320 (29.1%)\n",
      "  low: 12874 (26.2%)\n",
      "\\nTimeframe Distribution:\n",
      "  short_term: 24764 (50.4%)\n",
      "  medium_term: 24407 (49.6%)\n"
     ]
    }
   ],
   "source": [
    "# Display comprehensive sample outputs\n",
    "if not df_training.empty:\n",
    "    print(\"üîç COMPREHENSIVE SAMPLE ANALYSIS\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Select a representative sample\n",
    "    sample_idx = len(df_training) // 2\n",
    "    sample = df_training.iloc[sample_idx]\n",
    "    \n",
    "    print(\"\\\\nüìã INSTRUCTION:\")\n",
    "    print(\"-\" * 50)\n",
    "    print(sample['instruction'])\n",
    "    \n",
    "    print(\"\\\\nüìä INPUT (News Context):\")\n",
    "    print(\"-\" * 50)\n",
    "    input_lines = sample['input'].split('\\\\n')\n",
    "    for line in input_lines[:30]:  # Show first 30 lines\n",
    "        print(line)\n",
    "    if len(input_lines) > 30:\n",
    "        print(f\"... ({len(input_lines) - 30} more lines)\")\n",
    "    \n",
    "    print(\"\\\\nüéØ OUTPUT (Bitcoin Effects Analysis):\")\n",
    "    print(\"-\" * 50)\n",
    "    try:\n",
    "        output_dict = json.loads(sample['output'])\n",
    "        print(json.dumps(output_dict, indent=2, ensure_ascii=False)[:1000])  # First 1000 chars\n",
    "        if len(sample['output']) > 1000:\n",
    "            print(\"\\\\n... (output truncated for display)\")\n",
    "            \n",
    "        # Extract key insights (using correct field names)\n",
    "        print(\"\\\\nüí° KEY INSIGHTS FROM THIS ANALYSIS:\")\n",
    "        print(\"-\" * 40)\n",
    "        print(f\"Sentiment: {output_dict.get('sentiment', 'N/A')}\")\n",
    "        print(f\"Price Direction: {output_dict.get('price_direction', 'N/A')}\")\n",
    "        print(f\"Impact Strength: {output_dict.get('impact_strength', 'N/A')}\")\n",
    "        print(f\"Timeframe: {output_dict.get('timeframe', 'N/A')}\")\n",
    "        print(f\"Confidence: {output_dict.get('confidence', 0):.2f}\")\n",
    "        print(f\"Key Reason: {output_dict.get('key_reason', 'N/A')}\")\n",
    "            \n",
    "    except json.JSONDecodeError:\n",
    "        print(\"Raw output (JSON parsing failed):\")\n",
    "        print(sample['output'][:500] + \"...\" if len(sample['output']) > 500 else sample['output'])\n",
    "    \n",
    "    print(\"\\\\nüìà DATASET OVERVIEW:\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"Total samples: {len(df_training)}\")\n",
    "    \n",
    "    # Analyze output patterns (using correct field names)\n",
    "    sentiments = []\n",
    "    directions = []\n",
    "    strengths = []\n",
    "    timeframes = []\n",
    "    \n",
    "    for _, row in df_training.iterrows():\n",
    "        try:\n",
    "            output_dict = json.loads(row['output'])\n",
    "            sentiments.append(output_dict.get('sentiment', 'unknown'))\n",
    "            directions.append(output_dict.get('price_direction', 'unknown'))\n",
    "            strengths.append(output_dict.get('impact_strength', 'unknown'))\n",
    "            timeframes.append(output_dict.get('timeframe', 'unknown'))\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    if sentiments:\n",
    "        print(f\"\\\\nSentiment Distribution:\")\n",
    "        sentiment_counts = pd.Series(sentiments).value_counts()\n",
    "        for sentiment, count in sentiment_counts.items():\n",
    "            print(f\"  {sentiment}: {count} ({count/len(sentiments)*100:.1f}%)\")\n",
    "        \n",
    "        print(f\"\\\\nPrice Direction Distribution:\")\n",
    "        direction_counts = pd.Series(directions).value_counts()\n",
    "        for direction, count in direction_counts.items():\n",
    "            print(f\"  {direction}: {count} ({count/len(directions)*100:.1f}%)\")\n",
    "            \n",
    "        print(f\"\\\\nImpact Strength Distribution:\")\n",
    "        strength_counts = pd.Series(strengths).value_counts()\n",
    "        for strength, count in strength_counts.items():\n",
    "            print(f\"  {strength}: {count} ({count/len(strengths)*100:.1f}%)\")\n",
    "            \n",
    "        print(f\"\\\\nTimeframe Distribution:\")\n",
    "        timeframe_counts = pd.Series(timeframes).value_counts()\n",
    "        for timeframe, count in timeframe_counts.items():\n",
    "            print(f\"  {timeframe}: {count} ({count/len(timeframes)*100:.1f}%)\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå No training samples available for analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bcd2776",
   "metadata": {},
   "source": [
    "# Bitcoin News Effects Analysis Dataset\n",
    "\n",
    "This notebook creates a specialized dataset that analyzes news summaries and their predicted effects on Bitcoin prices. It uses:\n",
    "\n",
    "1. **Existing comprehensive news data** from your previous dataset\n",
    "2. **Gemini Flash LLM** to analyze news summaries and predict Bitcoin effects\n",
    "3. **Structured output format** for effect prediction training\n",
    "\n",
    "**Dataset Purpose:**\n",
    "- Train models to predict Bitcoin price effects from news summaries\n",
    "- Analyze sentiment, market impact, and price drivers\n",
    "- Generate actionable trading insights from news analysis\n",
    "\n",
    "**Output Format:**\n",
    "- Input: News summary + market context\n",
    "- Output: Structured Bitcoin effect analysis (direction, magnitude, timeframe, confidence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5ddfe8cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ INDIVIDUAL BITCOIN NEWS TRAINING DATASET - READY!\n",
      "============================================================\n",
      "\n",
      "üìã WHAT THIS CREATES:\n",
      "‚Ä¢ Training sample for EACH individual news item (~100 per day)\n",
      "‚Ä¢ Massive dataset with detailed news-to-price-impact predictions\n",
      "‚Ä¢ Each news article gets its own training example\n",
      "\n",
      "üìä DATASET SCALE:\n",
      "‚Ä¢ ~100 news items per day\n",
      "‚Ä¢ Thousands of days of data\n",
      "‚Ä¢ Potentially 100,000+ training samples\n",
      "\n",
      "üéØ TRAINING FORMAT PER NEWS ITEM:\n",
      "‚Ä¢ INSTRUCTION: 'Analyze Bitcoin news and predict price impact'\n",
      "‚Ä¢ INPUT: Individual news title + summary + tags + market context\n",
      "‚Ä¢ OUTPUT: JSON with sentiment, direction, strength, timeframe, confidence, reason\n",
      "\n",
      "üí° PERFECT FOR:\n",
      "‚Ä¢ Training models on granular news analysis\n",
      "‚Ä¢ Learning specific news patterns and their Bitcoin impacts\n",
      "‚Ä¢ Building highly detailed prediction systems\n",
      "\n",
      "‚ö° Execute all cells to generate comprehensive individual news dataset!\n"
     ]
    }
   ],
   "source": [
    "print(\"üöÄ INDIVIDUAL BITCOIN NEWS TRAINING DATASET - READY!\")\n",
    "print(\"=\" * 60)\n",
    "print()\n",
    "print(\"üìã WHAT THIS CREATES:\")\n",
    "print(\"‚Ä¢ Training sample for EACH individual news item (~100 per day)\")\n",
    "print(\"‚Ä¢ Massive dataset with detailed news-to-price-impact predictions\")\n",
    "print(\"‚Ä¢ Each news article gets its own training example\")\n",
    "print()\n",
    "print(\"üìä DATASET SCALE:\")\n",
    "print(\"‚Ä¢ ~100 news items per day\")\n",
    "print(\"‚Ä¢ Thousands of days of data\")\n",
    "print(\"‚Ä¢ Potentially 100,000+ training samples\")\n",
    "print()\n",
    "print(\"üéØ TRAINING FORMAT PER NEWS ITEM:\")\n",
    "print(\"‚Ä¢ INSTRUCTION: 'Analyze Bitcoin news and predict price impact'\")\n",
    "print(\"‚Ä¢ INPUT: Individual news title + summary + tags + market context\")  \n",
    "print(\"‚Ä¢ OUTPUT: JSON with sentiment, direction, strength, timeframe, confidence, reason\")\n",
    "print()\n",
    "print(\"üí° PERFECT FOR:\")\n",
    "print(\"‚Ä¢ Training models on granular news analysis\")\n",
    "print(\"‚Ä¢ Learning specific news patterns and their Bitcoin impacts\")\n",
    "print(\"‚Ä¢ Building highly detailed prediction systems\")\n",
    "print()\n",
    "print(\"‚ö° Execute all cells to generate comprehensive individual news dataset!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab90475",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
