{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e4c98883",
   "metadata": {},
   "outputs": [],
   "source": [
    "API_KEYS = [\"AIzaSyAMcPkozrBy3kBUKMpH9ZFWBNmNUyMwqDk\",\"AIzaSyBh2az_zewPe_J6CAFIdhx69eDlnKCXaUU\",\"AIzaSyBRUEAZ7p5w4Cr5XQCkS5WJJJPCs232L3Q\", \"AIzaSyBxMeLxWnw8bwmp2mJRSKWJlMla_YY-Kzk\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cdbca420",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================== YOUR API KEYS ====================================\n",
    "API_KEYS: List[str] = [\n",
    "    \"AIzaSyCHcRH2rxSJr1AxMeIIZD3jbYfFC9jxwXg\",\n",
    "    \"AIzaSyB_-MxHa7eU1gkJtFoLQAbrUlUvfCoBjqU\",\n",
    "    \"AIzaSyC5Eq-TLStY80ZJO_oS9hH_d6YD19PPRLw\",\n",
    "    \"AIzaSyBsjgTc0HNagH0-ZjO5phzCO2DmXs53fCs\",\n",
    "    \"AIzaSyAlBXQKAC6dz0JMjfq1T9T_w9dkKEUM0io\",\n",
    "    \"AIzaSyCs4JlPkzC9H3J4blim26erQgUM4yajpiM\",\n",
    "    \"AIzaSyDzHrToi_SvJq1tQq9zJu_xtM-BWGBWWDc\",\n",
    "    \"AIzaSyBtRx-SCSgPkXYjDapQCSOFoIsn6TgUD2M\",\n",
    "    \"AIzaSyDveD6rvxEApQGuCjaKMY16pcYFsIviNo4\",\n",
    "    \"AIzaSyD7G-lakjCP6exsVtZiFMoVH_WWXRlgVb8\",\n",
    "    \"AIzaSyBbvVRuhUGsqijI7_lsvBt9i9OpknFWZrw\",\n",
    "    \"AIzaSyAO4ya7c2nkEgvbCyNl8ZCWdLOM-GA6f9U\",\n",
    "    \"AIzaSyBtYHoV8slNiVdF5ARIyTq5ZVKfeD6LFTA\",\n",
    "    \"AIzaSyADs4Sje1AO3aqq-EIf9Bo5D4czega01HY\",\n",
    "    \"AIzaSyDoMJOxdM6EAj_rt52QVRL721WxCKWhQos\",\n",
    "    \"AIzaSyBF5YJl1pWA_OEsrzhNrVWyThgAFSYqO2Y\",\n",
    "    \"AIzaSyBZnZogRQqcm21U6_nr3iJIB7f2NBoNH_w\",\n",
    "    \"AIzaSyBPYSJTPKStXfEp7yDIDrpQ42hO_KRtJqc\",\n",
    "    \"AIzaSyAcswgs2xk2z2LNgsx_s7Q8mBNncB_0OzA\"\n",
    "    \n",
    "    \n",
    "    \n",
    "]\n",
    "assert len(API_KEYS) > 0, \"Provide at least one API key.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca0cec21",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ecac2795",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Attempting full load from HF (non-streaming)…\n",
      "[INFO] Loaded 210832 rows into memory.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Indexing by date: 210832it [01:15, 2808.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loaded-first and indexed 192008 items (of 210832 seen) across 2614 dates from 2017-11-02 to 2024-12-31.\n",
      "[INFO] Capping workers to 19 (available API keys = 19).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LLM jobs (workers=19):   5%|▌         | 133/2557 [03:00<4:53:15,  7.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] [WARN] 2018-03-11 failed: JSONDecodeError: Expecting ',' delimiter: line 254 column 44 (char 13691)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LLM jobs (workers=19):   6%|▌         | 141/2557 [04:13<5:45:40,  8.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] [WARN] 2018-03-10 failed: JSONDecodeError: Unterminated string starting at: line 200 column 18 (char 11655)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LLM jobs (workers=19):   6%|▌         | 143/2557 [04:13<4:34:45,  6.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] [WARN] 2018-02-12 failed: JSONDecodeError: Expecting ',' delimiter: line 357 column 29 (char 19204)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LLM jobs (workers=19):   6%|▌         | 147/2557 [04:13<2:41:10,  4.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] [WARN] 2018-03-16 failed: JSONDecodeError: Expecting ',' delimiter: line 49 column 44 (char 2514)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LLM jobs (workers=19):   6%|▌         | 151/2557 [04:14<1:29:32,  2.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] [WARN] 2018-03-01 failed: JSONDecodeError: Expecting ',' delimiter: line 1 column 4416 (char 4415)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LLM jobs (workers=19):   6%|▌         | 154/2557 [04:15<58:31,  1.46s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] [WARN] 2018-04-11 failed: JSONDecodeError: Expecting ',' delimiter: line 258 column 49 (char 13185)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LLM jobs (workers=19):   6%|▋         | 166/2557 [07:22<9:04:28, 13.66s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] [WARN] 2018-06-20 failed: JSONDecodeError: Expecting ',' delimiter: line 213 column 83 (char 11961)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LLM jobs (workers=19):   7%|▋         | 172/2557 [07:28<2:06:45,  3.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] [WARN] 2018-06-28 failed: JSONDecodeError: Unterminated string starting at: line 217 column 9 (char 10807)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LLM jobs (workers=19):   7%|▋         | 175/2557 [07:45<2:43:04,  4.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] [WARN] 2018-04-13 failed: JSONDecodeError: Expecting ',' delimiter: line 6 column 29 (char 124)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LLM jobs (workers=19):   7%|▋         | 181/2557 [08:38<1:53:29,  2.87s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 629\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    626\u001b[0m futures \u001b[38;5;241m=\u001b[39m {ex\u001b[38;5;241m.\u001b[39msubmit(process_date_worker, d_str, date2items, per_date_dir, prompt_dir, keypool): d_str\n\u001b[1;32m    627\u001b[0m            \u001b[38;5;28;01mfor\u001b[39;00m d_str \u001b[38;5;129;01min\u001b[39;00m target_dates}\n\u001b[0;32m--> 629\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m fut \u001b[38;5;129;01min\u001b[39;00m tqdm(as_completed(futures), total\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(futures), desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLLM jobs (workers=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mworkers\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    630\u001b[0m     d_str \u001b[38;5;241m=\u001b[39m futures[fut]\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/my_project-rq9fZLKI/lib/python3.10/site-packages/tqdm/std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1181\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1182\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/concurrent/futures/_base.py:245\u001b[0m, in \u001b[0;36mas_completed\u001b[0;34m(fs, timeout)\u001b[0m\n\u001b[1;32m    241\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m(\n\u001b[1;32m    242\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m (of \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m) futures unfinished\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m (\n\u001b[1;32m    243\u001b[0m                 \u001b[38;5;28mlen\u001b[39m(pending), total_futures))\n\u001b[0;32m--> 245\u001b[0m \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwait_timeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    247\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m waiter\u001b[38;5;241m.\u001b[39mlock:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/threading.py:607\u001b[0m, in \u001b[0;36mEvent.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    606\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m signaled:\n\u001b[0;32m--> 607\u001b[0m     signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cond\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    608\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m signaled\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/threading.py:320\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 320\u001b[0m     \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    321\u001b[0m     gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 658\u001b[0m\n\u001b[1;32m    655\u001b[0m     _log(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m - Prompts (debug): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprompt_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    657\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 658\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[15], line 625\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    622\u001b[0m agg_effects_lines: List[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    623\u001b[0m agg_daily_rows: List[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m--> 625\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ThreadPoolExecutor(max_workers\u001b[38;5;241m=\u001b[39mworkers) \u001b[38;5;28;01mas\u001b[39;00m ex:\n\u001b[1;32m    626\u001b[0m     futures \u001b[38;5;241m=\u001b[39m {ex\u001b[38;5;241m.\u001b[39msubmit(process_date_worker, d_str, date2items, per_date_dir, prompt_dir, keypool): d_str\n\u001b[1;32m    627\u001b[0m                \u001b[38;5;28;01mfor\u001b[39;00m d_str \u001b[38;5;129;01min\u001b[39;00m target_dates}\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m fut \u001b[38;5;129;01min\u001b[39;00m tqdm(as_completed(futures), total\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(futures), desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLLM jobs (workers=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mworkers\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/concurrent/futures/_base.py:649\u001b[0m, in \u001b[0;36mExecutor.__exit__\u001b[0;34m(self, exc_type, exc_val, exc_tb)\u001b[0m\n\u001b[1;32m    648\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__exit__\u001b[39m(\u001b[38;5;28mself\u001b[39m, exc_type, exc_val, exc_tb):\n\u001b[0;32m--> 649\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshutdown\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwait\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    650\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/concurrent/futures/thread.py:235\u001b[0m, in \u001b[0;36mThreadPoolExecutor.shutdown\u001b[0;34m(self, wait, cancel_futures)\u001b[0m\n\u001b[1;32m    233\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m wait:\n\u001b[1;32m    234\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_threads:\n\u001b[0;32m--> 235\u001b[0m         \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/threading.py:1096\u001b[0m, in \u001b[0;36mThread.join\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1093\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcannot join current thread\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1095\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1096\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_wait_for_tstate_lock\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1097\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1098\u001b[0m     \u001b[38;5;66;03m# the behavior of a negative timeout isn't documented, but\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m     \u001b[38;5;66;03m# historically .join(timeout=x) for x<0 has acted as if timeout=0\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wait_for_tstate_lock(timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mmax\u001b[39m(timeout, \u001b[38;5;241m0\u001b[39m))\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/threading.py:1116\u001b[0m, in \u001b[0;36mThread._wait_for_tstate_lock\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m   1113\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m   1115\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1116\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mlock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblock\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m   1117\u001b[0m         lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m   1118\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stop()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# ONE-CELL BTC NEWS EFFECTS — LOAD FIRST, THEN PARALLEL TWO-POOL SAMPLING (2018–2024)\n",
    "# ------------------------------------------------------------------------------------\n",
    "# Per date d:\n",
    "#   • LONG-TERM POOL: random 50 from prior 60 days (d-60 .. d-1)\n",
    "#   • SHORT-TERM POOL: random 50 from SAME DAY (d)\n",
    "# Use Gemini (multi-key rotation) with a prediction/recommendation-focused prompt:\n",
    "#   • direction/magnitude/horizon/confidence/features\n",
    "#   • BUY/SELL/HOLD, scenario probabilities, risks\n",
    "#\n",
    "# PARALLELIZED: per-date LLM jobs run with ThreadPoolExecutor (configurable workers).\n",
    "# Files written:\n",
    "#   • outputs_btc_effects/per_date/YYYY-MM-DD.json\n",
    "#   • outputs_btc_effects/aggregated_effects.jsonl\n",
    "#   • outputs_btc_effects/aggregated_daily_view.jsonl\n",
    "#   • outputs_btc_effects/prompts_debug/YYYY-MM-DD.prompt.txt\n",
    "#\n",
    "# Security note: keys can be provided via env `GEMINI_API_KEYS=\"k1,k2,...\"` or inline list below.\n",
    "\n",
    "import os, sys, json, time, random, re, hashlib, threading\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Optional, Tuple\n",
    "from datetime import datetime, timedelta\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "# ----------------------------- DEPENDENCIES ----------------------------------\n",
    "try:\n",
    "    from dateutil import parser as dateparser\n",
    "except Exception as _e:\n",
    "    raise RuntimeError(\"Missing dependency python-dateutil. Install: pip install python-dateutil\") from _e\n",
    "\n",
    "try:\n",
    "    from datasets import load_dataset, Dataset\n",
    "except Exception as _e:\n",
    "    raise RuntimeError(\"Missing dependency 'datasets'. Install: pip install datasets\") from _e\n",
    "\n",
    "try:\n",
    "    from tqdm import tqdm\n",
    "except Exception:\n",
    "    def tqdm(x, **kwargs): return x\n",
    "\n",
    "try:\n",
    "    import google.generativeai as genai\n",
    "except Exception as _e:\n",
    "    raise RuntimeError(\"Missing dependency 'google-generativeai'. Install: pip install google-generativeai\") from _e\n",
    "\n",
    "# ===================== YOUR API KEYS (PER-WORKER ASSIGNMENT) =================\n",
    "# Preferred: set env GEMINI_API_KEYS=\"key1,key2,key3\"\n",
    "# API_KEYS: List[str] = [k.strip() for k in os.getenv(\"GEMINI_API_KEYS\", \"\").split(\",\") if k.strip()] or [\n",
    "#     # \"AIzaSy....\",  # optional inline keys; avoid committing to VCS\n",
    "#     # \"AIzaSy....\",\n",
    "# ]\n",
    "# ============================================================================\n",
    "\n",
    "# -------------------------------- CONFIG ------------------------------------\n",
    "HF_REPO                = \"edaschau/bitcoin_news\"\n",
    "\n",
    "START_DATE             = \"2018-01-01\"   # inclusive\n",
    "END_DATE               = \"2024-12-31\"   # inclusive\n",
    "\n",
    "LONG_WINDOW_DAYS       = 60\n",
    "LONG_POOL_N            = 50\n",
    "SHORT_POOL_N           = 50\n",
    "\n",
    "MODEL_NAME             = \"gemini-2.5-flash\"  # or \"gemini-2.5-flash\"\n",
    "GEN_TEMPERATURE        = 0.15\n",
    "GEN_TOP_P              = 0.9\n",
    "GEN_MAX_OUTPUT_TOKENS  = 12192\n",
    "\n",
    "OUTPUT_DIR             = \"outputs_btc_effects\"\n",
    "MIN_CHARS              = 120\n",
    "MAX_CHARS              = 600\n",
    "\n",
    "RETRIES                = 3\n",
    "SKIP_EXISTING          = True\n",
    "DRY_RUN                = False\n",
    "\n",
    "SEED                   = 42\n",
    "PREFER_FULL_LOAD       = True\n",
    "\n",
    "# Key/backoff controls\n",
    "MAX_GLOBAL_ATTEMPTS    = 1       # now handled per-worker via KeyPool; kept for compatibility\n",
    "PER_KEY_ATTEMPTS       = 1       # ditto\n",
    "BASE_BACKOFF_SEC       = 0.75\n",
    "JITTER_SEC             = 0.25\n",
    "\n",
    "# PARALLELISM\n",
    "MAX_WORKERS            = 20\n",
    "PER_JOB_SLEEP_JITTER   = (0.05, 0.20)\n",
    "\n",
    "# KeyPool tuning\n",
    "KEY_BASE_COOLDOWN_SEC  = 10.0    # start cooldown for a key after a retryable error\n",
    "KEY_MAX_COOLDOWN_SEC   = 120.0   # cap cooldown\n",
    "THREAD_THROTTLE_STEP   = 0.40    # how much extra delay to add to the worker after each retryable error\n",
    "THREAD_THROTTLE_MAX    = 3.0     # cap extra delay per worker\n",
    "\n",
    "# --------------------------------- UTILS ------------------------------------\n",
    "def _fail(msg: str, code: int = 1):\n",
    "    print(f\"[ERROR] {msg}\", file=sys.stderr); sys.exit(code)\n",
    "\n",
    "def _log(msg: str):\n",
    "    print(f\"[INFO] {msg}\")\n",
    "\n",
    "def ymd(dt_str: str) -> str:\n",
    "    try:\n",
    "        dt = dateparser.parse(dt_str)\n",
    "        return dt.strftime(\"%Y-%m-%d\")\n",
    "    except Exception:\n",
    "        return (dt_str or \"\")[:10]\n",
    "\n",
    "def sanitize_text(s: Optional[str]) -> str:\n",
    "    if not s: return \"\"\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    s = re.sub(r\"([=*_#\\-])\\1{5,}\", r\"\\1\\1\\1\", s)\n",
    "    return s\n",
    "\n",
    "def ensure_dir(p: str):\n",
    "    os.makedirs(p, exist_ok=True)\n",
    "\n",
    "def save_json(path: str, obj: Any):\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(obj, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "def force_json(s: str) -> Dict[str, Any]:\n",
    "    t = s.strip()\n",
    "    t = re.sub(r\"^```(?:json)?\\s*\", \"\", t)\n",
    "    t = re.sub(r\"\\s*```$\", \"\", t)\n",
    "    t = t.strip(\"\\ufeff\").strip()\n",
    "    t = re.sub(r\",\\s*}\", \"}\", t)\n",
    "    t = re.sub(r\",\\s*]\", \"]\", t)\n",
    "    t = t.replace(\"\\u201c\", '\"').replace(\"\\u201d\", '\"')\n",
    "    return json.loads(t)\n",
    "\n",
    "def stable_id(*parts: str) -> str:\n",
    "    base = \"||\".join([p if p is not None else \"\" for p in parts])\n",
    "    h = hashlib.blake2b(base.encode(\"utf-8\"), digest_size=6).hexdigest()\n",
    "    return f\"n{h}\"\n",
    "\n",
    "def sample_k(items: List[Any], k: int, rng: random.Random) -> List[Any]:\n",
    "    if len(items) <= k:\n",
    "        return list(items)\n",
    "    return rng.sample(items, k)\n",
    "\n",
    "# ---------------------------- DATA STRUCTURES --------------------------------\n",
    "@dataclass\n",
    "class NewsItem:\n",
    "    id: str\n",
    "    date: str\n",
    "    date_time: str\n",
    "    time_unix: Optional[int]\n",
    "    title: str\n",
    "    url: Optional[str]\n",
    "    source: Optional[str]\n",
    "    source_url: Optional[str]\n",
    "    text: str\n",
    "\n",
    "# --------------------------- LOAD DATASET FIRST ------------------------------\n",
    "def load_full_then_stream_if_needed(repo_id: str):\n",
    "    if PREFER_FULL_LOAD:\n",
    "        try:\n",
    "            _log(\"Attempting full load from HF (non-streaming)…\")\n",
    "            ds: Dataset = load_dataset(repo_id, split=\"train\")\n",
    "            _log(f\"Loaded {len(ds)} rows into memory.\")\n",
    "            return ds, False\n",
    "        except Exception as e:\n",
    "            _log(f\"Full load failed ({type(e).__name__}: {e}). Falling back to streaming.\")\n",
    "    try:\n",
    "        _log(\"Attempting streaming load from HF…\")\n",
    "        ds_stream = load_dataset(repo_id, split=\"train\", streaming=True)\n",
    "        _log(\"Streaming dataset ready.\")\n",
    "        return ds_stream, True\n",
    "    except Exception:\n",
    "        try:\n",
    "            _log(\"Trying CSV fallback (BTC_yahoo.csv)…\")\n",
    "            ds_stream = load_dataset(\n",
    "                \"csv\",\n",
    "                data_files=\"https://huggingface.co/datasets/edaschau/bitcoin_news/resolve/main/BTC_yahoo.csv?download=true\",\n",
    "                split=\"train\",\n",
    "                streaming=True,\n",
    "            )\n",
    "            _log(\"Streaming CSV ready.\")\n",
    "            return ds_stream, True\n",
    "        except Exception:\n",
    "            _log(\"Trying CSV fallback (bitcoin_news.csv)…\")\n",
    "            ds_stream = load_dataset(\n",
    "                \"csv\",\n",
    "                data_files=\"https://huggingface.co/datasets/edaschau/bitcoin_news/resolve/main/bitcoin_news.csv?download=true\",\n",
    "                split=\"train\",\n",
    "                streaming=True,\n",
    "            )\n",
    "            _log(\"Streaming CSV ready.\")\n",
    "            return ds_stream, True\n",
    "\n",
    "def row_iter(ds, streaming: bool):\n",
    "    if streaming:\n",
    "        for r in ds:\n",
    "            yield r\n",
    "    else:\n",
    "        for i in range(len(ds)):\n",
    "            yield ds[i]\n",
    "\n",
    "def pick_text(row: Dict[str, Any]) -> str:\n",
    "    body = sanitize_text(row.get(\"article_text\") or \"\")\n",
    "    if len(body) < MIN_CHARS:\n",
    "        body = sanitize_text(row.get(\"title\") or \"\")\n",
    "    if len(body) > MAX_CHARS:\n",
    "        body = body[:MAX_CHARS]\n",
    "    return body\n",
    "\n",
    "def to_news_item(row: Dict[str, Any]) -> Optional[NewsItem]:\n",
    "    dt_s = str(row.get(\"date_time\") or \"\")\n",
    "    d = ymd(dt_s)\n",
    "    if not d or len(d) != 10:\n",
    "        return None\n",
    "    text = pick_text(row)\n",
    "    tu = row.get(\"time_unix\", None)\n",
    "    try:\n",
    "        tu = int(tu)\n",
    "    except Exception:\n",
    "        tu = None\n",
    "    rid = stable_id(str(tu), str(row.get(\"url\")), str(row.get(\"title\")))\n",
    "    return NewsItem(\n",
    "        id=rid,\n",
    "        date=d,\n",
    "        date_time=dt_s,\n",
    "        time_unix=tu,\n",
    "        title=sanitize_text(row.get(\"title\") or \"\"),\n",
    "        url=row.get(\"url\"),\n",
    "        source=row.get(\"source\"),\n",
    "        source_url=row.get(\"source_url\"),\n",
    "        text=text\n",
    "    )\n",
    "\n",
    "# ------------------------ GEMINI PER-KEY CALL + HELPERS ----------------------\n",
    "def _configure_gemini(key: str):\n",
    "    genai.configure(api_key=key)\n",
    "\n",
    "def _mk_model():\n",
    "    return genai.GenerativeModel(\n",
    "        model_name=MODEL_NAME,\n",
    "        generation_config={\n",
    "            \"temperature\": GEN_TEMPERATURE,\n",
    "            \"top_p\": GEN_TOP_P,\n",
    "            \"max_output_tokens\": GEN_MAX_OUTPUT_TOKENS,\n",
    "        }\n",
    "    )\n",
    "\n",
    "def _is_retryable(exc: Exception) -> bool:\n",
    "    msg = f\"{type(exc).__name__}: {str(exc)}\".lower()\n",
    "    return any(x in msg for x in [\n",
    "        \"429\", \"quota\", \"rate\", \"resourceexhausted\", \"deadline\",\n",
    "        \"timeout\", \"temporarily\", \"unavailable\", \"internal\", \"server\"\n",
    "    ])\n",
    "\n",
    "def gemini_generate_json_with_key(key: str, prompt: str) -> Dict[str, Any]:\n",
    "    _configure_gemini(key)\n",
    "    model = _mk_model()\n",
    "    resp = model.generate_content(prompt)\n",
    "    text = getattr(resp, \"text\", None)\n",
    "    if not text:\n",
    "        raise RuntimeError(\"Empty response text from Gemini.\")\n",
    "    return force_json(text)\n",
    "\n",
    "# ----------------------------- KEY POOL MANAGER ------------------------------\n",
    "class KeyPool:\n",
    "    \"\"\"Assigns a stable API key to each worker thread. On retryable error:\n",
    "       - put the key on cooldown\n",
    "       - remove from thread\n",
    "       - worker will acquire a new available key\n",
    "    \"\"\"\n",
    "    def __init__(self, keys: List[str],\n",
    "                 base_cooldown: float = KEY_BASE_COOLDOWN_SEC,\n",
    "                 max_cooldown: float = KEY_MAX_COOLDOWN_SEC):\n",
    "        if not keys:\n",
    "            raise RuntimeError(\"No API keys provided. Set GEMINI_API_KEYS or fill API_KEYS.\")\n",
    "        self._lock = threading.Lock()\n",
    "        self._now = time.time\n",
    "        self._base_cooldown = base_cooldown\n",
    "        self._max_cooldown = max_cooldown\n",
    "        self._keys = [{\n",
    "            \"key\": k,\n",
    "            \"cooldown_until\": 0.0,\n",
    "            \"failures\": 0,\n",
    "            \"in_use\": set(),  # thread ids\n",
    "        } for k in keys]\n",
    "        self._thread_map: Dict[int, int] = {}  # thread_id -> key index\n",
    "\n",
    "    def _pick_available_index(self) -> Optional[int]:\n",
    "        now = self._now()\n",
    "        # Prefer not-in-use & not-cooling keys\n",
    "        candidates = [(i, ks) for i, ks in enumerate(self._keys) if ks[\"cooldown_until\"] <= now]\n",
    "        if not candidates:\n",
    "            return None\n",
    "        # Sort by current load (in_use size) then failures (prefer healthier keys)\n",
    "        candidates.sort(key=lambda t: (len(t[1][\"in_use\"]), t[1][\"failures\"]))\n",
    "        return candidates[0][0]\n",
    "\n",
    "    def acquire_for_thread(self) -> Optional[str]:\n",
    "        tid = threading.get_ident()\n",
    "        with self._lock:\n",
    "            # If this thread already has a healthy key, keep it\n",
    "            idx = self._thread_map.get(tid)\n",
    "            now = self._now()\n",
    "            if idx is not None:\n",
    "                ks = self._keys[idx]\n",
    "                if ks[\"cooldown_until\"] <= now:\n",
    "                    return ks[\"key\"]\n",
    "                # else drop the assignment (cooling)\n",
    "                ks[\"in_use\"].discard(tid)\n",
    "                del self._thread_map[tid]\n",
    "\n",
    "            # Find a new key\n",
    "            idx = self._pick_available_index()\n",
    "            if idx is None:\n",
    "                return None\n",
    "            self._keys[idx][\"in_use\"].add(tid)\n",
    "            self._thread_map[tid] = idx\n",
    "            return self._keys[idx][\"key\"]\n",
    "\n",
    "    def mark_failure_for_thread(self, extra_cooldown: Optional[float] = None):\n",
    "        \"\"\"Put current thread's key on cooldown and unassign it.\"\"\"\n",
    "        tid = threading.get_ident()\n",
    "        with self._lock:\n",
    "            idx = self._thread_map.get(tid)\n",
    "            if idx is None:  # nothing assigned\n",
    "                return\n",
    "            ks = self._keys[idx]\n",
    "            ks[\"failures\"] += 1\n",
    "            base = extra_cooldown if extra_cooldown is not None else self._base_cooldown\n",
    "            cd = min(self._max_cooldown, base * (2 ** (ks[\"failures\"] - 1)))\n",
    "            ks[\"cooldown_until\"] = self._now() + cd\n",
    "            ks[\"in_use\"].discard(tid)\n",
    "            del self._thread_map[tid]\n",
    "\n",
    "    def mark_success_for_thread(self):\n",
    "        \"\"\"Optional: on success, gently decay failure count to shorten future cooldowns.\"\"\"\n",
    "        tid = threading.get_ident()\n",
    "        with self._lock:\n",
    "            idx = self._thread_map.get(tid)\n",
    "            if idx is None:\n",
    "                return\n",
    "            ks = self._keys[idx]\n",
    "            if ks[\"failures\"] > 0:\n",
    "                ks[\"failures\"] -= 1\n",
    "\n",
    "    def all_cooling_time_left(self) -> float:\n",
    "        with self._lock:\n",
    "            now = self._now()\n",
    "            return max(0.0, max((ks[\"cooldown_until\"] - now) for ks in self._keys) if self._keys else 0.0)\n",
    "\n",
    "# -------------------------- PREDICTION-READY PROMPT --------------------------\n",
    "def build_llm_prompt(date_str: str, long_pool: List[NewsItem], short_pool: List[NewsItem]) -> str:\n",
    "    def as_line(idx: int, it: NewsItem) -> str:\n",
    "        return json.dumps({\n",
    "            \"idx\": idx,\n",
    "            \"id\": it.id,\n",
    "            \"title\": it.title,\n",
    "            \"url\": it.url,\n",
    "            \"source\": it.source,\n",
    "            \"source_url\": it.source_url,\n",
    "            \"date_time\": it.date_time,\n",
    "            \"time_unix\": it.time_unix,\n",
    "            \"body_snippet\": (it.text or it.title or \"\")[:MAX_CHARS],\n",
    "        }, ensure_ascii=False)\n",
    "\n",
    "    long_lines  = [as_line(i, it) for i, it in enumerate(long_pool,  start=1)]\n",
    "    short_lines = [as_line(i, it) for i, it in enumerate(short_pool, start=1)]\n",
    "\n",
    "    schema = {\n",
    "        \"date\": \"YYYY-MM-DD\",\n",
    "        \"long_term\": [{\n",
    "            \"pick_idx\": \"int from LONG_TERM_POOL\",\n",
    "            \"id\": \"original id\",\n",
    "            \"title\": \"string\",\n",
    "            \"url\": \"string or null\",\n",
    "            \"summary\": \"≤120 words focusing on durable drivers & why they matter\",\n",
    "            \"impact_horizon_months\": \"int months (e.g., 6, 12, 24)\",\n",
    "            \"direction\": \"bullish|bearish|uncertain\",\n",
    "            \"magnitude\": \"low|medium|high\",\n",
    "            \"confidence\": \"float 0-1\",\n",
    "            \"impact_tags\": [\"regulation\",\"institutional\",\"protocol\",\"macro\",\"adoption\",\"infrastructure\"],\n",
    "            \"features_for_model\": [\"tokens/phrases/entities useful for ML features\"],\n",
    "            \"rationale\": \"2–3 sentences explaining the causal pathway\"\n",
    "        }],\n",
    "        \"short_term\": [{\n",
    "            \"pick_idx\": \"int from SHORT_TERM_POOL\",\n",
    "            \"id\": \"original id\",\n",
    "            \"title\": \"string\",\n",
    "            \"url\": \"string or null\",\n",
    "            \"summary\": \"≤80 words focusing on near-term flow/volatility catalysts\",\n",
    "            \"impact_horizon_days\": \"int 1–14\",\n",
    "            \"direction\": \"bullish|bearish|uncertain\",\n",
    "            \"magnitude\": \"low|medium|high\",\n",
    "            \"confidence\": \"float 0-1\",\n",
    "            \"impact_window_days\": {\"start\": \"int ≥0\", \"end\": \"int ≥start\"},\n",
    "            \"impact_tags\": [\"ETF\",\"hack/outage\",\"listing\",\"CPI/FOMC\",\"miner\",\"liquidity\",\"legal\"],\n",
    "            \"features_for_model\": [\"tokens/phrases/entities useful for ML features\"],\n",
    "            \"rationale\": \"1–2 sentences on mechanism & timing\"\n",
    "        }],\n",
    "        \"daily_view\": {\n",
    "            \"date\": \"YYYY-MM-DD\",\n",
    "            \"summary\": \"Brief 3–5 sentence synthesis of the day’s most important signals across pools.\",\n",
    "            \"scenario_probs\": {\"bull\": \"float 0-1\", \"base\": \"float 0-1\", \"bear\": \"float 0-1\"},\n",
    "            \"recommendation_short_term\": {\"action\": \"BUY|SELL|HOLD\", \"probability\": \"float 0-1\"},\n",
    "            \"recommendation_long_term\":  {\"action\": \"BUY|SELL|HOLD\", \"probability\": \"float 0-1\"},\n",
    "            \"key_risks\": [\"bullet list of 3–6 risks to the view\"],\n",
    "            \"watch_items\": [\"specific tickers/venues/events to monitor next 1–7 days\"]\n",
    "        }\n",
    "    }\n",
    "\n",
    "    instructions = f\"\"\"\n",
    "ROLE\n",
    "You are a disciplined crypto market analyst optimizing **news for BTC prediction and actionable recommendations**. Focus date: {date_str}.\n",
    "\n",
    "INPUT POOLS\n",
    "- LONG_TERM_POOL: up to {LONG_POOL_N} randomly sampled items from the prior {LONG_WINDOW_DAYS} calendar days (strictly before the focus date).\n",
    "- SHORT_TERM_POOL: up to {SHORT_POOL_N} randomly sampled items from the focus date itself.\n",
    "\n",
    "TASKS\n",
    "1) From LONG_TERM_POOL select EXACTLY 10 items with credible **months/years** impact on BTC.\n",
    "2) From SHORT_TERM_POOL select EXACTLY 10 items with credible **1–14 day** impact on BTC price/volatility/liquidity.\n",
    "3) For each pick, provide direction (bullish/bearish/uncertain), magnitude (low/medium/high), confidence (0–1), and short rationale.\n",
    "4) Engineer **features_for_model**: short tokens/phrases/entities that a forecasting model can ingest (e.g., 'ETF_flow', 'SEC_approval', 'Lightning_capacity', 'exchange_outage', 'CPI_surprise').\n",
    "5) Produce a **daily_view** with scenario probabilities and explicit BUY/SELL/HOLD recommendations for short-term and long-term, plus key risks and watch items.\n",
    "6) Return ONE STRICT JSON object matching the schema below. **No extra text, no markdown, no code fences, no trailing commas.**\n",
    "\n",
    "STRICT JSON SCHEMA (structure only):\n",
    "{json.dumps(schema, ensure_ascii=False, indent=2)}\n",
    "\n",
    "LONG_TERM_POOL (one JSON per line; indices start at 1):\n",
    "{chr(10).join(long_lines)}\n",
    "\n",
    "SHORT_TERM_POOL (one JSON per line; indices start at 1):\n",
    "{chr(10).join(short_lines)}\n",
    "\n",
    "Return ONLY the final JSON.\n",
    "\"\"\".strip()\n",
    "    return instructions\n",
    "\n",
    "# ------------------------------ WORKER LOGIC ---------------------------------\n",
    "_THREAD_LOCAL = threading.local()\n",
    "\n",
    "def _get_thread_throttle() -> float:\n",
    "    val = getattr(_THREAD_LOCAL, \"throttle\", 0.0)\n",
    "    return float(val)\n",
    "\n",
    "def _inc_thread_throttle():\n",
    "    cur = _get_thread_throttle()\n",
    "    new = min(THREAD_THROTTLE_MAX, (cur or 0.0) + THREAD_THROTTLE_STEP)\n",
    "    _THREAD_LOCAL.throttle = new\n",
    "\n",
    "def _decay_thread_throttle():\n",
    "    cur = _get_thread_throttle()\n",
    "    if cur > 0:\n",
    "        _THREAD_LOCAL.throttle = max(0.0, cur * 0.5)\n",
    "\n",
    "def _maybe_desync():\n",
    "    time.sleep(random.uniform(*PER_JOB_SLEEP_JITTER))\n",
    "\n",
    "def process_date_worker(d_str: str,\n",
    "                        date2items: Dict[str, List[NewsItem]],\n",
    "                        per_date_dir: str,\n",
    "                        prompt_dir: str,\n",
    "                        keypool: KeyPool) -> Tuple[str, List[str], Optional[Dict[str, Any]]]:\n",
    "    \"\"\"Per-date: build pools, prompt, call Gemini with per-thread key assignment.\"\"\"\n",
    "    _maybe_desync()\n",
    "\n",
    "    out_fp = os.path.join(per_date_dir, f\"{d_str}.json\")\n",
    "    if SKIP_EXISTING and os.path.exists(out_fp):\n",
    "        return d_str, [], None\n",
    "\n",
    "    rng = random.Random(f\"{SEED}-{d_str}\")\n",
    "\n",
    "    # Short-term (same day)\n",
    "    same_day_items = date2items.get(d_str, [])\n",
    "    short_pool = sample_k(same_day_items, SHORT_POOL_N, rng)\n",
    "\n",
    "    # Long-term (prior window)\n",
    "    try:\n",
    "        d = datetime.strptime(d_str, \"%Y-%m-%d\").date()\n",
    "    except Exception:\n",
    "        return d_str, [], None\n",
    "    long_pool_all: List[NewsItem] = []\n",
    "    for i in range(1, LONG_WINDOW_DAYS + 1):\n",
    "        day = (d - timedelta(days=i)).strftime(\"%Y-%m-%d\")\n",
    "        if day in date2items:\n",
    "            long_pool_all.extend(date2items[day])\n",
    "    long_pool = sample_k(long_pool_all, LONG_POOL_N, rng)\n",
    "\n",
    "    # Build prompt and persist for debug\n",
    "    prompt = build_llm_prompt(d_str, long_pool, short_pool)\n",
    "    with open(os.path.join(prompt_dir, f\"{d_str}.prompt.txt\"), \"w\", encoding=\"utf-8\") as pf:\n",
    "        pf.write(prompt)\n",
    "\n",
    "    if DRY_RUN:\n",
    "        stub = {\n",
    "            \"date\": d_str,\n",
    "            \"long_term\": [],\n",
    "            \"short_term\": [],\n",
    "            \"daily_view\": {\n",
    "                \"date\": d_str,\n",
    "                \"summary\": \"\",\n",
    "                \"scenario_probs\": {\"bull\": 0.0, \"base\": 1.0, \"bear\": 0.0},\n",
    "                \"recommendation_short_term\": {\"action\": \"HOLD\", \"probability\": 0.5},\n",
    "                \"recommendation_long_term\":  {\"action\": \"HOLD\", \"probability\": 0.5},\n",
    "                \"key_risks\": [],\n",
    "                \"watch_items\": []\n",
    "            },\n",
    "            \"note\": f\"dry_run (long_pool={len(long_pool)}, short_pool={len(short_pool)})\"\n",
    "        }\n",
    "        save_json(out_fp, stub)\n",
    "        return d_str, [], stub[\"daily_view\"]\n",
    "\n",
    "    # ===== Gemini call with per-thread key, swap-on-limit, and worker throttle =====\n",
    "    last_exc = None\n",
    "    attempts = 0\n",
    "    while attempts < (RETRIES + 1):\n",
    "        attempts += 1\n",
    "\n",
    "        # Wait if all keys cooling\n",
    "        key = keypool.acquire_for_thread()\n",
    "        if key is None:\n",
    "            sleep_left = max(0.25, keypool.all_cooling_time_left())\n",
    "            time.sleep(sleep_left)\n",
    "            continue\n",
    "\n",
    "        # Apply worker-specific throttle (increases after rate limits)\n",
    "        throttle = _get_thread_throttle()\n",
    "        if throttle > 0:\n",
    "            time.sleep(throttle + random.uniform(0, 0.10))\n",
    "\n",
    "        try:\n",
    "            obj = gemini_generate_json_with_key(key, prompt)\n",
    "            obj[\"date\"] = d_str  # enforce\n",
    "            # success: decay worker throttle and mark success to relax future cooldowns\n",
    "            _decay_thread_throttle()\n",
    "            keypool.mark_success_for_thread()\n",
    "            break\n",
    "        except Exception as e:\n",
    "            last_exc = e\n",
    "            if _is_retryable(e):\n",
    "                # Put this key on cooldown and make the worker slower, then try a different key\n",
    "                keypool.mark_failure_for_thread(extra_cooldown=BASE_BACKOFF_SEC)\n",
    "                _inc_thread_throttle()\n",
    "                time.sleep(BASE_BACKOFF_SEC + random.uniform(0, JITTER_SEC))\n",
    "                continue\n",
    "            else:\n",
    "                # Non-retryable\n",
    "                raise\n",
    "    else:\n",
    "        # Exhausted attempts\n",
    "        if last_exc:\n",
    "            raise last_exc\n",
    "        raise RuntimeError(\"Gemini call failed after retries.\")\n",
    "\n",
    "    # Save per-date JSON\n",
    "    save_json(out_fp, obj)\n",
    "\n",
    "    # Flattened lines\n",
    "    flat_lines: List[str] = []\n",
    "    for effect_type in (\"long_term\", \"short_term\"):\n",
    "        for rec in obj.get(effect_type, []):\n",
    "            rec_flat = dict(rec)\n",
    "            rec_flat[\"effect_type\"] = effect_type\n",
    "            rec_flat[\"date\"] = d_str\n",
    "            flat_lines.append(json.dumps(rec_flat, ensure_ascii=False))\n",
    "\n",
    "    daily_view = obj.get(\"daily_view\", None) if isinstance(obj, dict) else None\n",
    "    return d_str, flat_lines, daily_view\n",
    "\n",
    "# --------------------------------- MAIN --------------------------------------\n",
    "def main():\n",
    "    # Validate dates\n",
    "    try:\n",
    "        start_dt = datetime.strptime(START_DATE, \"%Y-%m-%d\").date()\n",
    "        end_dt   = datetime.strptime(END_DATE, \"%Y-%m-%d\").date()\n",
    "    except Exception:\n",
    "        _fail(\"START_DATE/END_DATE must be YYYY-MM-DD\")\n",
    "\n",
    "    earliest_needed = start_dt - timedelta(days=LONG_WINDOW_DAYS)\n",
    "\n",
    "    # Output dirs\n",
    "    per_date_dir = os.path.join(OUTPUT_DIR, \"per_date\")\n",
    "    prompt_dir   = os.path.join(OUTPUT_DIR, \"prompts_debug\")\n",
    "    ensure_dir(OUTPUT_DIR); ensure_dir(per_date_dir); ensure_dir(prompt_dir)\n",
    "    agg_effects_path = os.path.join(OUTPUT_DIR, \"aggregated_effects.jsonl\")\n",
    "    agg_daily_path   = os.path.join(OUTPUT_DIR, \"aggregated_daily_view.jsonl\")\n",
    "\n",
    "    # ===== 1) LOAD THE DATASET FIRST =====\n",
    "    ds, is_streaming = load_full_then_stream_if_needed(HF_REPO)\n",
    "\n",
    "    # ===== 2) Build index date -> items =====\n",
    "    date2items: Dict[str, List[NewsItem]] = {}\n",
    "    kept, seen = 0, 0\n",
    "    for row in tqdm(row_iter(ds, is_streaming), desc=\"Indexing by date\"):\n",
    "        seen += 1\n",
    "        it = to_news_item(row)\n",
    "        if it is None:\n",
    "            continue\n",
    "        try:\n",
    "            d = datetime.strptime(it.date, \"%Y-%m-%d\").date()\n",
    "        except Exception:\n",
    "            continue\n",
    "        if d < earliest_needed or d > end_dt:\n",
    "            continue\n",
    "        date2items.setdefault(it.date, []).append(it)\n",
    "        kept += 1\n",
    "    _log(f\"Loaded-first and indexed {kept} items (of {seen} seen) across {len(date2items)} dates from {earliest_needed} to {end_dt}.\")\n",
    "\n",
    "    # Target dates\n",
    "    target_dates: List[str] = []\n",
    "    cur = start_dt\n",
    "    while cur <= end_dt:\n",
    "        target_dates.append(cur.strftime(\"%Y-%m-%d\"))\n",
    "        cur += timedelta(days=1)\n",
    "\n",
    "    # ===== 3) KeyPool and executor =====\n",
    "    keypool = KeyPool(API_KEYS)  # one key per worker when possible\n",
    "    workers = min(MAX_WORKERS, max(1, len(API_KEYS)))  # keep stable mapping 1:1 when feasible\n",
    "    if workers < MAX_WORKERS:\n",
    "        _log(f\"Capping workers to {workers} (available API keys = {len(API_KEYS)}).\")\n",
    "\n",
    "    agg_effects_lines: List[str] = []\n",
    "    agg_daily_rows: List[str] = []\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=workers) as ex:\n",
    "        futures = {ex.submit(process_date_worker, d_str, date2items, per_date_dir, prompt_dir, keypool): d_str\n",
    "                   for d_str in target_dates}\n",
    "\n",
    "        for fut in tqdm(as_completed(futures), total=len(futures), desc=f\"LLM jobs (workers={workers})\"):\n",
    "            d_str = futures[fut]\n",
    "            try:\n",
    "                _d, flat_lines, daily_view = fut.result()\n",
    "                agg_effects_lines.extend(flat_lines)\n",
    "                if isinstance(daily_view, dict):\n",
    "                    dv = dict(daily_view)\n",
    "                    dv[\"date\"] = d_str\n",
    "                    agg_daily_rows.append(json.dumps(dv, ensure_ascii=False))\n",
    "            except Exception as e:\n",
    "                _log(f\"[WARN] {d_str} failed: {type(e).__name__}: {e}\")\n",
    "\n",
    "    # ===== 4) Write aggregated outputs =====\n",
    "    if agg_effects_lines:\n",
    "        with open(agg_effects_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            for line in agg_effects_lines:\n",
    "                f.write(line + \"\\n\")\n",
    "    if agg_daily_rows:\n",
    "        with open(agg_daily_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            for line in agg_daily_rows:\n",
    "                f.write(line + \"\\n\")\n",
    "\n",
    "    _log(f\"Done. Outputs in: {OUTPUT_DIR}\")\n",
    "    _log(f\" - Per-date JSON: {per_date_dir}\")\n",
    "    _log(f\" - Aggregated (effects): {agg_effects_path} ({len(agg_effects_lines)} lines)\")\n",
    "    _log(f\" - Aggregated (daily_view): {agg_daily_path} ({len(agg_daily_rows)} lines)\")\n",
    "    _log(f\" - Prompts (debug): {prompt_dir}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fc844abf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "It seems you are trying to upload a large folder at once. This might take some time and then fail if the folder is too large. For such cases, it is recommended to upload in smaller batches or to use `HfApi().upload_large_folder(...)`/`hf upload-large-folder` instead. For more details, check out https://huggingface.co/docs/huggingface_hub/main/en/guides/upload#upload-a-large-folder.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02f32881d2ed4ab49a1dadea4e3b2db4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0)                : |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c94e4fbb42094c8b995e5ece614e9d61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload                         : |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0978541f6bd34f99b3d20d1b98109842",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  ...ects/_db_package/daily_view.parquet: 100%|##########|  222kB /  222kB            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "002c2944aea848918c9cfa8aaa97b820",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  ...effects/_db_package/effects.parquet: 100%|##########| 3.59MB / 3.59MB            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "102ee09fdbaa4adda489d014f1cc6f7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  ..._db_package/btc_news_effects.sqlite: 100%|##########| 10.2MB / 10.2MB            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m create_repo(repo_id\u001b[38;5;241m=\u001b[39mREPO_ID, repo_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Upload the whole folder under data/\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m \u001b[43mapi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupload_folder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mREPO_ID\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdataset\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfolder_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mLOCAL_FOLDER\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath_in_repo\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m     \u001b[49m\u001b[38;5;66;43;03m# you'll get data/per_date, data/aggregated_*.jsonl, data/prompts_debug\u001b[39;49;00m\n\u001b[1;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcommit_message\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mInitial upload of BTC news effects (2018–2024)\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m     22\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# Minimal dataset card (README)\u001b[39;00m\n\u001b[1;32m     25\u001b[0m readme \u001b[38;5;241m=\u001b[39m textwrap\u001b[38;5;241m.\u001b[39mdedent(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;124m# BTC News Effects (2018–2024)\u001b[39m\n\u001b[1;32m     27\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     64\u001b[0m \n\u001b[1;32m     65\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m)\u001b[38;5;241m.\u001b[39mstrip()\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/my_project-rq9fZLKI/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[1;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/my_project-rq9fZLKI/lib/python3.10/site-packages/huggingface_hub/hf_api.py:1665\u001b[0m, in \u001b[0;36mfuture_compatible.<locals>._inner\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1662\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun_as_future(fn, \u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1664\u001b[0m \u001b[38;5;66;03m# Otherwise, call the function normally\u001b[39;00m\n\u001b[0;32m-> 1665\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/my_project-rq9fZLKI/lib/python3.10/site-packages/huggingface_hub/hf_api.py:4976\u001b[0m, in \u001b[0;36mHfApi.upload_folder\u001b[0;34m(self, repo_id, folder_path, path_in_repo, commit_message, commit_description, token, repo_type, revision, create_pr, parent_commit, allow_patterns, ignore_patterns, delete_patterns, run_as_future)\u001b[0m\n\u001b[1;32m   4972\u001b[0m commit_operations \u001b[38;5;241m=\u001b[39m delete_operations \u001b[38;5;241m+\u001b[39m add_operations\n\u001b[1;32m   4974\u001b[0m commit_message \u001b[38;5;241m=\u001b[39m commit_message \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUpload folder using huggingface_hub\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 4976\u001b[0m commit_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_commit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4977\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4978\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4979\u001b[0m \u001b[43m    \u001b[49m\u001b[43moperations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit_operations\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4980\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcommit_message\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit_message\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4981\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcommit_description\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit_description\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4982\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4983\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4984\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_pr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcreate_pr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4985\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparent_commit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparent_commit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4986\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4988\u001b[0m \u001b[38;5;66;03m# Create url to uploaded folder (for legacy return value)\u001b[39;00m\n\u001b[1;32m   4989\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m create_pr \u001b[38;5;129;01mand\u001b[39;00m commit_info\u001b[38;5;241m.\u001b[39mpr_url \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/my_project-rq9fZLKI/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[1;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/my_project-rq9fZLKI/lib/python3.10/site-packages/huggingface_hub/hf_api.py:1665\u001b[0m, in \u001b[0;36mfuture_compatible.<locals>._inner\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1662\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun_as_future(fn, \u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1664\u001b[0m \u001b[38;5;66;03m# Otherwise, call the function normally\u001b[39;00m\n\u001b[0;32m-> 1665\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/my_project-rq9fZLKI/lib/python3.10/site-packages/huggingface_hub/hf_api.py:4324\u001b[0m, in \u001b[0;36mHfApi.create_commit\u001b[0;34m(self, repo_id, operations, commit_message, commit_description, token, repo_type, revision, create_pr, num_threads, parent_commit, run_as_future)\u001b[0m\n\u001b[1;32m   4321\u001b[0m params \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcreate_pr\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m\"\u001b[39m} \u001b[38;5;28;01mif\u001b[39;00m create_pr \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   4323\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 4324\u001b[0m     commit_resp \u001b[38;5;241m=\u001b[39m \u001b[43mget_session\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpost\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit_url\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4325\u001b[0m     hf_raise_for_status(commit_resp, endpoint_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcommit\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   4326\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m RepositoryNotFoundError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/my_project-rq9fZLKI/lib/python3.10/site-packages/requests/sessions.py:637\u001b[0m, in \u001b[0;36mSession.post\u001b[0;34m(self, url, data, json, **kwargs)\u001b[0m\n\u001b[1;32m    626\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpost\u001b[39m(\u001b[38;5;28mself\u001b[39m, url, data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, json\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    627\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a POST request. Returns :class:`Response` object.\u001b[39;00m\n\u001b[1;32m    628\u001b[0m \n\u001b[1;32m    629\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    634\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[1;32m    635\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 637\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPOST\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjson\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/my_project-rq9fZLKI/lib/python3.10/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[1;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    587\u001b[0m }\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/my_project-rq9fZLKI/lib/python3.10/site-packages/requests/sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[1;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43madapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/my_project-rq9fZLKI/lib/python3.10/site-packages/huggingface_hub/utils/_http.py:96\u001b[0m, in \u001b[0;36mUniqueRequestIdAdapter.send\u001b[0;34m(self, request, *args, **kwargs)\u001b[0m\n\u001b[1;32m     94\u001b[0m     logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSend: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_curlify(request)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m requests\u001b[38;5;241m.\u001b[39mRequestException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     98\u001b[0m     request_id \u001b[38;5;241m=\u001b[39m request\u001b[38;5;241m.\u001b[39mheaders\u001b[38;5;241m.\u001b[39mget(X_AMZN_TRACE_ID)\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/my_project-rq9fZLKI/lib/python3.10/site-packages/requests/adapters.py:667\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    664\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m TimeoutSauce(connect\u001b[38;5;241m=\u001b[39mtimeout, read\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m    666\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 667\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    668\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    669\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    670\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    671\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    672\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    673\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    674\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    675\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    676\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    677\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    678\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    679\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    681\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    682\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/my_project-rq9fZLKI/lib/python3.10/site-packages/urllib3/connectionpool.py:787\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    784\u001b[0m response_conn \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    786\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[0;32m--> 787\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    788\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    789\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    790\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    791\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    800\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    802\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n\u001b[1;32m    803\u001b[0m clean_exit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/my_project-rq9fZLKI/lib/python3.10/site-packages/urllib3/connectionpool.py:493\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    490\u001b[0m \u001b[38;5;66;03m# conn.request() calls http.client.*.request, not the method in\u001b[39;00m\n\u001b[1;32m    491\u001b[0m \u001b[38;5;66;03m# urllib3.request. It also calls makefile (recv) on the socket.\u001b[39;00m\n\u001b[1;32m    492\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 493\u001b[0m     \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    495\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    496\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    499\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43menforce_content_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menforce_content_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    502\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    504\u001b[0m \u001b[38;5;66;03m# We are swallowing BrokenPipeError (errno.EPIPE) since the server is\u001b[39;00m\n\u001b[1;32m    505\u001b[0m \u001b[38;5;66;03m# legitimately able to close the connection after sending a valid response.\u001b[39;00m\n\u001b[1;32m    506\u001b[0m \u001b[38;5;66;03m# With this behaviour, the received response is still readable.\u001b[39;00m\n\u001b[1;32m    507\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBrokenPipeError\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/my_project-rq9fZLKI/lib/python3.10/site-packages/urllib3/connection.py:508\u001b[0m, in \u001b[0;36mHTTPConnection.request\u001b[0;34m(self, method, url, body, headers, chunked, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    506\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msend(\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%x\u001b[39;00m\u001b[38;5;130;01m\\r\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;130;01m\\r\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (\u001b[38;5;28mlen\u001b[39m(chunk), chunk))\n\u001b[1;32m    507\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 508\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    510\u001b[0m \u001b[38;5;66;03m# Regardless of whether we have a body or not, if we're in\u001b[39;00m\n\u001b[1;32m    511\u001b[0m \u001b[38;5;66;03m# chunked mode we want to send an explicit empty chunk.\u001b[39;00m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunked:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/http/client.py:998\u001b[0m, in \u001b[0;36mHTTPConnection.send\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    996\u001b[0m sys\u001b[38;5;241m.\u001b[39maudit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttp.client.send\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m, data)\n\u001b[1;32m    997\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 998\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msendall\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    999\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   1000\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, collections\u001b[38;5;241m.\u001b[39mabc\u001b[38;5;241m.\u001b[39mIterable):\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/ssl.py:1237\u001b[0m, in \u001b[0;36mSSLSocket.sendall\u001b[0;34m(self, data, flags)\u001b[0m\n\u001b[1;32m   1235\u001b[0m         amount \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(byte_view)\n\u001b[1;32m   1236\u001b[0m         \u001b[38;5;28;01mwhile\u001b[39;00m count \u001b[38;5;241m<\u001b[39m amount:\n\u001b[0;32m-> 1237\u001b[0m             v \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbyte_view\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcount\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1238\u001b[0m             count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m v\n\u001b[1;32m   1239\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/ssl.py:1206\u001b[0m, in \u001b[0;36mSSLSocket.send\u001b[0;34m(self, data, flags)\u001b[0m\n\u001b[1;32m   1202\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1203\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1204\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to send() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1205\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m-> 1206\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1207\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1208\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39msend(data, flags)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os, textwrap\n",
    "from huggingface_hub import HfApi, create_repo, upload_folder, upload_file\n",
    "\n",
    "# ==== CONFIG: edit these ====\n",
    "HF_USERNAME = \"tahamajs\"                   # your HF username or org\n",
    "DATASET_NAME = \"btc_news_effects_2018_2024\"  # repo name on HF\n",
    "LOCAL_FOLDER = \"outputs_btc_effects\"       # your generated folder\n",
    "# ============================\n",
    "\n",
    "REPO_ID = f\"{HF_USERNAME}/{DATASET_NAME}\"\n",
    "\n",
    "api = HfApi()\n",
    "create_repo(repo_id=REPO_ID, repo_type=\"dataset\", exist_ok=True)\n",
    "\n",
    "# Upload the whole folder under data/\n",
    "api.upload_folder(\n",
    "    repo_id=REPO_ID,\n",
    "    repo_type=\"dataset\",\n",
    "    folder_path=LOCAL_FOLDER,\n",
    "    path_in_repo=\"data\",     # you'll get data/per_date, data/aggregated_*.jsonl, data/prompts_debug\n",
    "    commit_message=\"Initial upload of BTC news effects (2018–2024)\"\n",
    ")\n",
    "\n",
    "# Minimal dataset card (README)\n",
    "readme = textwrap.dedent(f\"\"\"\n",
    "# BTC News Effects (2018–2024)\n",
    "\n",
    "This dataset contains per-date JSON outputs and aggregated JSONL files generated by a parallel LLM pipeline that scores **news impact on BTC** (direction, magnitude, horizon, confidence), with **daily synthesized recommendations**.\n",
    "\n",
    "## Contents\n",
    "- `data/per_date/*.json` — one JSON per day (schema: `long_term`, `short_term`, `daily_view`).\n",
    "- `data/aggregated_effects.jsonl` — flattened per-pick rows (long/short) with `effect_type`, `date`, etc.\n",
    "- `data/aggregated_daily_view.jsonl` — daily synthesis rows (scenario probabilities, short/long recommendations).\n",
    "- `data/prompts_debug/*.prompt.txt` — rendered prompts per date (debugging / reproducibility).\n",
    "\n",
    "## Example: load aggregated JSONL with 🤗 Datasets\n",
    "```python\n",
    "from datasets import load_dataset\n",
    "\n",
    "repo = \"{REPO_ID}\"\n",
    "\n",
    "ds_effects = load_dataset(\n",
    "    repo, data_files={\"train\": \"data/aggregated_effects.jsonl\"},\n",
    "    split=\"train\"\n",
    ")\n",
    "ds_daily = load_dataset(\n",
    "    repo, data_files={\"train\": \"data/aggregated_daily_view.jsonl\"},\n",
    "    split=\"train\"\n",
    ")\n",
    "print(ds_effects[0])\n",
    "print(ds_daily[0])\n",
    "````\n",
    "\n",
    "## Notes\n",
    "\n",
    "* Source news: `edaschau/bitcoin_news`\n",
    "* LLM: Gemini 2.5 (multi-key rotation), parallel workers with per-key cooldown.\n",
    "* Windows: long-term pool = last 60d (excluding focus day), short-term pool = focus day.\n",
    "* Repro knobs: see code comments (`LONG_WINDOW_DAYS`, `LONG_POOL_N`, `SHORT_POOL_N`, etc.).\n",
    "\n",
    "## License\n",
    "\n",
    "Data license should match the underlying news usage terms. You are responsible for compliance when using/redistributing this dataset.\n",
    "\n",
    "\"\"\").strip()\n",
    "\n",
    "upload_file(\n",
    "path_or_fileobj=readme.encode(\"utf-8\"),\n",
    "path_in_repo=\"README.md\",\n",
    "repo_id=REPO_ID,\n",
    "repo_type=\"dataset\",\n",
    "commit_message=\"Add dataset card\"\n",
    ")\n",
    "\n",
    "print(f\"✅ Uploaded to: [https://huggingface.co/datasets/{REPO_ID}](https://huggingface.co/datasets/{REPO_ID})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "55cc31ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loading aggregated JSONL files …\n",
      "[INFO] Saved Parquet: outputs_btc_effects/_db_package/effects.parquet, outputs_btc_effects/_db_package/daily_view.parquet\n",
      "[INFO] Saved SQLite DB: outputs_btc_effects/_db_package/btc_news_effects.sqlite\n",
      "[INFO] Creating dataset repo: tahamajs/btc_news_effects_db_20250823_201546\n",
      "[INFO] Uploading DB + Parquet …\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad2224adbac34357993fe1deffcc64ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0)                : |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30748b1ff7094eda93e815de7daf08a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload                         : |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc41bcf050624b0d8085e01d15a1bd27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  ..._db_package/btc_news_effects.sqlite:   6%|6         |  639kB / 10.2MB            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4fade30c4f74f8ba1021d1f69c57c10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  ...ects/_db_package/daily_view.parquet:   6%|6         | 13.9kB /  222kB            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56be906e380f426287567c4a948583eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  ...effects/_db_package/effects.parquet:   6%|6         |  225kB / 3.59MB            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "It seems you are trying to upload a large folder at once. This might take some time and then fail if the folder is too large. For such cases, it is recommended to upload in smaller batches or to use `HfApi().upload_large_folder(...)`/`hf upload-large-folder` instead. For more details, check out https://huggingface.co/docs/huggingface_hub/main/en/guides/upload#upload-a-large-folder.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Uploading raw outputs …\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tahamajs/.local/share/virtualenvs/my_project-rq9fZLKI/lib/python3.10/site-packages/huggingface_hub/hf_api.py:9696: UserWarning: Warnings while validating metadata in README.md:\n",
      "- empty or missing yaml metadata in repo card\n",
      "  warnings.warn(f\"Warnings while validating metadata in README.md:\\n{message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ DONE\n",
      "Dataset: https://huggingface.co/datasets/tahamajs/btc_news_effects_db_20250823_201546\n"
     ]
    }
   ],
   "source": [
    "# pack_and_push.py\n",
    "import os, json, glob, textwrap, sqlite3, re\n",
    "from datetime import datetime\n",
    "from typing import List, Dict, Any\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from huggingface_hub import HfApi, create_repo\n",
    "\n",
    "# ========= CONFIG (edit these) =========\n",
    "LOCAL_DIR     = \"outputs_btc_effects\"     # your pipeline output folder\n",
    "HF_USERNAME   = \"tahamajs\"                # your HF user/org\n",
    "REPO_PREFIX   = \"btc_news_effects_db\"     # new dataset name = prefix + timestamp\n",
    "INCLUDE_RAW   = True                      # also upload raw JSON/JSONL/prompts\n",
    "# ======================================\n",
    "\n",
    "def _log(msg): print(f\"[INFO] {msg}\")\n",
    "\n",
    "def read_jsonl(path: str) -> List[Dict[str, Any]]:\n",
    "    rows = []\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            s = line.strip()\n",
    "            if not s: \n",
    "                continue\n",
    "            try:\n",
    "                rows.append(json.loads(s))\n",
    "            except Exception:\n",
    "                s = re.sub(r\",\\s*}\", \"}\", s)\n",
    "                s = re.sub(r\",\\s*]\", \"]\", s)\n",
    "                rows.append(json.loads(s))\n",
    "    return rows\n",
    "\n",
    "def coerce_df(obj_list: List[Dict[str, Any]]) -> pd.DataFrame:\n",
    "    return pd.json_normalize(obj_list) if obj_list else pd.DataFrame()\n",
    "\n",
    "def sanitize_column_names(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    new_cols = []\n",
    "    for c in df.columns:\n",
    "        c2 = c.replace(\".\", \"_\")\n",
    "        c2 = re.sub(r\"\\s+\", \"_\", c2)\n",
    "        c2 = re.sub(r\"[^0-9a-zA-Z_]\", \"_\", c2)\n",
    "        new_cols.append(c2)\n",
    "    df.columns = new_cols\n",
    "    return df\n",
    "\n",
    "def stringify_cells(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    def _stringify(x):\n",
    "        if x is None or isinstance(x, (str, int, float, bool, np.integer, np.floating, np.bool_)):\n",
    "            return x\n",
    "        if isinstance(x, float) and np.isnan(x):\n",
    "            return None\n",
    "        if hasattr(x, \"isoformat\"):\n",
    "            try: return x.isoformat()\n",
    "            except Exception: pass\n",
    "        if isinstance(x, (list, dict, set, tuple)):\n",
    "            try: return json.dumps(x, ensure_ascii=False)\n",
    "            except Exception: return str(x)\n",
    "        return str(x)\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype == \"O\":\n",
    "            df[col] = df[col].apply(_stringify)\n",
    "    return df\n",
    "\n",
    "def build_from_per_date(per_date_dir: str):\n",
    "    effects_rows, daily_rows = [], []\n",
    "    files = sorted(glob.glob(os.path.join(per_date_dir, \"*.json\")))\n",
    "    for fp in files:\n",
    "        try:\n",
    "            with open(fp, \"r\", encoding=\"utf-8\") as f:\n",
    "                obj = json.load(f)\n",
    "        except Exception:\n",
    "            continue\n",
    "        d = obj.get(\"date\") or os.path.splitext(os.path.basename(fp))[0]\n",
    "        for typ in (\"long_term\", \"short_term\"):\n",
    "            for rec in (obj.get(typ) or []):\n",
    "                rec_flat = dict(rec)\n",
    "                rec_flat[\"effect_type\"] = typ\n",
    "                rec_flat[\"date\"] = d\n",
    "                effects_rows.append(rec_flat)\n",
    "        dv = obj.get(\"daily_view\")\n",
    "        if isinstance(dv, dict):\n",
    "            dv_flat = dict(dv); dv_flat[\"date\"] = d\n",
    "            daily_rows.append(dv_flat)\n",
    "    return coerce_df(effects_rows), coerce_df(daily_rows)\n",
    "\n",
    "def main():\n",
    "    if not os.path.isdir(LOCAL_DIR):\n",
    "        raise SystemExit(f\"Folder not found: {LOCAL_DIR}\")\n",
    "\n",
    "    per_date_dir = os.path.join(LOCAL_DIR, \"per_date\")\n",
    "    agg_effects  = os.path.join(LOCAL_DIR, \"aggregated_effects.jsonl\")\n",
    "    agg_daily    = os.path.join(LOCAL_DIR, \"aggregated_daily_view.jsonl\")\n",
    "\n",
    "    # ---- Load data first\n",
    "    if os.path.isfile(agg_effects) and os.path.isfile(agg_daily):\n",
    "        _log(\"Loading aggregated JSONL files …\")\n",
    "        effects_df = coerce_df(read_jsonl(agg_effects))\n",
    "        daily_df   = coerce_df(read_jsonl(agg_daily))\n",
    "    else:\n",
    "        _log(\"Aggregated files missing. Building from per_date/*.json …\")\n",
    "        effects_df, daily_df = build_from_per_date(per_date_dir)\n",
    "\n",
    "    if effects_df.empty and daily_df.empty:\n",
    "        raise SystemExit(\"No data found to package.\")\n",
    "\n",
    "    # Ensure minimal columns\n",
    "    if \"date\" not in effects_df.columns: effects_df[\"date\"] = None\n",
    "    if \"effect_type\" not in effects_df.columns: effects_df[\"effect_type\"] = None\n",
    "    if \"date\" not in daily_df.columns: daily_df[\"date\"] = None\n",
    "\n",
    "    # ---- Clean & stringify BEFORE writing\n",
    "    effects_df = sanitize_column_names(effects_df.copy())\n",
    "    daily_df   = sanitize_column_names(daily_df.copy())\n",
    "    effects_df = stringify_cells(effects_df)\n",
    "    daily_df   = stringify_cells(daily_df)\n",
    "\n",
    "    # ---- Output dir\n",
    "    out_dir = os.path.join(LOCAL_DIR, \"_db_package\")\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    # ---- Parquet\n",
    "    effects_parquet = os.path.join(out_dir, \"effects.parquet\")\n",
    "    daily_parquet   = os.path.join(out_dir, \"daily_view.parquet\")\n",
    "    effects_df.to_parquet(effects_parquet, index=False)\n",
    "    daily_df.to_parquet(daily_parquet, index=False)\n",
    "    _log(f\"Saved Parquet: {effects_parquet}, {daily_parquet}\")\n",
    "\n",
    "    # ---- SQLite (coerce all columns to TEXT to avoid binding edge-cases)\n",
    "    sqlite_path = os.path.join(out_dir, \"btc_news_effects.sqlite\")\n",
    "    conn = sqlite3.connect(sqlite_path)\n",
    "    effects_df.to_sql(\"effects\", conn, if_exists=\"replace\", index=False,\n",
    "                      dtype={c: \"TEXT\" for c in effects_df.columns})\n",
    "    daily_df.to_sql(\"daily_view\", conn, if_exists=\"replace\", index=False,\n",
    "                    dtype={c: \"TEXT\" for c in daily_df.columns})\n",
    "    conn.close()\n",
    "    _log(f\"Saved SQLite DB: {sqlite_path}\")\n",
    "\n",
    "    # ---- Create HF repo (timestamped)\n",
    "    ts = datetime.utcnow().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    repo_id = f\"{HF_USERNAME}/{REPO_PREFIX}_{ts}\"\n",
    "    _log(f\"Creating dataset repo: {repo_id}\")\n",
    "    api = HfApi()\n",
    "    create_repo(repo_id=repo_id, repo_type=\"dataset\", exist_ok=False)\n",
    "\n",
    "    # ---- Upload DB + Parquet\n",
    "    _log(\"Uploading DB + Parquet …\")\n",
    "    api.upload_folder(\n",
    "        repo_id=repo_id,\n",
    "        repo_type=\"dataset\",\n",
    "        folder_path=out_dir,\n",
    "        path_in_repo=\"data\",\n",
    "        commit_message=\"Add SQLite database and Parquet exports\",\n",
    "    )\n",
    "\n",
    "    # ---- (Optional) Upload raw outputs for provenance\n",
    "    if INCLUDE_RAW:\n",
    "        _log(\"Uploading raw outputs …\")\n",
    "        api.upload_folder(\n",
    "            repo_id=repo_id,\n",
    "            repo_type=\"dataset\",\n",
    "            folder_path=LOCAL_DIR,\n",
    "            path_in_repo=\"raw\",\n",
    "            commit_message=\"Add raw per_date JSON and aggregated JSONL + prompts\",\n",
    "            ignore_patterns=[\"_db_package/**\"],  # avoid re-uploading generated files\n",
    "        )\n",
    "\n",
    "    # ---- README\n",
    "    readme = textwrap.dedent(f\"\"\"\n",
    "    # BTC News Effects — SQLite + Parquet\n",
    "\n",
    "    This dataset bundles:\n",
    "    - **SQLite**: `data/btc_news_effects.sqlite` (tables: `effects`, `daily_view`)\n",
    "    - **Parquet**: `data/effects.parquet`, `data/daily_view.parquet`\n",
    "    - **Raw**: original outputs under `raw/` (per-date JSON, aggregated JSONL, prompts)\n",
    "\n",
    "    ## Quick start (Parquet via pandas)\n",
    "    ```python\n",
    "    import pandas as pd\n",
    "    effects = pd.read_parquet(\"hf://datasets/{repo_id}/data/effects.parquet\")\n",
    "    daily   = pd.read_parquet(\"hf://datasets/{repo_id}/data/daily_view.parquet\")\n",
    "    print(effects.head()); print(daily.head())\n",
    "    ```\n",
    "\n",
    "    ## 🤗 Datasets loader\n",
    "    ```python\n",
    "    from datasets import load_dataset\n",
    "    ds_effects = load_dataset(\"{repo_id}\", data_files=\"data/effects.parquet\")[\"train\"]\n",
    "    ds_daily   = load_dataset(\"{repo_id}\", data_files=\"data/daily_view.parquet\")[\"train\"]\n",
    "    print(ds_effects[0]); print(ds_daily[0])\n",
    "    ```\n",
    "\n",
    "    ## Notes\n",
    "    Nested fields are JSON-encoded strings in DB/Parquet (e.g., lists/dicts).\n",
    "    \"\"\").strip()\n",
    "\n",
    "    from huggingface_hub import upload_file\n",
    "    upload_file(\n",
    "        path_or_fileobj=readme.encode(\"utf-8\"),\n",
    "        path_in_repo=\"README.md\",\n",
    "        repo_id=repo_id,\n",
    "        repo_type=\"dataset\",\n",
    "        commit_message=\"Add dataset card\",\n",
    "    )\n",
    "\n",
    "    print(\"\\n✅ DONE\")\n",
    "    print(f\"Dataset: https://huggingface.co/datasets/{repo_id}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb22e584",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf16a676",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
